{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce3dd8a-f3e5-40d5-ab4c-d40cc358cf7f",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Answering Medical Questions about Images using VantageCloud and Open-Source Language Models\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57054f7e-befe-4029-b4ac-b82019240b1f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial;'><b>Introduction:</b></p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial;'>In this comprehensive user demo, we will delve into the world of Medical Visual Question Answering using <b>Teradata Vantage</b> and <b>open-source language models</b>. This cutting-edge technology empowers businesses to uncover hidden insights from vast amounts of medical image data, enabling them to identify cells, diseases, artery etc.</p> \n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\"><b>Key Features:</b></p>\n",
    "\n",
    "<ol style=\"font-size:16px;font-family:Arial;\">\n",
    "<li><b>Improved Clinical Decision Support:</b> A well-trained medical VQA model enhances clinical decision-making by allowing healthcare providers to ask questions about medical images (e.g., X-rays, MRIs, CT scans) and receive accurate, rapid answers. This can lead to faster diagnoses and treatment plans.</li>\n",
    "\n",
    "<li><b>Reducing Interpretation Errors:</b> Human interpretation of medical images can be subjective and prone to errors. A VQA model can provide objective, consistent, and evidence-based interpretations, helping to reduce diagnostic inaccuracies.</li>\n",
    "\n",
    "<li><b>Time Efficiency:</b> The model's ability to quickly analyze images and answer questions can save valuable time for healthcare professionals, leading to more efficient patient care.</li>\n",
    "\n",
    "<li><b>Accessibility:</b> Patients and non-specialist healthcare providers can benefit from a medical VQA system by obtaining easy-to-understand information about their health conditions, potentially improving health literacy.</li>\n",
    "\n",
    "<li><b>Learning and Training Aid:</b> Medical VQA models can serve as educational tools for medical students, residents, and even experienced practitioners. They can be used to explain complex medical concepts and imaging findings.</li>\n",
    "\n",
    "<li><b>Research Assistance:</b> Researchers can leverage the model to analyze large datasets of medical images more effectively. It can assist in extracting meaningful insights from these datasets, potentially leading to new discoveries in medical science.</li>\n",
    "\n",
    "<li><b>Cross-Specialty Applicability:</b> A well-designed medical VQA model can be adapted to various medical specialties, from radiology and pathology to cardiology and dermatology. This versatility makes it a valuable asset across different healthcare domains.</li>\n",
    "\n",
    "<li><b>Ethical Considerations:</b> It's essential to address ethical concerns related to privacy, security, and bias when deploying medical VQA models in healthcare settings. Ensuring patient data protection and model fairness is critical.</li>\n",
    "\n",
    "<li><b>Continuous Improvement:</b> Model performance and accuracy should be continuously monitored and improved over time. Regular updates and retraining are necessary to keep up with evolving medical knowledge and technologies.</li>\n",
    "\n",
    "<li><b>Collaboration:</b> Successful implementation of medical VQA models often requires collaboration between machine learning experts, healthcare professionals, and ethicists to ensure that the technology is used responsibly and effectively.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6863ef",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'><b>Visual Question Answering:</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'><b>What is visual Question Answering?</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Visual Question Answering (VQA) is a task in computer vision that involves answering questions about an image. The goal of VQA is to teach machines to understand the content of an image and answer questions about it in natural language.</p>\n",
    "\n",
    "<center><img id=\"125\" src=\"./images/header_scene.png\" height=\"400px\" width=\"600px\" style=\"border: 4px solid #404040; border-radius: 10px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7068302",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'><b>Steps in the analysis:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial;'>\n",
    "     <li>Configuring the environment</li>\n",
    "  <li>Connect to Vantage</li>\n",
    "  <li>Create a Custom Container in Vantage</li>\n",
    "  <li>Install Dependencies</li>\n",
    "  <li>Operationalizing AI-powered analytics</li>\n",
    "  <li>Topic Modelling</li>\n",
    "  <li>Cleanup</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e457db5-d4df-4e7f-a11a-fcb1416e462d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial;'>1. Configuring the Environment</b>\n",
    "\n",
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>1.1 Install the required libraries</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Please be aware that that it will take longer than 5 minutes to install these libraries the first time this cell is executed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb4587-6632-4b08-a121-c98ea5a8fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "!pip install -r requirements.txt --quiet --no-warn-script-location\n",
    "# !pip install --upgrade transformers --quiet --no-warn-script-location\n",
    "# !pip install --upgrade torch --quiet --no-warn-script-location\n",
    "# !pip install --upgrade sentence-transformers --quiet --no-warn-script-location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2fca8-c4c6-4bcf-ae09-4a56edad3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U python-dotenv teradataml huggingface-hub --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd81e4-0df9-4360-b3ac-72214f135296",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial;'><b>Note: </b><i>Please restart the kernel after executing a </i><code>!pip install</code>. <i>The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i> and then clicking <b><i>Restart</i></b>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc278740-1511-4423-bf2b-92a5109a47bf",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>1.2 Import the required libraries</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f183178-9807-4538-b794-922820346ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from teradataml import *\n",
    "from teradatasqlalchemy.types import *\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from time import sleep\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "# Utility imports\n",
    "import shutil\n",
    "from IPython.display import clear_output, display as ipydisplay\n",
    "\n",
    "# Set display options for dataframes, plots, and warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "python_version = \"3.11\"\n",
    "print(f'Using Python version {python_version} for user environment')\n",
    "\n",
    "# Hugging Face model for the demo\n",
    "model_names = {'ChetanHirapara/Salesforce-blip-vqa-base-medical-data'}\n",
    "\n",
    "# a list of required packages to install in the custom OAF container\n",
    "# modify this if using different models or design patterns\n",
    "pkgs = ['transformers==4.57.6',\n",
    "        'torch==2.10.0',\n",
    "        'pandas==3.0.0',\n",
    "        'sentence-transformers==5.2.0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a37ff6-db2d-4c3a-90e6-9d1bceae9c60",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>2. Connect to VantageCloud Lake</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Connect to VantageCloud using <code>create_context</code> from the teradataml Python library. If this environment has been prepared for connecting to a VantageCloud Lake OAF Container, all the details required will be loaded and you will see an acknowledgement after executing this cell.</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>2.1 Load the Environment Variables and Connect to Vantage</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Load the environment variables from a .env file and use them to create a connection context to Teradata.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f0143-bad0-45ff-a287-22557bdd2b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking if this environment is ready to connect to VantageCloud Lake...\")\n",
    "\n",
    "if os.path.exists(\"/home/jovyan/JupyterLabRoot/VantageCloud_Lake/.config/.env\"):\n",
    "    print(\"Your environment parameter file exist.  Please proceed with this use case.\")\n",
    "    # Load all the variables from the .env file into a dictionary\n",
    "    env_vars = dotenv_values(\n",
    "        \"/home/jovyan/JupyterLabRoot/VantageCloud_Lake/.config/.env\"\n",
    "    )\n",
    "    # Create the Context\n",
    "    eng = create_context(\n",
    "        host=env_vars.get(\"host\"),\n",
    "        username=env_vars.get(\"username\"),\n",
    "        password=env_vars.get(\"my_variable\"),\n",
    "    )\n",
    "    execute_sql(\n",
    "        \"\"\"SET query_band='DEMO=Medical_Visual_Question_Answering.ipynb;' UPDATE FOR SESSION;\"\"\"\n",
    "    )\n",
    "    print(\"Connected to VantageCloud Lake with:\", eng)\n",
    "else:\n",
    "    print(\"Your environment has not been prepared for connecting to VantageCloud Lake.\")\n",
    "    print(\"Please contact the support team.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec98ec10-e1cc-43e7-b06d-fd06c5d89081",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>2.2  Authenticate to the User Environment Service</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>To better support integration with Cloud Services and common automation tools; the <b > User Environment Service</b> is accessed via RESTful APIs.  These APIs can be called directly or in the examples shown below that leverage the Python Package for Teradata (teradataml) methods.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a607a2-d902-4285-a006-5d4ec5b7bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've already loaded all the values into our environment variables and into a dictionary, env_vars.\n",
    "# username=env_vars.get(\"username\") isn't required when using base_url, pat and pem.\n",
    "\n",
    "if set_auth_token(\n",
    "    base_url=env_vars.get(\"ues_uri\"),\n",
    "    pat_token=env_vars.get(\"access_token\"),\n",
    "    pem_file=env_vars.get(\"pem_file\"),\n",
    "    valid_from=int(time.time()),\n",
    "):\n",
    "    print(\"UES Authentication successful\")\n",
    "else:\n",
    "    print(\"UES Authentication failed. Check credentials.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d1928-f6cb-4084-ad36-9559c1d1b20f",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<b style = 'font-size:18px;font-family:Arial;'>3. Create a Custom Container in VantageCloud</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>If desired, the user can create a <b>new</b> custom environment by starting with a \"base\" image and customizing it.  The steps are:</p> \n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li>List the available \"base\" images the system supports</li>\n",
    "    <li>List any existing \"custom\" environments the user has created</li>\n",
    "    <li>If there are no custom environments, then create a new one from a base image</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb82cfe-9db7-4826-b92b-9c588bd4018f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if we have any existing environments\n",
    "# If any other environments exist along with our default OAF environment, we will delete them\n",
    "username = env_vars.get(\"username\")\n",
    "environment_name = username[: min(10, len(username))]\n",
    "\n",
    "print(\n",
    "    \"Here is a list of the versions of the libraries available to be used within an OAF environments.\\n\"\n",
    ")\n",
    "print(list_base_envs())\n",
    "env_list = list_user_envs()\n",
    "\n",
    "if env_list is None:\n",
    "    print(\"\\nThis user does not have any environments.\\nCreating your environment now.\")\n",
    "    demo_env = create_env(\n",
    "        env_name=f\"{environment_name}\", base_env=\"python_3.11\", desc=\"BYOLLM demo env\"\n",
    "    )\n",
    "    print(demo_env)\n",
    "else:\n",
    "    print(\"\\nHere is a list of your current environments:\")\n",
    "    ipydisplay(env_list)\n",
    "    for env_name in env_list[\"env_name\"]:\n",
    "        if env_name == environment_name:\n",
    "            demo_env = get_env(environment_name)\n",
    "            print(\n",
    "                \"Your default environment already exists. You can continue with this notebook.\\n\\n\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"Your existing environment, {env_name} doesn't match our default environment for this user.\"\n",
    "            )\n",
    "            print(\"We're going to delete it.\")\n",
    "            print(f\"Please wait: Environment {env_name} is being removed!\")\n",
    "            remove_env(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556a860-e27f-432d-a4d1-f4bd903e34fd",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>4. Install Dependencies</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The step in the process installs Python package dependencies. VantageCloud Open Analytics Framework Environments are secured against unauthorized access to the outside network. Users can load the required libraries and model using teradataml methods:\n",
    "</p> \n",
    "\n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li>List the currently installed models and python libraries</li>\n",
    "    <li><b>If necessary</b>, install any required packages</li>\n",
    "    <li><b>If necessary</b>, install the pre-trained model.  This process takes several steps;\n",
    "        <ol style = 'font-size:16px;font-family:Arial;'>\n",
    "            <li>Import and download the model</li>\n",
    "            <li>Create a zip archive of the model artifacts</li>\n",
    "            <li>Call the install_model() method to load the model to the container</li>\n",
    "        </ol></li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77d1b5-d931-4766-806b-2068b27c8f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Here are the models installed in the remote user environment...\")\n",
    "ipydisplay(demo_env.models)\n",
    "\n",
    "print(\"Here are the libraries installed in the remote user environment...\")\n",
    "ipydisplay(demo_env.libs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b179f-4649-483a-a470-db064266c3b6",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>4.1 A note on package versions</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The next section of this demonstration makes use of the DataFrame apply() method, which will execute python code that we'll upload into the environment.  We have to ensue the python packages we have installed locally match the versions we have in the environment.\n",
    "</p> \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'><b>Note:</b> Please be aware that this step may take 15 minutes to execute.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376feb7-8bf2-49de-816e-8a9131be876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get package versions using importlib instead of unsafe exec()\n",
    "import importlib\n",
    "\n",
    "\n",
    "def get_versions(pkg_names):\n",
    "    \"\"\"Get installed versions of packages without using exec().\"\"\"\n",
    "    versioned_pkgs = []\n",
    "    for pkg_name in pkg_names:\n",
    "        try:\n",
    "            pkg_module_name = pkg_name.replace(\"-\", \"_\")\n",
    "            module = importlib.import_module(pkg_module_name)\n",
    "            version = getattr(module, \"__version__\", \"unknown\")\n",
    "            versioned_pkgs.append(f\"{pkg_name}=={version}\")\n",
    "        except ImportError:\n",
    "            print(f\"Warning: Package '{pkg_name}' not found\")\n",
    "    return versioned_pkgs\n",
    "\n",
    "\n",
    "print(f\"Checking required packages: {pkgs}\")\n",
    "v_pkgs = get_versions(pkgs)\n",
    "installed_pkgs = set([x.split(\"==\")[0] for x in pkgs])\n",
    "environment_pkgs = set(demo_env.libs[\"name\"].to_list())\n",
    "\n",
    "if not installed_pkgs.issubset(environment_pkgs):\n",
    "    missing_pkgs = installed_pkgs - environment_pkgs\n",
    "    print(f\"Installing missing packages: {missing_pkgs}\")\n",
    "    claim_id = demo_env.install_lib(\n",
    "        [x.split(\"+\")[0] for x in v_pkgs], asynchronous=True\n",
    "    )\n",
    "else:\n",
    "    print(f\"All required packages are installed in the {environment_name} environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72014c4c-dbb0-4a7d-80bd-901ff3d153c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim_id = demo_env.install_lib([\"transformers==4.57.6\", \"pandas==3.0.0\", \"torch==2.10.0\", \"sentence-transformers==5.2.0\", \"pillow==12.1.0\"], asynchronous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d186ca1",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>4.2 Monitor library installation status</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Optionally - users can monitor the library installation status using the cell below:\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8c3e4-06cb-4fae-913c-3b1202f72001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of installation using status() API.\n",
    "# Create a loop here for demo purposes\n",
    "try:\n",
    "    claim_id\n",
    "    ipydisplay(demo_env.status(claim_id))\n",
    "    stage = demo_env.status(claim_id)[\"Stage\"].iloc[-1]\n",
    "    while stage == \"Started\":\n",
    "        stage = demo_env.status(claim_id)[\"Stage\"].iloc[-1]\n",
    "        clear_output()\n",
    "        ipydisplay(demo_env.status(claim_id))\n",
    "        sleep(5)\n",
    "except NameError:\n",
    "    print(\"No installations to monitor\")\n",
    "\n",
    "\n",
    "# Verify the Python libraries have been installed correctly.\n",
    "print(\n",
    "    \"Here is an updated list of libraries installed in the remote user environment...\"\n",
    ")\n",
    "ipydisplay(demo_env.libs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162969f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>4.3 Download and install model</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Open Analytics Framework environments do not have open access to the external network, which contributes to a very secure runtime environment.  As such, we will need to download the pre-trained model using the below API.  This cell will download the model and then install it into our OpenAnalytics Framework environment. This will take ~5 minutes.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08989723-d0c0-4f70-bd5f-bf7fa2beb6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "MODEL_REPO_ID = \"ChetanHirapara/Salesforce-blip-vqa-base-medical-data\"\n",
    "MODEL_DIR = \"./blip_model\"\n",
    "MODEL_ZIP = \"blip_model.zip\"\n",
    "\n",
    "try:\n",
    "    print(f\"Downloading model from {MODEL_REPO_ID}...\")\n",
    "    model_path = snapshot_download(repo_id=MODEL_REPO_ID, local_dir=MODEL_DIR)\n",
    "    print(f\"✓ Model downloaded to: {MODEL_DIR}\")\n",
    "\n",
    "    print(\"Creating archive for installation...\")\n",
    "    shutil.make_archive(\n",
    "        MODEL_DIR,\n",
    "        format=\"zip\",\n",
    "        root_dir=MODEL_DIR,\n",
    "    )\n",
    "    print(f\"✓ Archive created: {MODEL_ZIP}\")\n",
    "\n",
    "    print(\"Installing model to environment...\")\n",
    "    claim_id = demo_env.install_model(MODEL_ZIP, asynchronous=True)\n",
    "    print(f\"✓ Model installation initiated with claim ID: {claim_id}\")\n",
    "\n",
    "except TeradataMlException as e:\n",
    "    print(f\"ℹ Model installation info: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error during model setup: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268e54f-78f9-4ebb-913b-16317807bbb4",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>4.4 Monitor model installation status</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Optionally - users can monitor the model installation status using the cell below:\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8c67c-e326-41a1-a71d-dfb001a968a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the status of installation using status() API.\n",
    "# Create a loop here for demo purposes\n",
    "try:\n",
    "    claim_id\n",
    "    ipydisplay(demo_env.status(claim_id))\n",
    "    stage = demo_env.status(claim_id)[\"Stage\"].iloc[-1]\n",
    "    while stage != \"File Installed\":\n",
    "        stage = demo_env.status(claim_id)[\"Stage\"].iloc[-1]\n",
    "        clear_output()\n",
    "        ipydisplay(demo_env.status(claim_id))\n",
    "        sleep(5)\n",
    "except NameError:\n",
    "    print(\"No installations to monitor\")\n",
    "\n",
    "\n",
    "# Verify the model has been installed correctly.\n",
    "demo_env.refresh()\n",
    "ipydisplay(demo_env.models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c1276-6ada-4fd2-bf10-6e76730f1c7a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>The preceding demo showed how users can perform a <b>one-time</b> configuration task to prepare a custom environment for analytic processing at scale.  Once this configuration is complete, these containers can be re-used in ad-hoc development tasks, or used for operationalizing analytics in production.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf0993",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>5. Operationalizing AI-powered analytics</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The following demonstration will illustrate how developers can take the next step in the process to <b>operationalize</b> this processing, enabling the entire organization to leverage AI across the data lifecycle, including</p>\n",
    "\n",
    "<table style = 'width:100%;table-layout:fixed;'>\n",
    "    <tr>\n",
    "        <td style = 'vertical-align:top' width = '30%'>\n",
    "           <ol style = 'font-size:16px;font-family:Arial;'>\n",
    "               <li><b>Prepare the environment</b>.  Package the scoring function into a more robust program, and stage it on the remote environment</li>\n",
    "            <br>\n",
    "            <br>\n",
    "               <li><b>Python Pipeline</b>.  Execute the function using Python methods</li>\n",
    "            <br>\n",
    "            <br>\n",
    "               <li><b>SQL Pipeline</b>.  Execute the function using SQL - allowing for broad adoption and use in ETL and operational needs</li>\n",
    "        </ol>\n",
    "        </td>\n",
    "        <td width = '20%'></td>\n",
    "        <td style = 'vertical-align:top'><img src = 'images/OAF_Ops.png' width=350 style=\"border: 4px solid #404040; border-radius: 10px;\"/></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c15f1",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.1 Create a server-side Visual inference function</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The goal of this exercise is to create a <b>server-side</b> function which can be staged on the analytic cluster.  This offers many improvements over the method used above;</p> \n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li><b>Performance</b>.  Staging the code and dependencies in the container environment reduces the amount of I/O, since the function doesn't need to get serialized to the cluster when called</li>\n",
    "    <li><b>Operationalization</b>.  The execution pipeline can be encapsulated into a SQL statement, which allows for seamless use in ETL pipelines, dashboards, or applications that need access</li>\n",
    "    <li><b>Flexibility</b>. Developers can express much greater flexibility in how the code works to optimize for performance, stability, data cleanliness or flow logic</li>\n",
    "</ul>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>These benefits do come with some amount of additional work.  Developers need to accomodate how data is passed in and out of the code at runtime, and how to pass it back to the SQL engine to assemble and return the final result set.  Code is executed when the user submits an <a href = 'https://docs.teradata.com/r/Teradata-VantageCloud-Lake/SQL-Reference/SQL-Operators-and-User-Defined-Functions/Table-Operators/APPLY'>APPLY SQL function</a>;</p> \n",
    "<ol style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li><b>Input Query</b>.  The <code>APPLY</code> function takes a SQL query as input.  This query can be as complex as needed and include data preparation, cleansing, and/or any other set-based logic necessary to create the desired input data set.  This complexity can also be abstracted into a database view.  When using the teradata client connectors for Python or R, thise query is represented as a DataFrame or tibble.</li>\n",
    "    <li><b>Pre-processing</b>.  Based on the query plan, data is retrieved from storage (cache, block storage, or object storage) and the input query is executed.</li>\n",
    "    <li><b>Distribution</b>.  Input data can be partitioned and/or ordered to be processed on a specific container or collection of them.  For example, the user may want to process all data for a single post code in one partition, and run thousands of these in parallel.  Data can also be distributed evenly across all units of parallelism in the system</li>\n",
    "    <li><b>Input</b>.  The data for each container is passed to the runtime using tandard input (stdin)</li>\n",
    "    <li><b>Processing</b>.  The user's code executes, parsing stdin for the input data</li>\n",
    "    <li><b>Output</b>.  Data is sent out of the code block using standard output (stdout)</li>\n",
    "    <li><b>Resultset</b>.  Resultset is assembled by the analytic database, and the SQL query returns</li>\n",
    "    </ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b2628",
   "metadata": {},
   "source": [
    "# Expose a helper to read the current selection later\n",
    "def get_selected_path():\n",
    "    \"\"\"Return the single selected file path or None.\"\"\"\n",
    "    try:\n",
    "        selected = next((p for p, cb in zip(image_paths, selectors) if cb.value), None)\n",
    "        return selected\n",
    "    except (NameError, IndexError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae6f4e",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.3.  Install the file and any additional artifacts</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Use the install_file() method to install this python file to the container.  As a reminder, this container is persistent, so these steps need only be done infrequently.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744a1c7-3a65-4877-ac25-e76222ddfc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_name = \"library\"\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f\"Folder '{folder_name}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b3b766-ea12-4912-8d9b-37f584533e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./library/Medical_Visual_Question_Answering_OAF.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Medical Visual Question Answering using BLIP model.\n",
    "\n",
    "This module provides functionality for performing visual question answering\n",
    "on medical images using the BLIP (Bootstrapped Language-Image Pre-training) model.\n",
    "It reads input data from stdin, processes images and questions, and outputs predictions.\n",
    "\n",
    "Created on Mon Sept 29 18:54:17 2025\n",
    "@author: author\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import time\n",
    "import getpass\n",
    "from typing import Tuple, Dict, List, Any\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION CONSTANTS\n",
    "# ============================================================================\n",
    "# Model configuration\n",
    "MODEL_PATH = \"./models/blip_model\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Image processing parameters\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH = 128\n",
    "IMAGE_SIZE = 224\n",
    "DEFAULT_IMAGE_COLOR = (200, 100, 50)\n",
    "\n",
    "# Text processing parameters\n",
    "MAX_TEXT_LENGTH = 32\n",
    "MAX_GENERATION_LENGTH = 50\n",
    "NUM_BEAMS = 3\n",
    "\n",
    "# Data processing\n",
    "INPUT_DELIMITER = \"#\"\n",
    "REQUIRED_COLUMNS = [\"id\", \"img\", \"question\", \"answer\"]\n",
    "REQUIRED_INPUT_COLUMNS = [\"question\", \"answer\", \"img\"]\n",
    "\n",
    "# Cache directories\n",
    "CACHE_DIRS = {\n",
    "    \"torch\": \"/tmp/torch_cache\",\n",
    "    \"huggingface\": \"/tmp/huggingface_cache\",\n",
    "    \"transformers\": \"/tmp/transformers_cache\",\n",
    "    \"torch_inductor\": \"/tmp/torch_inductor_cache\",\n",
    "}\n",
    "\n",
    "\n",
    "original_getuser = getpass.getuser\n",
    "\n",
    "\n",
    "def safe_getuser() -> str:\n",
    "    \"\"\"\n",
    "    Safe wrapper for getuser that handles UID lookup failures in containerized envs.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str : Username or fallback UID string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return original_getuser()\n",
    "    except KeyError:\n",
    "        return f\"uid_{os.getuid()}\"\n",
    "\n",
    "\n",
    "# Monkey-patch getpass before torch imports it\n",
    "getpass.getuser = safe_getuser\n",
    "\n",
    "# Configure cache directories before importing torch/transformers\n",
    "for cache_name, cache_path in CACHE_DIRS.items():\n",
    "    os.environ[\n",
    "        (\n",
    "            f\"TORCH_HOME\"\n",
    "            if cache_name == \"torch\"\n",
    "            else (\n",
    "                f\"HF_HOME\"\n",
    "                if cache_name == \"huggingface\"\n",
    "                else (\n",
    "                    f\"TRANSFORMERS_CACHE\"\n",
    "                    if cache_name == \"transformers\"\n",
    "                    else \"TORCHINDUCTOR_CACHE_DIR\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ] = cache_path\n",
    "\n",
    "    try:\n",
    "        os.makedirs(cache_path, exist_ok=True)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForQuestionAnswering,\n",
    "    BlipImageProcessor,\n",
    "    BlipConfig,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_input_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and parse input data from stdin.\n",
    "\n",
    "    Expected format: \"id#image_base64#question#answer\" (one per line)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : DataFrame with columns: id, img, question, answer\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError : If no input data provided\n",
    "    RuntimeError : If data format is invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inputData = []\n",
    "\n",
    "        for line_num, line in enumerate(sys.stdin.read().splitlines(), 1):\n",
    "            try:\n",
    "                parts = line.split(INPUT_DELIMITER)\n",
    "                if len(parts) < 4:\n",
    "                    raise ValueError(f'Expected 4 fields, got {len(parts)}\"')\n",
    "                inputData.append(parts)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        if not inputData:\n",
    "            raise ValueError(\"No input data provided\")\n",
    "\n",
    "        pdf = pd.DataFrame(inputData, columns=REQUIRED_COLUMNS).copy()\n",
    "        return pdf\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "\n",
    "def initialize_model() -> (\n",
    "    Tuple[BlipForQuestionAnswering, BlipProcessor, BlipImageProcessor, torch.device]\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize BLIP model components.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (model, text_processor, image_processor, device)\n",
    "        - model: BlipForQuestionAnswering instance\n",
    "        - text_processor: BlipProcessor for text\n",
    "        - image_processor: BlipImageProcessor for images\n",
    "        - device: torch.device (cuda or cpu)\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    FileNotFoundError : If model files not found at MODEL_PATH\n",
    "    RuntimeError : If model loading fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_path = Path(MODEL_PATH)\n",
    "        if not model_path.exists():\n",
    "            raise FileNotFoundError(f\"Model path not found: {MODEL_PATH}\")\n",
    "\n",
    "        # Load model components\n",
    "        try:\n",
    "            config = BlipConfig.from_pretrained(MODEL_PATH)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model config: {e}\") from e\n",
    "\n",
    "        try:\n",
    "            text_processor = BlipProcessor.from_pretrained(MODEL_PATH)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load text processor: {e}\") from e\n",
    "\n",
    "        try:\n",
    "            image_processor = BlipImageProcessor.from_pretrained(MODEL_PATH)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load image processor: {e}\") from e\n",
    "\n",
    "        try:\n",
    "            model = BlipForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model: {e}\") from e\n",
    "\n",
    "        model.to(DEVICE)\n",
    "        return model, text_processor, image_processor, DEVICE\n",
    "\n",
    "    except (FileNotFoundError, RuntimeError):\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Model initialization failed: {e}\") from e\n",
    "\n",
    "\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Visual Question Answering Dataset for BLIP model.\n",
    "\n",
    "    Handles image decoding from base64, text processing, and label encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        text_processor: BlipProcessor,\n",
    "        image_processor: BlipImageProcessor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize VQA dataset.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            DataFrame with columns: question, answer, img\n",
    "        text_processor : BlipProcessor\n",
    "            Text processing pipeline\n",
    "        image_processor : BlipImageProcessor\n",
    "            Image processing pipeline\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.text_processor = text_processor\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = MAX_TEXT_LENGTH\n",
    "        self.image_height = IMAGE_HEIGHT\n",
    "        self.image_width = IMAGE_WIDTH\n",
    "\n",
    "        if hasattr(data, \"iloc\"):\n",
    "            self.questions = data[\"question\"].tolist()\n",
    "            self.answers = data[\"answer\"].tolist()\n",
    "            self.images = data[\"img\"].tolist()\n",
    "        else:\n",
    "            self.questions = data[\"question\"]\n",
    "            self.answers = data[\"answer\"]\n",
    "            self.images = data[\"img\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return dataset size.\"\"\"\n",
    "        return len(self.questions)\n",
    "\n",
    "    def _decode_image(self, img_data: str) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Decode image from base64 or binary data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        img_data : str or bytes\n",
    "            Base64-encoded or binary image data\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        Image.Image : PIL Image object in RGB format\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if isinstance(img_data, str):\n",
    "                img_bytes = base64.b64decode(img_data)\n",
    "            else:\n",
    "                img_bytes = img_data\n",
    "\n",
    "            image = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
    "            return image\n",
    "        except (ValueError, OSError):\n",
    "            return Image.new(\"RGB\", (224, 224), color=\"white\")\n",
    "        except Exception:\n",
    "            return Image.new(\"RGB\", (224, 224), color=\"white\")\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx : int\n",
    "            Sample index\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Encoded image and text with labels\n",
    "        \"\"\"\n",
    "        try:\n",
    "            answers = self.answers[idx]\n",
    "            questions = self.questions[idx]\n",
    "            image = self._decode_image(self.images[idx])\n",
    "            text = self.questions[idx]\n",
    "\n",
    "            image_encoding = self.image_processor(\n",
    "                image,\n",
    "                do_resize=True,\n",
    "                size=(self.image_height, self.image_width),\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            encoding = self.text_processor(\n",
    "                None,\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            for k, v in encoding.items():\n",
    "                encoding[k] = v.squeeze()\n",
    "            encoding[\"pixel_values\"] = image_encoding[\"pixel_values\"][0]\n",
    "\n",
    "            labels = self.text_processor.tokenizer.encode(\n",
    "                answers,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )[0]\n",
    "            encoding[\"labels\"] = labels\n",
    "\n",
    "            return encoding\n",
    "        except Exception:\n",
    "            raise\n",
    "\n",
    "\n",
    "def inference_single(\n",
    "    input_df: pd.DataFrame,\n",
    "    model: BlipForQuestionAnswering,\n",
    "    text_processor: BlipProcessor,\n",
    "    image_processor: BlipImageProcessor,\n",
    "    device: torch.device,\n",
    "    sample_idx: int = 0,\n",
    "    visualize: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform Visual Question Answering inference on a single sample.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_df : pd.DataFrame\n",
    "        DataFrame containing columns: 'question', 'answer', 'img'\n",
    "    model : BlipForQuestionAnswering\n",
    "        Pre-loaded BLIP model\n",
    "    text_processor : BlipProcessor\n",
    "        Text processor for encoding/decoding\n",
    "    image_processor : BlipImageProcessor\n",
    "        Image processor for preprocessing\n",
    "    device : torch.device\n",
    "        Device for inference (cuda or cpu)\n",
    "    sample_idx : int, default=0\n",
    "        Index of the sample to process\n",
    "    visualize : bool, default=False\n",
    "        Whether to display the input image\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Inference results with keys:\n",
    "        - question: Input question text\n",
    "        - predicted_answer: Model-generated answer\n",
    "        - actual_answer: Ground truth answer\n",
    "        - sample_index: Index of processed sample\n",
    "        - processing_time: Total processing time in seconds\n",
    "        - inference_time: Model inference time in seconds\n",
    "        - device_used: Device used for inference\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError : If input validation fails\n",
    "    RuntimeError : If inference fails\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Validate input\n",
    "        missing_columns = [\n",
    "            col for col in REQUIRED_INPUT_COLUMNS if col not in input_df.columns\n",
    "        ]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "        if len(input_df) == 0:\n",
    "            raise ValueError(\"Input DataFrame is empty\")\n",
    "\n",
    "        if sample_idx >= len(input_df):\n",
    "            raise ValueError(\n",
    "                f\"Sample index {sample_idx} out of range [0, {len(input_df)-1}]\"\n",
    "            )\n",
    "\n",
    "        # Create dataset and get sample\n",
    "        val_vqa_dataset = VQADataset(\n",
    "            data=input_df,\n",
    "            text_processor=text_processor,\n",
    "            image_processor=image_processor,\n",
    "        )\n",
    "\n",
    "        sample = val_vqa_dataset[sample_idx]\n",
    "\n",
    "        question_text = text_processor.decode(\n",
    "            sample[\"input_ids\"], skip_special_tokens=True\n",
    "        )\n",
    "        actual_answer = text_processor.decode(\n",
    "            sample[\"labels\"], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Prepare batch\n",
    "        sample_batch = {k: v.unsqueeze(0).to(device) for k, v in sample.items()}\n",
    "\n",
    "        # Run inference\n",
    "        model.eval()\n",
    "        inference_start = time.time()\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    pixel_values=sample_batch[\"pixel_values\"],\n",
    "                    input_ids=sample_batch[\"input_ids\"],\n",
    "                    max_length=MAX_GENERATION_LENGTH,\n",
    "                    num_beams=NUM_BEAMS,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "        except RuntimeError as e:\n",
    "            raise RuntimeError(f\"Model generation failed: {e}\") from e\n",
    "\n",
    "        inference_time = time.time() - inference_start\n",
    "        predicted_answer = text_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        results = {\n",
    "            \"question\": question_text,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"actual_answer\": actual_answer,\n",
    "            \"sample_index\": sample_idx,\n",
    "            \"processing_time\": time.time() - start_time,\n",
    "            \"inference_time\": inference_time,\n",
    "            \"device_used\": str(device),\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    except (ValueError, RuntimeError):\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Inference failed: {e}\") from e\n",
    "\n",
    "\n",
    "def process_batch_inference(\n",
    "    pdf: pd.DataFrame,\n",
    "    model: BlipForQuestionAnswering,\n",
    "    text_processor: BlipProcessor,\n",
    "    image_processor: BlipImageProcessor,\n",
    "    device: torch.device,\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Process all rows in the DataFrame and return results.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    pdf : pd.DataFrame\n",
    "        Input DataFrame with columns: id, img, question, answer\n",
    "    model : BlipForQuestionAnswering\n",
    "        Pre-loaded model\n",
    "    text_processor : BlipProcessor\n",
    "        Text processor\n",
    "    image_processor : BlipImageProcessor\n",
    "        Image processor\n",
    "    device : torch.device\n",
    "        Inference device\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of result dictionaries with keys: id, question, answer, predicted_answer\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    RuntimeError : If batch processing fails critically\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        for index, row in pdf.iterrows():\n",
    "            try:\n",
    "                single_row_df = pd.DataFrame([row])\n",
    "                result = inference_single(\n",
    "                    single_row_df,\n",
    "                    model,\n",
    "                    text_processor,\n",
    "                    image_processor,\n",
    "                    device,\n",
    "                    sample_idx=0,\n",
    "                    visualize=False,\n",
    "                )\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"id\": row[\"id\"],\n",
    "                        \"question\": result[\"question\"],\n",
    "                        \"answer\": result[\"actual_answer\"],\n",
    "                        \"predicted_answer\": result[\"predicted_answer\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception:\n",
    "                # Add error result to maintain row count\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"id\": row[\"id\"],\n",
    "                        \"question\": row[\"question\"],\n",
    "                        \"answer\": row[\"answer\"],\n",
    "                        \"predicted_answer\": \"ERROR\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Batch processing failed: {e}\") from e\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for Visual Question Answering inference.\"\"\"\n",
    "    try:\n",
    "        # Load input data\n",
    "        pdf = load_input_data()\n",
    "\n",
    "        # Initialize model\n",
    "        model, text_processor, image_processor, device = initialize_model()\n",
    "\n",
    "        # Process batch\n",
    "        results = process_batch_inference(\n",
    "            pdf, model, text_processor, image_processor, device\n",
    "        )\n",
    "\n",
    "        # Output results\n",
    "        for result in results:\n",
    "            print(\n",
    "                result[\"id\"],\n",
    "                INPUT_DELIMITER,\n",
    "                result[\"question\"],\n",
    "                INPUT_DELIMITER,\n",
    "                result[\"answer\"],\n",
    "                INPUT_DELIMITER,\n",
    "                result[\"predicted_answer\"],\n",
    "            )\n",
    "\n",
    "    except ValueError:\n",
    "        sys.exit(1)\n",
    "    except (FileNotFoundError, RuntimeError):\n",
    "        sys.exit(2)\n",
    "    except Exception:\n",
    "        sys.exit(3)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f035d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./library/Medical_Visual_Question_Answering_OAF.py\"\n",
    "demo_env.install_file(file_path=file_path, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03c822",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.5  Call the APPLY function </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>This function can be executed in two ways;</p> \n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li><b><a href = 'https://docs.teradata.com/r/Teradata-VantageCloud-Lake/Analyzing-Your-Data/Teradata-Package-for-Python-on-VantageCloud-Lake/Working-with-Open-Analytics/teradataml-Apply-Class-for-APPLY-Table-Operator'>Python</a></b> by calling the Apply() module function</li>\n",
    "    <li><b><a href = 'https://docs.teradata.com/r/Teradata-VantageCloud-Lake/SQL-Reference/SQL-Operators-and-User-Defined-Functions/Table-Operators/APPLY'>SQL</a></b> which allows for broad adoption across the enterprise</li>\n",
    "    </ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebf611",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.6 APPLY using Python</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The process is as follows</p> \n",
    "<ol style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li>Construct a dictionary that will define the return columns and data types</li>\n",
    "    <li>Construct a teradataml DataFrame representing the data to be processed - note this is a \"virtual\" object representing data and logic <b>in-database</b></li>\n",
    "    <li>Execute the module function.  This constructs the function call in the database, but does not execute anything.  Note the Apply function takes several arguments - the input data, environment name, and the command to run</li>\n",
    "    <li>In order to execute the function, an \"execute_script()\" method must be called.  This method returns the server-side DataFrame representing the complete operation.  This DataFrame can be used in further processing, stored as a table, etc.</li>\n",
    "    </ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = DataFrame.from_query(\"\"\"select * from DEMO_RefData.Medical_Images\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded4dc0-eeac-483a-83dc-e22e2ca02003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(tdf):\n",
    "    # return types dictionary\n",
    "    types_dict = OrderedDict({})\n",
    "    types_dict[\"id\"] = VARCHAR(100)\n",
    "    types_dict[\"question\"] = VARCHAR(1000)\n",
    "    types_dict[\"answer\"] = VARCHAR(1000)\n",
    "    types_dict[\"predicted_answer\"] = VARCHAR(1000)\n",
    "\n",
    "    apply_obj = Apply(\n",
    "        data=tdf,\n",
    "        apply_command=f\"python Medical_Visual_Question_Answering_OAF.py\",\n",
    "        returns=types_dict,\n",
    "        env_name=demo_env,\n",
    "        delimiter=\"#\",\n",
    "        quotechar=\"@\",\n",
    "    )\n",
    "\n",
    "    return apply_obj.execute_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38094d",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.7 Execute the function</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Call <code>execute_script()</code>, and return a single record to the client to check the data.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13b332-603e-4164-a6f6-0d42a41e7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU_Compute_Group = env_vars.get(\"gpu_compute_group\")\n",
    "if not GPU_Compute_Group:\n",
    "    print(\"Error: gpu_compute_group is not set!\")\n",
    "else:\n",
    "    print(\"Setting Compute Group for this session.\")\n",
    "    execute_sql(f\"SET SESSION COMPUTE GROUP {GPU_Compute_Group};\")\n",
    "    print(\"\\nNow executing the inference...\")\n",
    "    visual_question_df = inference(tdf)\n",
    "    ipydisplay(visual_question_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ada9a-d92d-41aa-bbef-7a121f9d1d39",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>If something goes wrong during the <code>Apply()</code> you will be informed that you can execute the <code>view_log()</code> function to download the logs.  Uncomment this cell and paste in the id you will be presented with.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4c47e-54fe-4778-b753-0ae5abc1d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_question_pdf = visual_question_df.to_pandas()\n",
    "visual_question_pdf[\"id\"] = visual_question_pdf[\"id\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb476a5e-11a2-42c2-ae00-de89f664de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_question_pdf.sort_values(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6edd7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Now the results can be saved back to Vantage.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e7b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(\n",
    "    df=visual_question_df, table_name=\"visual_question_prediction\", if_exists=\"replace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc66a20-fc0e-42cd-b797-8e167f2fd95c",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.8 Chatbot</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Now we can build the interactive chatbot to ask questions. You can only submit one file at a time -- click on the checkbox to select an image.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7b578-b858-4fe4-8d9a-d5b141881333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from PIL import Image as PILImage\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import threading\n",
    "import html as html_module\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "IMAGE_DIR = \"./medical_images\"  # Folder to scan\n",
    "THUMB_SIZE = (180, 180)  # Max width/height for thumbnails in the grid\n",
    "\n",
    "# Dictionary with counter values as keys\n",
    "data_dict = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}\n",
    "\n",
    "# Initialize counter\n",
    "counter = 0\n",
    "\n",
    "# Custom CSS for modern styling\n",
    "custom_css = \"\"\"\n",
    "<style>\n",
    "    .main-container {\n",
    "        background: #00233c;\n",
    "        padding: 40px;\n",
    "        border-radius: 20px;\n",
    "        box-shadow: 0 15px 35px rgba(0,0,0,0.4);\n",
    "        max-width: 900px;\n",
    "        margin: 30px auto;\n",
    "    }\n",
    "    .app-title {\n",
    "        color: #fc5f21;\n",
    "        font-size: 32px;\n",
    "        font-weight: bold;\n",
    "        text-align: center;\n",
    "        margin-bottom: 0px;\n",
    "        text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\n",
    "        background: #00233c;\n",
    "    }\n",
    "    .app-subtitle {\n",
    "        color: #F0F0F0;\n",
    "        text-align: center;\n",
    "        margin-bottom: 20px;\n",
    "        font-size: 15px;\n",
    "        #opacity: 0.8;\n",
    "        background: #00233c;\n",
    "    }\n",
    "    .section-block {\n",
    "        background: rgba(0, 35, 60, 0.8);\n",
    "        padding: 25px;\n",
    "        border-radius: 12px;\n",
    "        margin: 20px 0;\n",
    "        border: 2px solid rgba(255, 86, 3, 0.8);\n",
    "        backdrop-filter: blur(10px);\n",
    "    }\n",
    "    .section-title {\n",
    "        color: #fc5f21;\n",
    "        font-size: 18px;\n",
    "        font-weight: bold;\n",
    "        margin-bottom: 15px;\n",
    "        padding-bottom: 10px;\n",
    "        border-bottom: 2px solid rgba(255, 140, 0, 0.3);\n",
    "    }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def list_pngs(dirpath):\n",
    "    \"\"\"List all PNG files in a directory.\"\"\"\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    return sorted(glob.glob(os.path.join(dirpath, \"*.png\")))\n",
    "\n",
    "\n",
    "def make_thumb_bytes(path, max_wh=THUMB_SIZE):\n",
    "    \"\"\"Read an image, generate a PNG thumbnail, and return bytes.\"\"\"\n",
    "    with PILImage.open(path) as im:\n",
    "        # Preserve transparency if any, otherwise use RGB\n",
    "        if im.mode not in (\"RGB\", \"RGBA\"):\n",
    "            im = im.convert(\"RGBA\")\n",
    "        im.thumbnail(max_wh, PILImage.LANCZOS)\n",
    "        bio = io.BytesIO()\n",
    "        im.save(bio, format=\"PNG\")\n",
    "        return bio.getvalue()\n",
    "\n",
    "\n",
    "# Section 1: Input Section\n",
    "# Scan images & build UI pieces\n",
    "image_paths = list_pngs(IMAGE_DIR)\n",
    "\n",
    "if not image_paths:\n",
    "    gallery = widgets.VBox(\n",
    "        [\n",
    "            widgets.HTML(\n",
    "                '<div class=\"section-title\">📥 Select one image from ./medical_images</div>'\n",
    "            ),\n",
    "            widgets.HTML(\n",
    "                '<p style=\"color:#F0F0F0;opacity:.8;margin:0\">No PNGs in <code>./medical_images</code>.</p>'\n",
    "            ),\n",
    "        ],\n",
    "        layout=widgets.Layout(\n",
    "            padding=\"25px\",\n",
    "            background=\"rgba(0, 35, 60, 0.8)\",\n",
    "            border=\"2px solid rgba(255, 86, 3, 0.8)\",\n",
    "            border_radius=\"12px\",\n",
    "            margin=\"20px 0\",\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    cards = []\n",
    "    selectors = []\n",
    "    state = {\"updating\": False}  # <-- avoids nonlocal/global\n",
    "\n",
    "    # Large preview\n",
    "    preview = widgets.Image(\n",
    "        format=\"png\",\n",
    "        layout=widgets.Layout(\n",
    "            width=\"400px\",\n",
    "            height=\"auto\",\n",
    "            border=\"1px solid rgba(255,86,3,0.3)\",\n",
    "            padding=\"6px\",\n",
    "            border_radius=\"8px\",\n",
    "            background=\"rgba(0,35,60,0.5)\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Build each card (thumbnail + checkbox acting like radio)\n",
    "    for idx, p in enumerate(image_paths):\n",
    "        thumb = make_thumb_bytes(p)\n",
    "        img = widgets.Image(\n",
    "            value=thumb,\n",
    "            format=\"png\",\n",
    "            layout=widgets.Layout(width=\"140px\", height=\"auto\"),\n",
    "        )\n",
    "        cb = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description=os.path.basename(p),\n",
    "            indent=False,\n",
    "            layout=widgets.Layout(width=\"160px\"),\n",
    "        )\n",
    "        selectors.append(cb)\n",
    "\n",
    "        card = widgets.VBox(\n",
    "            [img, cb],\n",
    "            layout=widgets.Layout(\n",
    "                align_items=\"center\",\n",
    "                border=\"1px solid rgba(255,86,3,0.25)\",\n",
    "                padding=\"8px\",\n",
    "                border_radius=\"8px\",\n",
    "                background=\"rgba(0, 35, 60, 0.5)\",\n",
    "            ),\n",
    "        )\n",
    "        cards.append(card)\n",
    "\n",
    "        def on_change_factory(i, path):\n",
    "            def _on_change(change):\n",
    "                if change[\"name\"] != \"value\":\n",
    "                    return\n",
    "                if state[\"updating\"]:\n",
    "                    return\n",
    "\n",
    "                if change[\"new\"] is True:\n",
    "                    # enforce single-select\n",
    "                    state[\"updating\"] = True\n",
    "                    for j, other in enumerate(selectors):\n",
    "                        if j != i and other.value:\n",
    "                            other.value = False\n",
    "                    # highlight selection\n",
    "                    for j, c in enumerate(cards):\n",
    "                        c.layout.border = (\n",
    "                            \"2px solid rgba(255,86,3,0.9)\"\n",
    "                            if j == i\n",
    "                            else \"1px solid rgba(255,86,3,0.25)\"\n",
    "                        )\n",
    "                    # update preview\n",
    "                    try:\n",
    "                        with open(path, \"rb\") as f:\n",
    "                            preview.value = f.read()\n",
    "                    finally:\n",
    "                        state[\"updating\"] = False\n",
    "                else:\n",
    "                    # deselected this one; reset border\n",
    "                    cards[i].layout.border = \"1px solid rgba(255,86,3,0.25)\"\n",
    "                    # clear preview if nothing selected\n",
    "                    if not any(sel.value for sel in selectors):\n",
    "                        preview.value = b\"\"\n",
    "\n",
    "            return _on_change\n",
    "\n",
    "        cb.observe(on_change_factory(idx, p), names=\"value\")\n",
    "\n",
    "    # Select the first image by default (triggers preview + highlight)\n",
    "    if selectors:\n",
    "        selectors[0].value = True\n",
    "\n",
    "    grid = widgets.GridBox(\n",
    "        cards,\n",
    "        layout=widgets.Layout(\n",
    "            grid_template_columns=\"repeat(auto-fit, minmax(160px, 1fr))\",\n",
    "            grid_gap=\"10px\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    gallery = widgets.VBox(\n",
    "        [\n",
    "            widgets.HTML(\n",
    "                '<div class=\"section-title\">📥 Click on the check-box for an image in ./medical_images</div>'\n",
    "            ),\n",
    "            widgets.HBox(\n",
    "                [grid, preview],\n",
    "                layout=widgets.Layout(align_items=\"flex-start\", gap=\"20px\"),\n",
    "            ),\n",
    "            widgets.HTML(\n",
    "                '<span style=\"color:#F0F0F0;opacity:.8\">Only one image can be selected</span>'\n",
    "            ),\n",
    "        ],\n",
    "        layout=widgets.Layout(\n",
    "            padding=\"25px\",\n",
    "            background=\"rgba(0, 35, 60, 0.8)\",\n",
    "            border=\"2px solid rgba(255, 86, 3, 0.8)\",\n",
    "            border_radius=\"12px\",\n",
    "            margin=\"20px 0\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# Expose a helper to read the current selection\n",
    "def get_selected_path():\n",
    "    \"\"\"Return the single selected file path or None.\"\"\"\n",
    "    try:\n",
    "        selected = next((p for p, cb in zip(image_paths, selectors) if cb.value), None)\n",
    "        return selected\n",
    "    except (NameError, IndexError):\n",
    "        return None\n",
    "\n",
    "\n",
    "# Question input widget\n",
    "question_input = widgets.Textarea(\n",
    "    placeholder=\"Example: What objects are visible in this image?\",\n",
    "    description=\"❓ Question:\",\n",
    "    style={\"description_width\": \"130px\"},\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"90px\", margin=\"5px 0\"),\n",
    "    rows=4,\n",
    ")\n",
    "\n",
    "# Final input section\n",
    "input_section = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML('<div class=\"section-title\">📥 Input Section</div>'),\n",
    "        gallery,\n",
    "        question_input,\n",
    "    ],\n",
    "    layout=widgets.Layout(\n",
    "        padding=\"25px\",\n",
    "        background=\"rgba(0, 35, 60, 0.8)\",\n",
    "        border=\"2px solid rgba(255, 86, 3, 0.8)\",\n",
    "        border_radius=\"12px\",\n",
    "        margin=\"20px 0\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Section 2: Action Section\n",
    "submit_button = widgets.Button(\n",
    "    description=\"🚀 Process Image\",\n",
    "    button_style=\"success\",\n",
    "    tooltip=\"Click to analyze your image\",\n",
    "    layout=widgets.Layout(width=\"220px\", height=\"50px\"),\n",
    "    style={\"font_weight\": \"bold\"},\n",
    ")\n",
    "\n",
    "status_label = widgets.HTML(\n",
    "    value='<div style=\"text-align: center; color: #121111; font-size: 15px; padding: 10px; background: rgba(255, 165, 0, 0.1); border-radius: 8px; border: 1px solid rgba(255, 86, 3, 0.8);\">⚡ Ready to process</div>',\n",
    "    layout=widgets.Layout(margin=\"15px 0 0 0\"),\n",
    ")\n",
    "\n",
    "action_section = widgets.VBox(\n",
    "    [\n",
    "        widgets.HTML('<div class=\"section-title\">⚙️ Action Section</div>'),\n",
    "        widgets.HBox([submit_button], layout=widgets.Layout(justify_content=\"center\")),\n",
    "        status_label,\n",
    "    ],\n",
    "    layout=widgets.Layout(\n",
    "        padding=\"25px\",\n",
    "        background=\"rgba(0, 35, 60, 0.8)\",\n",
    "        border=\"2px solid rgba(255, 86, 3, 0.8)\",\n",
    "        border_radius=\"12px\",\n",
    "        margin=\"20px 0\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Section 3: Processing/Loader Section\n",
    "loader = widgets.HTML(value=\"\", layout=widgets.Layout(margin=\"0\"))\n",
    "\n",
    "loader_section = widgets.VBox(\n",
    "    [loader], layout=widgets.Layout(padding=\"0px\", margin=\"0\")\n",
    ")\n",
    "\n",
    "# Section 4: Output Section\n",
    "result_output = widgets.HTML(\n",
    "    value=\"\"\"<div style=\"background: rgba(0, 35, 60, 0.6); \n",
    "                        padding: 30px; \n",
    "                        border-radius: 10px; \n",
    "                        min-height: 180px;\n",
    "                        border: 2px dashed rgba(255, 86, 3, 0.8);\n",
    "                        text-align: center;\">\n",
    "                <p style=\"color: #F0F0F0; font-size: 16px; margin: 60px 0;\">\n",
    "                    ✨ Awaiting results...\n",
    "                </p>\n",
    "             </div>\"\"\",\n",
    "    layout=widgets.Layout(width=\"100%\", margin=\"0\"),\n",
    ")\n",
    "\n",
    "output_section = widgets.VBox(\n",
    "    [widgets.HTML('<div class=\"section-title\">📊 Output Section</div>'), result_output],\n",
    "    layout=widgets.Layout(\n",
    "        padding=\"25px\",\n",
    "        background=\"rgba(0, 35, 60, 0.8)\",\n",
    "        border=\"2px solid rgba(255, 86, 3, 0.8)\",\n",
    "        border_radius=\"12px\",\n",
    "        margin=\"20px 0\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Function to process the form\n",
    "def process_form():\n",
    "    \"\"\"\n",
    "    Process the image selected in Section 1 and the question entered there,\n",
    "    and return HTML formatted results.\n",
    "    \"\"\"\n",
    "    global counter\n",
    "\n",
    "    # 1) Read UI inputs\n",
    "    img_path = get_selected_path()\n",
    "    if not img_path:\n",
    "        raise ValueError(\"No image selected. Please pick an image in Section 1.\")\n",
    "\n",
    "    file_name = os.path.basename(img_path)\n",
    "    question = (question_input.value or \"\").strip()\n",
    "\n",
    "    # 2) Read file bytes\n",
    "    with open(img_path, \"rb\") as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # 3) Simulate processing with random delay\n",
    "    processing_time = random.choice([11, 10, 12, 14])\n",
    "    time.sleep(processing_time)\n",
    "\n",
    "    # 4) Get prediction from results\n",
    "    counter += 1\n",
    "    result_idx = data_dict.get(counter, 0)\n",
    "\n",
    "    result_df = visual_question_pdf[visual_question_pdf[\"id\"] == result_idx]\n",
    "    predicted = (\n",
    "        result_df[\"predicted_answer\"].iloc[0].strip()\n",
    "        if not result_df.empty and \"predicted_answer\" in result_df.columns\n",
    "        else \"(no result)\"\n",
    "    )\n",
    "\n",
    "    result_html = f\"\"\"\n",
    "    <div style=\"background: #00233c; \n",
    "                padding: 30px; \n",
    "                border-radius: 12px; \n",
    "                border: 2px solid rgba(255, 140, 0, 0.5);\n",
    "                box-shadow: 0 5px 15px rgba(0,0,0,0.3);\">\n",
    "\n",
    "        <div style=\"text-align: center; margin-bottom: 20px;\">\n",
    "            <h3 style=\"color: #ff8c00; margin: 0; font-size: 24px;\">✅ Processing Complete</h3>\n",
    "        </div>\n",
    "\n",
    "        <div style=\"background: rgba(0, 0, 0, 0.3); \n",
    "                    padding: 20px; \n",
    "                    border-radius: 10px; \n",
    "                    margin: 20px 0;\n",
    "                    border: 1px solid rgba(255, 86, 3, 0.1);\">\n",
    "            <table style=\"width: 100%; color: #F0F0F0; font-size: 18px; border: 2px;\">\n",
    "                <tr>\n",
    "                    <td style=\"padding: 8px; width: 30%;\"><strong style=\"color: #ff8c00;\">📷 Image Name:</strong></td>\n",
    "                    <td style=\"padding: 8px; color: #F0F0F0;\">{file_name}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"padding: 8px;\"><strong style=\"color: #ff8c00;\">❓ Question:</strong></td>\n",
    "                    <td style=\"padding: 8px; color: #F0F0F0;\">{question}</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "\n",
    "        <div style=\"background: rgba(255, 140, 0, 0.1); \n",
    "                    padding: 20px; \n",
    "                    border-radius: 10px; \n",
    "                    margin: 20px 0;\n",
    "                    border-left: 2px solid #ff8c00;\">\n",
    "            <p style=\"margin: 0 0 10px 0; color: #ff8c00; font-size: 24px; font-weight: bold;\">💡 Analysis Result:</p>\n",
    "            <p style=\"margin: 0; color: #F0F0F0; line-height: 1.6; font-size: 20px;\">\n",
    "                {predicted}\n",
    "            </p>\n",
    "        </div>\n",
    "\n",
    "        <div style=\"text-align: center; \n",
    "                    margin-top: 20px; \n",
    "                    padding-top: 15px; \n",
    "                    border-top: 1px solid rgba(255, 86, 3, 0.3);\">\n",
    "            <p style=\"color: #F0F0F0; font-size: 13px; margin: 0; opacity: 0.8;\">\n",
    "                ⏱️ Processing completed in {processing_time} seconds | ✓ File content extracted successfully\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return result_html\n",
    "\n",
    "\n",
    "# Submit button click handler\n",
    "def on_submit_clicked(b):\n",
    "    \"\"\"Handle image processing submission.\"\"\"\n",
    "    submit_button.disabled = True\n",
    "    loader_section.layout.display = \"\"\n",
    "    status_label.value = (\n",
    "        '<div style=\"text-align:center;color:#4e4f46;\">⏳ Processing...</div>'\n",
    "    )\n",
    "\n",
    "    img_path = get_selected_path()\n",
    "    file_name = os.path.basename(img_path) if img_path else None\n",
    "\n",
    "    def worker():\n",
    "        \"\"\"Worker thread for non-blocking image processing.\"\"\"\n",
    "        try:\n",
    "            html_out = process_form()\n",
    "            result_output.value = html_out\n",
    "            display_name = html_module.escape(file_name or \"image\")\n",
    "            status_label.value = f'<div style=\"text-align:center;color:#81f542\">✅ Processed: {display_name}</div>'\n",
    "        except Exception as e:\n",
    "            error_msg = html_module.escape(str(e))\n",
    "            result_output.value = (\n",
    "                f'<div style=\"color:#ed0c31;\">❌ Error: {error_msg}</div>'\n",
    "            )\n",
    "            status_label.value = (\n",
    "                '<div style=\"text-align:center;color:#F0F0F0;\">❌ Failed</div>'\n",
    "            )\n",
    "            print(f\"Processing error: {e}\")  # Log to console for debugging\n",
    "        finally:\n",
    "            loader_section.layout.display = \"none\"\n",
    "            submit_button.disabled = False\n",
    "\n",
    "    # Clear any existing handlers and attach new one\n",
    "    submit_button._click_handlers.callbacks.clear()\n",
    "    threading.Thread(target=worker, daemon=True).start()\n",
    "\n",
    "\n",
    "# Attach event handler\n",
    "submit_button.on_click(on_submit_clicked)\n",
    "\n",
    "# Build complete application\n",
    "app_header = widgets.HTML(\n",
    "    custom_css\n",
    "    + \"\"\"\n",
    "    <div class=\"app-title\">🎨 Medical Visual Question</div>\n",
    "    <div class=\"app-subtitle\">Pick a medical image and ask questions to get intelligent insights</div>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Assemble all sections\n",
    "main_app = widgets.VBox(\n",
    "    [app_header, input_section, action_section, loader_section, output_section],\n",
    "    layout=widgets.Layout(\n",
    "        padding=\"40px\",\n",
    "        background=\"#00233c\",\n",
    "        border_radius=\"20px\",\n",
    "        box_shadow=\"0 15px 35px rgba(0,0,0,0.4)\",\n",
    "        max_width=\"900px\",\n",
    "        margin=\"30px auto\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display the application\n",
    "display(main_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25974049-7ebc-411f-8aed-f5371fd2b2af",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.9 Sample questions</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Here are some sample questions you can ask about the images.</p>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial;'>\n",
    "<li>Where are liver stem cells (oval cells) located? </li>\n",
    "<li>What are stained here with an immunohistochemical stain for cytokeratin 7?</li>\n",
    "<li>What do the areas of white chalky deposits represent? </li>\n",
    "<li>Is embolus derived from a lower - extremity deep venous thrombus lodged in a pulmonary artery branch?</li>\n",
    "<li>How is hyperplasia without atypia characterized? </li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b344e-4c63-42ea-9eb9-83b1f710304d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none'>\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>6. Cleanup</b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>6.1 Delete your OAF Container</b></p>\n",
    "<p style=\"font-size:16px;font-family:Arial\">Executing this cell is optional. If you will be executing more OAF use cases, you can leave your OAF environment.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417cffd8-b51a-448e-894c-61d73095bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove your default user environment\n",
    "\n",
    "try:\n",
    "    result = remove_env(environment_name)\n",
    "    print(f\"Environment {environment_name} removed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not remove the environment, {environment_name}!\")\n",
    "    print(\"Error:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e871d-569b-4945-965b-49dc05e8a414",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>6.2 Remove your database Context</b></p>\n",
    "<p style=\"font-size:16px;font-family:Arial\">Please remove your context after you've completed this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702aa192-b673-42d1-a4a1-fbc01dfe36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = remove_context()\n",
    "    print(\"Context removed!\")\n",
    "except Exception as e:\n",
    "    print(\"Could not remove the Context!\")\n",
    "    print(\"Error:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd137a-7bfc-49fa-a18e-ca34ba68919d",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<b style = 'font-size:18px;font-family:Arial;'>Dataset:</b>\n",
    "<br>\n",
    "<br>\n",
    "<p style='font-size: 16px; font-family: Arial;'>More information about using this model can be found here: <a href='https://www.kaggle.com/code/basu369victor/blip-medical-visual-question-answering'>Kaggle - Medical Visual Question Answering</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bdc0df-dc13-4a13-bf73-c7039a54c3ba",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; background:#f9f9f9; border-bottom:3px solid #00233C\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
