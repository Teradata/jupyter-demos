{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce3dd8a-f3e5-40d5-ab4c-d40cc358cf7f",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Medical Visual Question Answering using Teradata VantageCloud and open-source language models\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57054f7e-befe-4029-b4ac-b82019240b1f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial;'><b>Introduction:</b></p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial;'>In this comprehensive user demo, we will delve into the world of Medical Visual Question Answering using <b>Teradata Vantage</b> and <b>open-source language models</b>. This cutting-edge technology empowers businesses to uncover hidden insights from vast amounts of consumer complaints data, enabling them to identify trends, improve customer satisfaction, and enhance their overall brand reputation.</p> \n",
    "\n",
    "<center>![intro](https://imageio.forbes.com/specials-images/imageserve/636063ae49e46108de0472a1/Medical-technology-concept--Remote-medicine--Electronic-medical-record-/960x0.jpg)</center>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\"><b>Key Features:</b></p>\n",
    "\n",
    "<ol style=\"font-size:16px;font-family:Arial;\">\n",
    "<li><b>Improved Clinical Decision Support:</b> A well-trained medical VQA model enhances clinical decision-making by allowing healthcare providers to ask questions about medical images (e.g., X-rays, MRIs, CT scans) and receive accurate, rapid answers. This can lead to faster diagnoses and treatment plans.</li>\n",
    "\n",
    "<li><b>Reducing Interpretation Errors:</b> Human interpretation of medical images can be subjective and prone to errors. A VQA model can provide objective, consistent, and evidence-based interpretations, helping to reduce diagnostic inaccuracies.</li>\n",
    "\n",
    "<li><b>Time Efficiency:</b> The model's ability to quickly analyze images and answer questions can save valuable time for healthcare professionals, leading to more efficient patient care.</li>\n",
    "\n",
    "<li><b>Accessibility:</b> Patients and non-specialist healthcare providers can benefit from a medical VQA system by obtaining easy-to-understand information about their health conditions, potentially improving health literacy.</li>\n",
    "\n",
    "<li><b>Learning and Training Aid:</b> Medical VQA models can serve as educational tools for medical students, residents, and even experienced practitioners. They can be used to explain complex medical concepts and imaging findings.</li>\n",
    "\n",
    "<li><b>Research Assistance:</b> Researchers can leverage the model to analyze large datasets of medical images more effectively. It can assist in extracting meaningful insights from these datasets, potentially leading to new discoveries in medical science.</li>\n",
    "\n",
    "<li><b>Cross-Specialty Applicability:</b> A well-designed medical VQA model can be adapted to various medical specialties, from radiology and pathology to cardiology and dermatology. This versatility makes it a valuable asset across different healthcare domains.</li>\n",
    "\n",
    "<li><b>Ethical Considerations:</b> It's essential to address ethical concerns related to privacy, security, and bias when deploying medical VQA models in healthcare settings. Ensuring patient data protection and model fairness is critical.</li>\n",
    "\n",
    "<li><b>Continuous Improvement:</b> Model performance and accuracy should be continuously monitored and improved over time. Regular updates and retraining are necessary to keep up with evolving medical knowledge and technologies.</li>\n",
    "\n",
    "<li><b>Collaboration:</b> Successful implementation of medical VQA models often requires collaboration between machine learning experts, healthcare professionals, and ethicists to ensure that the technology is used responsibly and effectively.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6863ef",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'><b>Visual Question Answering:</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'><b>What is visual Question Answering?</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Visual Question Answering (VQA) is a task in computer vision that involves answering questions about an image. The goal of VQA is to teach machines to understand the content of an image and answer questions about it in natural language.</p>\n",
    "\n",
    "<center><img id=\"125\" src=\"./images/header_scene.png\" height=\"400px\" width=\"600px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7068302",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'><b>Steps in the analysis:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial;'>\n",
    "     <li>Configuring the environment</li>\n",
    "  <li>Connect to Vantage</li>\n",
    "  <li>Create a Custom Container in Vantage</li>\n",
    "  <li>Install Dependencies</li>\n",
    "  <li>Operationalizing AI-powered analytics</li>\n",
    "  <li>Topic Modelling</li>\n",
    "  <li>Cleanup</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e457db5-d4df-4e7f-a11a-fcb1416e462d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial;'>1. Configuring the environment</b>\n",
    "\n",
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>1.1 Install the required libraries</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb4587-6632-4b08-a121-c98ea5a8fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca175a-80bf-4d7f-ab9e-79403af74745",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install teradataml==20.0.0.7 teradatamlwidgets==20.0.0.6 teradatamodelops==7.0.3 teradatasql==20.0.0.34 teradatasqlalchemy==20.0.0.7 sentencepiece sentence-transformers wordcloud --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd81e4-0df9-4360-b3ac-72214f135296",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial;'><b>Note: </b><i>Please restart the kernel after executing these two lines. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc278740-1511-4423-bf2b-92a5109a47bf",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>1.2 Import the required libraries</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f183178-9807-4538-b794-922820346ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import *\n",
    "from teradatasqlalchemy.types import *\n",
    "from time import sleep\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv, sys, os, warnings\n",
    "from os.path import expanduser\n",
    "from collections import OrderedDict\n",
    "from IPython.display import clear_output , display as ipydisplay\n",
    "import matplotlib.pyplot as plt\n",
    "# from itables import init_notebook_mode\n",
    "# import itables.options as opt\n",
    "from dotenv import load_dotenv\n",
    "# Set display options for dataframes, plots, and warnings\n",
    "\n",
    "# import utils for lake environment\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..', '..','config'))\n",
    "sys.path.append(module_path)\n",
    "from utils.oaf_utils import *\n",
    "\n",
    "# opt.style=\"table-layout:auto;width:auto;float:left\"\n",
    "# opt.columnDefs = [{\"className\": \"dt-left\", \"targets\": \"_all\"}]\n",
    "# init_notebook_mode(all_interactive=True)\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "display.suppress_vantage_runtime_warnings = True\n",
    "\n",
    "python_version = \"3.10\"\n",
    "print(f'Using Python version {python_version} for user environment')\n",
    "\n",
    "\n",
    "# Hugging Face model for the demo\n",
    "model_name = 'ChetanHirapara/Salesforce-blip-vqa-base-medical-data'\n",
    "\n",
    "# a list of required packages to install in the custom OAF container\n",
    "# modify this if using different models or design patterns\n",
    "pkgs = ['transformers',\n",
    "        'torch',\n",
    "        'pandas',\n",
    "        'sentence-transformers']\n",
    "# container name - set here for easier notebook navigation\n",
    "### User will also be asked to change it ###\n",
    "# oaf_name = 'oaf_topic_classification'\n",
    "oaf_name = 'oaf_demo_gpu'\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c5e2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1763fd95-5000-4a2d-8b86-7be261e20847",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial;'>2. Connect to Vantage</b>\n",
    "\n",
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>2.1 Connect to Vantage</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>After connecting, check cluster status. Start it if necessary - note the cluster only needs to be running to execute the APPLY sections of the demo.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5eba56-d38f-4204-b30d-232c7d894eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating the context...\") \n",
    "load_dotenv(\"../../.config/.env\", override=True)\n",
    "host = os.getenv(\"host\")\n",
    "username = os.getenv(\"username\")\n",
    "my_variable = os.getenv(\"my_variable\")\n",
    "\n",
    "eng = create_context(host=host, username=username, password=my_variable)\n",
    "execute_sql('''SET query_band='DEMO=Complaint_Analysis_Customer360_VCL.ipynb;' UPDATE FOR SESSION;''')\n",
    "print(\"Connected to Teradata:\", eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce81699-6345-4dbf-a3a8-b9402c7b6a98",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Begin running steps with Shift + Enter keys. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca7332-48f5-4702-8b56-d8a03fbb87a2",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>2.2  Connect to the Environment Service</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>To better support integration with Cloud Services and common automation tools; the <b > User Environment Service</b> is accessed via RESTful APIs.  These APIs can be called directly or in the examples shown below that leverage the Python Package for Teradata (teradataml) methods.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e52db1-8126-4219-b6b5-def5f242ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/home/jovyan/JupyterLabRoot/VantageCloud_Lake/.config/.env\"):\n",
    "    print(\"Your environment parameter file exist.  Please proceed with this use case.\")\n",
    "    # Load all the variables from the .env file into a dictionary\n",
    "    env_vars = dotenv_values(\"/home/jovyan/JupyterLabRoot/VantageCloud_Lake/.config/.env\")\n",
    "else:\n",
    "    print(\"Your environment has not been prepared for connecting to VantageCloud Lake.\")\n",
    "    print(\"Please contact the support team.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b5394-f9c3-4d2b-b045-70cc2607e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_analytics_endpoint = os.getenv(\"ues_uri\")\n",
    "access_token = os.getenv(\"access_token\")\n",
    "pem_file = os.getenv(\"pem_file\")\n",
    "compute_group = os.getenv(\"gpu_compute_group\")\n",
    "\n",
    "if set_auth_token(base_url=env_vars.get(\"ues_uri\"),\n",
    "                  pat_token=env_vars.get(\"access_token\"), \n",
    "                  pem_file=env_vars.get(\"pem_file\"),\n",
    "                  valid_from=int(time.time())\n",
    "                 ):\n",
    "    print(\"UES Authentication successful\")\n",
    "else:\n",
    "    print(\"UES Authentication failed. Check credentials.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a22c6e-453c-446d-8d7b-f5eb50f032a2",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>After connecting and authenticating, check cluster status. Start it if necessary - note the cluster only needs to be running to execute the APPLY sections of the demo.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69e0b4-6c0c-4753-a1f2-c309767dc6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_sql(f\"SET SESSION COMPUTE GROUP {compute_group};\")\n",
    "res = check_cluster_start(compute_group=compute_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5a248-f430-48ac-82f1-9eb2a0311d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_user_envs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8a61e",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<b style = 'font-size:18px;font-family:Arial;'>3. Create a Custom Container in Vantage</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>If desired, the user can create a <b>new</b> custom environment by starting with a \"base\" image and customizing it.  The steps are:</p> \n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li>List the available \"base\" images the system supports</li>\n",
    "    <li>List any existing \"custom\" environments the user has created</li>\n",
    "    <li>If there are no custom environments, then create a new one from a base image</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new environment, or connect to an existing one\n",
    "try:\n",
    "    ipydisplay(list_user_envs())\n",
    "except Exception as e:\n",
    "    if str(e).find(\"No user environments found\") > 0:\n",
    "        print(\"No user environments found\")\n",
    "        pass\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(\"Use an existing environment, or create a new one:\")\n",
    "print(f\"OAF Environment is set to {oaf_name}.\")\n",
    "print(\"Enter to accept, or input a new value.\")\n",
    "print(\"If the environment is not in the list, an new one will be created\")\n",
    "i = oaf_name\n",
    "if len(i) != 0:\n",
    "    oaf_name = i\n",
    "    print(f\"OAF Environment is now {oaf_name}\")\n",
    "\n",
    "try:\n",
    "    demo_env = create_env(\n",
    "        env_name=oaf_name,\n",
    "        base_env=f\"python_{python_version}\",\n",
    "        desc=\"OAF Demo env for LLM\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    if str(e).find(\"same name already exists\") > 0:\n",
    "        print(\"Environment already exists, obtaining a reference to it\")\n",
    "        demo_env = get_env(oaf_name)\n",
    "        pass\n",
    "    elif \"Invalid value for base environment name\" in str(e):\n",
    "        print(\"Unsupported base environment version, using defaults\")\n",
    "        demo_env = create_env(env_name=oaf_name, desc=\"OAF Demo env for LLM\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Note create_env seems to be asynchronous - sleep a bit for it to register\n",
    "sleep(5)\n",
    "\n",
    "try:\n",
    "    ipydisplay(list_user_envs())\n",
    "except Exception as e:\n",
    "    if str(e).find(\"No user environments found\") > 0:\n",
    "        print(\"No user environments found\")\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556a860-e27f-432d-a4d1-f4bd903e34fd",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>4. Install Dependencies</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The second step in the customization process is to install Python package dependencies. This demonstration uses the Hugging Face <a href = 'https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english'>distilbert-base-uncased-finetuned-sst-2-english</a> Sentence Transformer.  Since VantageCloud Lake Analytic Clusters are secured by default against unauthorized access to the outside network, the user can load the required libraries and model using teradataml methods:\n",
    "</p> \n",
    "\n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li>List the currently installed models and python libraries</li>\n",
    "    <li><b>If necessary</b>, install any required packages</li>\n",
    "    <li><b>If necessary</b>, install the pre-trained model.  This process takes several steps;\n",
    "        <ol style = 'font-size:16px;font-family:Arial;'>\n",
    "            <li>Import and download the model</li>\n",
    "            <li>Create a zip archive of the model artifacts</li>\n",
    "            <li>Call the install_model() method to load the model to the container</li>\n",
    "        </ol></li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2a8a6-5a5b-483d-81e7-46682c764bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipydisplay(demo_env.models)\n",
    "\n",
    "ipydisplay(demo_env.libs.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b179f-4649-483a-a470-db064266c3b6",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>4.1 A note on package versions</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The next demonstration makes use of the DataFrame apply() method, which automatically passes the python code to the Analytic Cluster.  As such, one needs to ensue the python package versions match.  dill and pandas are required, as is any additional libraries for the use case.\n",
    "</p> \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'><b>Note</b> while not required for many OAF use cases, for this demo the required packages for the model execution must be installed in the local environment first.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6feda0d-d9da-4e57-9fdb-00635c6bdf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if these packages need to be installed\n",
    "# by comparing the len of the intersection of the list of required packages with the installed ones\n",
    "if not len(\n",
    "    set([x.split(\"==\")[0] for x in pkgs]).intersection(demo_env.libs[\"name\"].to_list())\n",
    ") == len(pkgs):\n",
    "\n",
    "    # pass the list of packages - split off any extra info from the version property e.g., plus sign\n",
    "    claim_id = demo_env.install_lib([\"transformers\", \"torch\", \"pandas\", \"sentence-transformers\"], asynchronous=True)\n",
    "else:\n",
    "    print(f\"All required packages are installed in the {oaf_name} environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d186ca1",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>4.2 Monitor library installation status</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Optionally - users can monitor the library installation status using the cell below:\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d533b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of installation using status() API.\n",
    "# Create a loop here for demo purposes\n",
    "try:\n",
    "    claim_id\n",
    "    ipydisplay(demo_env.status(claim_id))\n",
    "    stage = demo_env.status(claim_id)[\"Stage\"].iloc[-1]\n",
    "    while stage == \"Started\":\n",
    "        stage = demo_env.status(claim_id)[\"Stage\"].iloc[-1]\n",
    "        clear_output()\n",
    "        ipydisplay(demo_env.status(claim_id))\n",
    "        sleep(5)\n",
    "except NameError:\n",
    "    print(\"No installations to monitor\")\n",
    "\n",
    "\n",
    "# Verify the Python libraries have been installed correctly.\n",
    "ipydisplay(demo_env.libs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162969f",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>4.3 Download and install model</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Open Analytics Framework containers do not have open access to the external network, which contributes to a very secure runtime environment.  As such, users will load pre-trained models using the below APIs.  For illustration purposes, the following code will check to see if the model archive exists locally and if it doesn't, will import and download it by creating a model object.  The archive will then be created and installed into the remote environment.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8307e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    demo_env.install_model(\n",
    "        model_name=model_name, model_type=\"HF\"\n",
    "    )  # Added new arguments model_name, model_type and api_key to support installation of models from external model registry like HuggingFace .\n",
    "except Exception as e:\n",
    "    if \"Model with the same name already exists in the user environment\" in str(e):\n",
    "        print(\"Model already exists, skipping installation\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52abfa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_env.models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6eb89",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>The preceding demo showed how users can perform a <b>one-time</b> configuration task to prepare a custom environment for analytic processing at scale.  Once this configuration is complete, these containers can be re-used in ad-hoc development tasks, or used for operationalizing analytics in production.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3147145",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf0993",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>5. Operationalizing AI-powered analytics</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The following demonstration will illustrate how developers can take the next step in the process to <b>operationalize</b> this processing, enabling the entire organization to leverage AI across the data lifecycle, including</p>\n",
    "\n",
    "<table style = 'width:100%;table-layout:fixed;'>\n",
    "    <tr>\n",
    "        <td style = 'vertical-align:top' width = '30%'>\n",
    "           <ol style = 'font-size:16px;font-family:Arial;'>\n",
    "               <li><b>Prepare the environment</b>.  Package the scoring function into a more robust program, and stage it on the remote environment</li>\n",
    "            <br>\n",
    "            <br>\n",
    "               <li><b>Python Pipeline</b>.  Execute the function using Python methods</li>\n",
    "            <br>\n",
    "            <br>\n",
    "               <li><b>SQL Pipeline</b>.  Execute the function using SQL - allowing for broad adoption and use in ETL and operational needs</li>\n",
    "        </ol>\n",
    "        </td>\n",
    "        <td width = '20%'></td>\n",
    "        <td style = 'vertical-align:top'><img src = 'images/OAF_Ops.png' width=350></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.1 Check connection</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Reconnect to the database, UES, and start cluster if necessary<get_context()/p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88020f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for existing connection and connect.\n",
    "eng = check_and_connect(\n",
    "    host=host, username=username, password=my_variable, compute_group=compute_group\n",
    ")\n",
    "print(eng)\n",
    "\n",
    "# check to see if there is a valid UES auth\n",
    "if set_auth_token(\n",
    "    base_url=env_vars.get(\"ues_uri\"),\n",
    "    pat_token=env_vars.get(\"access_token\"),\n",
    "    pem_file=env_vars.get(\"pem_file\"),\n",
    "    valid_from=int(time.time())\n",
    "):\n",
    "    print(\"UES Authentication successful\")\n",
    "else:\n",
    "    print(\"UES Authentication failed. Check credentials.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Get environment\n",
    "demo_env = get_env(oaf_name)\n",
    "\n",
    "# Check cluster status\n",
    "check_cluster_start(compute_group=compute_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c15f1",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.2 Create a server-side Visual inference function</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The goal of this exercise is to create a <b>server-side</b> function which can be staged on the analytic cluster.  This offers many improvements over the method used above;</p> \n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li><b>Performance</b>.  Staging the code and dependencies in the container environment reduces the amount of I/O, since the function doesn't need to get serialized to the cluster when called</li>\n",
    "    <li><b>Operationalization</b>.  The execution pipeline can be encapsulated into a SQL statement, which allows for seamless use in ETL pipelines, dashboards, or applications that need access</li>\n",
    "    <li><b>Flexibility</b>. Developers can express much greater flexibility in how the code works to optimize for performance, stability, data cleanliness or flow logic</li>\n",
    "</ul>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>These benefits do come with some amount of additional work.  Developers need to account how data is passed in and out of the code runtime, and how to pass it back to the SQL engine to assemble and return the final resultset.  Code is executed when the user expresses an <a href = 'https://docs.teradata.com/r/Teradata-VantageCloud-Lake/SQL-Reference/SQL-Operators-and-User-Defined-Functions/Table-Operators/APPLY'>APPLY SQL function</a>;</p> \n",
    "<ol style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li><b>Input Query</b>.  The APPLY function takes a SQL query as input.  This query can be as complex as needed and include data preparation, cleansing, and/or any other set-based logic necessary to create the desired input data set.  This complexity can also be abstracted into a database view.  When using the teradata client connectors for Python or R, thise query is represented as a DataFrame or tibble.</li>\n",
    "    <li><b>Pre-processing</b>.  Based on the query plan, data is retrieved from storage (cache, block storage, or object storage) and the input query is executed.</li>\n",
    "    <li><b>Distribution</b>.  Input data can be partitioned and/or ordered to be processed on a specific container or collection of them.  For example, the user may want to process all data for a single post code in one partition, and run thousands of these in parallel.  Data can also be distributed evenly across all units of parallelism in the system</li>\n",
    "    <li><b>Input</b>.  The data for each container is passed to the runtime using tandard input (stdin)</li>\n",
    "    <li><b>Processing</b>.  The user's code executes, parsing stdin for the input data</li>\n",
    "    <li><b>Output</b>.  Data is sent out of the code block using standard output (stdout)</li>\n",
    "    <li><b>Resultset</b>.  Resultset is assembled by the analytic database, and the SQL query returns</li>\n",
    "    </ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b2628",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.3 Example server-side code block</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>This is the python script used in the demonstration.  It is saved to the filesystem as <code>Medical_Visual_Question_Answering_OAF.py</code>.  Note here the original client-side processing function has been reused, and the additional logic is for input, output, and error handling.</p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae6f4e",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.4.  Install the file and any additional artifacts</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Use the install_file() method to install this python file to the container.  As a reminder, this container is persistent, so these steps need only be done infrequently.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b3b766-ea12-4912-8d9b-37f584533e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Medical_Visual_Question_Answering_OAF.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Sept 29 18:54:17 2025\n",
    "\n",
    "@author: author\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForQuestionAnswering,\n",
    "    BlipImageProcessor,\n",
    "    BlipConfig,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "def load_input_data():\n",
    "    \"\"\"Load and parse input data from stdin\"\"\"\n",
    "    delimiter = \"#\"\n",
    "    inputData = []\n",
    "    print(\"Reading input data...\", file=sys.stderr)\n",
    "    for line in sys.stdin.read().splitlines():\n",
    "        line = line.split(delimiter)\n",
    "        inputData.append(line)\n",
    "\n",
    "    if not inputData:\n",
    "        sys.exit()\n",
    "\n",
    "    columns = [\"id\", \"img\", \"question\", \"answer\"]\n",
    "    pdf = pd.DataFrame(inputData, columns=columns).copy()\n",
    "\n",
    "    return pdf\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    \"\"\"Initialize BLIP model components\"\"\"\n",
    "    try:\n",
    "        print(\"Loading BLIP model components...\", file=sys.stderr)\n",
    "        model_path = \"ChetanHirapara/Salesforce-blip-vqa-base-medical-data\"\n",
    "\n",
    "        print(f\"---- Loading model from: ---- {model_path}\", file=sys.stderr)\n",
    "        config = BlipConfig.from_pretrained(model_path)\n",
    "        text_processor = BlipProcessor.from_pretrained(model_path)\n",
    "        image_processor = BlipImageProcessor.from_pretrained(model_path)\n",
    "        model = BlipForQuestionAnswering.from_pretrained(model_path)\n",
    "        print(\"---- Model loaded successfully. ----\", file=sys.stderr)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        return model, text_processor, image_processor, device\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, text_processor, image_processor):\n",
    "        self.data = data\n",
    "        self.text_processor = text_processor\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = 32\n",
    "        self.image_height = 128\n",
    "        self.image_width = 128\n",
    "\n",
    "        if hasattr(data, \"iloc\"):\n",
    "            self.questions = data[\"question\"].tolist()\n",
    "            self.answers = data[\"answer\"].tolist()\n",
    "            self.images = data[\"img\"].tolist()\n",
    "        else:\n",
    "            self.questions = data[\"question\"]\n",
    "            self.answers = data[\"answer\"]\n",
    "            self.images = data[\"img\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def _decode_image(self, img_data):\n",
    "        \"\"\"Decode image from base64 or binary data\"\"\"\n",
    "        try:\n",
    "            if isinstance(img_data, str):\n",
    "                img_bytes = base64.b64decode(img_data)\n",
    "            else:\n",
    "                img_bytes = img_data\n",
    "\n",
    "            image = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
    "            return image\n",
    "        except Exception:\n",
    "            return Image.new(\"RGB\", (224, 224), color=\"white\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        answers = self.answers[idx]\n",
    "        questions = self.questions[idx]\n",
    "        image = self._decode_image(self.images[idx])\n",
    "        text = self.questions[idx]\n",
    "\n",
    "        image_encoding = self.image_processor(\n",
    "            image,\n",
    "            do_resize=True,\n",
    "            size=(self.image_height, self.image_width),\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        encoding = self.text_processor(\n",
    "            None,\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        encoding[\"pixel_values\"] = image_encoding[\"pixel_values\"][0]\n",
    "\n",
    "        labels = self.text_processor.tokenizer.encode(\n",
    "            answers,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[0]\n",
    "        encoding[\"labels\"] = labels\n",
    "\n",
    "        return encoding\n",
    "\n",
    "\n",
    "def inference_single(\n",
    "    input_df,\n",
    "    model,\n",
    "    text_processor,\n",
    "    image_processor,\n",
    "    device,\n",
    "    sample_idx=0,\n",
    "    visualize=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform Visual Question Answering inference on a single sample\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_df : pd.DataFrame\n",
    "        DataFrame containing columns: 'question', 'answer', 'img'\n",
    "    model : BlipForQuestionAnswering\n",
    "        Pre-loaded BLIP model\n",
    "    text_processor, image_processor : BLIP processors\n",
    "    device : torch.device\n",
    "        Device for inference\n",
    "    sample_idx : int, default=0\n",
    "        Index of the sample to process\n",
    "    visualize : bool, default=False\n",
    "        Whether to display the input image\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Inference results\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"Starting inference...\", file=sys.stderr)\n",
    "    try:\n",
    "        required_columns = [\"question\", \"answer\", \"img\"]\n",
    "        missing_columns = [\n",
    "            col for col in required_columns if col not in input_df.columns\n",
    "        ]\n",
    "\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "        if len(input_df) == 0 or sample_idx >= len(input_df):\n",
    "            raise ValueError(\"Invalid sample index or empty DataFrame\")\n",
    "\n",
    "        val_vqa_dataset = VQADataset(\n",
    "            data=input_df,\n",
    "            text_processor=text_processor,\n",
    "            image_processor=image_processor,\n",
    "        )\n",
    "\n",
    "        sample = val_vqa_dataset[sample_idx]\n",
    "\n",
    "        question_text = text_processor.decode(\n",
    "            sample[\"input_ids\"], skip_special_tokens=True\n",
    "        )\n",
    "        actual_answer = text_processor.decode(\n",
    "            sample[\"labels\"], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        sample_batch = {k: v.unsqueeze(0).to(device) for k, v in sample.items()}\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inference_start = time.time()\n",
    "            outputs = model.generate(\n",
    "                pixel_values=sample_batch[\"pixel_values\"],\n",
    "                input_ids=sample_batch[\"input_ids\"],\n",
    "                max_length=50,\n",
    "                num_beams=3,\n",
    "                early_stopping=True,\n",
    "                do_sample=False,\n",
    "            )\n",
    "            inference_time = time.time() - inference_start\n",
    "\n",
    "        predicted_answer = text_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        results = {\n",
    "            \"question\": question_text,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"actual_answer\": actual_answer,\n",
    "            \"sample_index\": sample_idx,\n",
    "            \"processing_time\": time.time() - start_time,\n",
    "            \"inference_time\": inference_time,\n",
    "            \"device_used\": str(device),\n",
    "        }\n",
    "\n",
    "        if visualize:\n",
    "            try:\n",
    "                image_mean = image_processor.image_mean\n",
    "                image_std = image_processor.image_std\n",
    "\n",
    "                unnormalized_image = (\n",
    "                    sample_batch[\"pixel_values\"][0].cpu().numpy()\n",
    "                    * np.array(image_std)[:, None, None]\n",
    "                ) + np.array(image_mean)[:, None, None]\n",
    "                unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "                unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "\n",
    "                # plt.figure(figsize=(10, 8))\n",
    "                # plt.imshow(Image.fromarray(unnormalized_image))\n",
    "                # plt.axis(\"off\")\n",
    "\n",
    "                title = f\"Visual Question Answering Results\\n\"\n",
    "                title += f\"Q: {question_text}\\n\"\n",
    "                title += f\"Predicted: {predicted_answer}\\n\"\n",
    "                title += f\"Actual: {actual_answer}\"\n",
    "\n",
    "                # plt.title(title, fontsize=12, pad=20)\n",
    "                # plt.tight_layout()\n",
    "                # plt.show()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_batch_inference(pdf, model, text_processor, image_processor, device):\n",
    "    \"\"\"Process all rows in the DataFrame and return results\"\"\"\n",
    "    results = []\n",
    "    print(\"Processing batch inference...\", file=sys.stderr)\n",
    "    for index, row in pdf.iterrows():\n",
    "        try:\n",
    "            single_row_df = pd.DataFrame([row])\n",
    "            result = inference_single(\n",
    "                single_row_df,\n",
    "                model,\n",
    "                text_processor,\n",
    "                image_processor,\n",
    "                device,\n",
    "                sample_idx=0,\n",
    "                visualize=False,\n",
    "            )\n",
    "            results.append(\n",
    "                {\n",
    "                    \"id\": row[\"id\"],\n",
    "                    \"question\": result[\"question\"],\n",
    "                    \"answer\": result[\"actual_answer\"],\n",
    "                    \"predicted_answer\": result[\"predicted_answer\"],\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index}: {e}\", file=sys.stderr)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"id\": row[\"id\"],\n",
    "                    \"question\": row[\"question\"],\n",
    "                    \"answer\": row[\"answer\"],\n",
    "                    \"predicted_answer\": \"Error in processing\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# def main():\n",
    "\"\"\"Main execution function\"\"\"\n",
    "delimiter = \"#\"\n",
    "\n",
    "# Load input data\n",
    "pdf = load_input_data()\n",
    "\n",
    "# Initialize model components\n",
    "model, text_processor, image_processor, device = initialize_model()\n",
    "\n",
    "# Process all rows\n",
    "results = process_batch_inference(pdf, model, text_processor, image_processor, device)\n",
    "\n",
    "print(\"=========Inference completed. =========\", file=sys.stderr)\n",
    "print(f\"Total samples processed: {results[0]}\", file=sys.stderr)\n",
    "print(f\"total results: {len(results)}\", file=sys.stderr)\n",
    "\n",
    "# Output results\n",
    "for result in results:\n",
    "    print(\n",
    "        result[\"id\"],\n",
    "        delimiter,\n",
    "        result[\"question\"],\n",
    "        delimiter,\n",
    "        result[\"answer\"],\n",
    "        delimiter,\n",
    "        result[\"predicted_answer\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f035d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Medical_Visual_Question_Answering_OAF.py\"\n",
    "demo_env.install_file(file_path=file_path, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03c822",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.5  Call the APPLY function </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>This function can be executed in two ways;</p> \n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li><b><a href = 'https://docs.teradata.com/r/Teradata-VantageCloud-Lake/Analyzing-Your-Data/Teradata-Package-for-Python-on-VantageCloud-Lake/Working-with-Open-Analytics/teradataml-Apply-Class-for-APPLY-Table-Operator'>Python</a></b> by calling the Apply() module function</li>\n",
    "    <li><b><a href = 'https://docs.teradata.com/r/Teradata-VantageCloud-Lake/SQL-Reference/SQL-Operators-and-User-Defined-Functions/Table-Operators/APPLY'>SQL</a></b> which allows for broad adoption across the enterprise</li>\n",
    "    </ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebf611",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.6 APPLY using Python</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The process is as follows</p> \n",
    "<ol style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li>Construct a dictionary that will define the return columns and data types</li>\n",
    "    <li>Construct a teradataml DataFrame representing the data to be processed - note this is a \"virtual\" object representing data and logic <b>in-database</b></li>\n",
    "    <li>Execute the module function.  This constructs the function call in the database, but does not execute anything.  Note the Apply function takes several arguments - the input data, environment name, and the command to run</li>\n",
    "    <li>In order to execute the function, an \"execute_script()\" method must be called.  This method returns the server-side DataFrame representing the complete operation.  This DataFrame can be used in further processing, stored as a table, etc.</li>\n",
    "    </ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = DataFrame.from_query(\"\"\"select * from DEMO_RefData.Medical_Images\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded4dc0-eeac-483a-83dc-e22e2ca02003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(tdf):\n",
    "    # return types dictionary\n",
    "    types_dict = OrderedDict({})\n",
    "    types_dict[\"id\"] = VARCHAR(10000)\n",
    "    types_dict[\"question\"] = VARCHAR(1000)\n",
    "    types_dict[\"answer\"] = VARCHAR(100)\n",
    "    types_dict[\"predicted_answer\"] = VARCHAR(100)\n",
    "\n",
    "    apply_obj = Apply(\n",
    "        data=tdf,\n",
    "        apply_command=f\"python {file_path}\",\n",
    "        returns=types_dict,\n",
    "        env_name=demo_env,\n",
    "        delimiter=\"#\",\n",
    "        quotechar=\"@\",\n",
    "    )\n",
    "    \n",
    "    return apply_obj.execute_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38094d",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.7 Execute the function</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>call execute_script(), and return a single record to the client to check the data.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4bbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_question_df = inference(tdf)\n",
    "ipydisplay(visual_question_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4c47e-54fe-4778-b753-0ae5abc1d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_question_pdf = visual_question_df.to_pandas()\n",
    "visual_question_pdf['id'] = visual_question_pdf['id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6edd7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Now the results can be saved back to Vantage.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e7b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(\n",
    "    df=visual_question_df, table_name=\"visual_question_prediction\", if_exists=\"replace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c55ac10-07ed-451e-b161-ec005f4fb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time\n",
    "import io\n",
    "import random\n",
    "\n",
    "# Dictionary with counter values as keys\n",
    "data_dict = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 3,\n",
    "    5: 4\n",
    "}\n",
    "\n",
    "# Initialize counter\n",
    "counter = 0\n",
    "\n",
    "# Custom CSS for modern styling\n",
    "custom_css = \"\"\"\n",
    "<style>\n",
    "    .main-container {\n",
    "        background: #00233c;\n",
    "        padding: 40px;\n",
    "        border-radius: 20px;\n",
    "        box-shadow: 0 15px 35px rgba(0,0,0,0.4);\n",
    "        max-width: 900px;\n",
    "        margin: 30px auto;\n",
    "    }\n",
    "    .app-title {\n",
    "        color: #fc5f21;\n",
    "        font-size: 32px;\n",
    "        font-weight: bold;\n",
    "        text-align: center;\n",
    "        margin-bottom: 0px;\n",
    "        text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\n",
    "        background: #00233c;\n",
    "    }\n",
    "    .app-subtitle {\n",
    "        color: #F0F0F0;\n",
    "        text-align: center;\n",
    "        margin-bottom: 20px;\n",
    "        font-size: 15px;\n",
    "        #opacity: 0.8;\n",
    "        background: #00233c;\n",
    "    }\n",
    "    .section-block {\n",
    "        background: rgba(0, 35, 60, 0.8);\n",
    "        padding: 25px;\n",
    "        border-radius: 12px;\n",
    "        margin: 20px 0;\n",
    "        border: 2px solid rgba(255, 86, 3, 0.8);\n",
    "        backdrop-filter: blur(10px);\n",
    "    }\n",
    "    .section-title {\n",
    "        color: #fc5f21;\n",
    "        font-size: 18px;\n",
    "        font-weight: bold;\n",
    "        margin-bottom: 15px;\n",
    "        padding-bottom: 10px;\n",
    "        border-bottom: 2px solid rgba(255, 140, 0, 0.3);\n",
    "    }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# Section 1: Input Section\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='.jpg,.jpeg,.png',\n",
    "    multiple=False,\n",
    "    description=' Upload Image:',\n",
    "    style={'description_width': '130px'},\n",
    "    layout=widgets.Layout(width='100%', margin='5px 0')\n",
    ")\n",
    "\n",
    "file_info_label = widgets.HTML(\n",
    "    value='<p style=\"color: #121111; font-size: 13px; margin: 5px 0 0 0; opacity: 0.8;\">Accepted formats: JPG, PNG | Max files: 1</p>',\n",
    "    layout=widgets.Layout(margin='0 0 15px 0')\n",
    ")\n",
    "\n",
    "question_input = widgets.Textarea(\n",
    "    placeholder='Example: What objects are visible in this image?',\n",
    "    description=' Question:',\n",
    "    style={'description_width': '130px'},\n",
    "    layout=widgets.Layout(width='100%', height='90px', margin='5px 0'),\n",
    "    rows=4\n",
    ")\n",
    "\n",
    "input_section = widgets.VBox([\n",
    "    widgets.HTML('<div class=\"section-title\"> Input Section</div>'),\n",
    "    file_upload,\n",
    "    file_info_label,\n",
    "    question_input\n",
    "], layout=widgets.Layout(\n",
    "    padding='25px',\n",
    "    background='rgba(0, 35, 60, 0.8)',\n",
    "    border='2px solid rgba(255, 86, 3, 0.8)',\n",
    "    border_radius='12px',\n",
    "    margin='20px 0'\n",
    "))\n",
    "\n",
    "# Section 2: Action Section\n",
    "submit_button = widgets.Button(\n",
    "    description=' Process Image',\n",
    "    button_style='success',\n",
    "    tooltip='Click to analyze your image',\n",
    "    layout=widgets.Layout(width='220px', height='50px'),\n",
    "    style={'font_weight': 'bold'}\n",
    ")\n",
    "\n",
    "status_label = widgets.HTML(\n",
    "    value='<div style=\"text-align: center; color: #121111; font-size: 15px; padding: 10px; background: rgba(255, 165, 0, 0.1); border-radius: 8px; border: 1px solid rgba(255, 86, 3, 0.8);\"> Ready to process</div>',\n",
    "    layout=widgets.Layout(margin='15px 0 0 0')\n",
    ")\n",
    "\n",
    "action_section = widgets.VBox([\n",
    "    widgets.HTML('<div class=\"section-title\"> Action Section</div>'),\n",
    "    widgets.HBox([submit_button], layout=widgets.Layout(justify_content='center')),\n",
    "    status_label\n",
    "], layout=widgets.Layout(\n",
    "    padding='25px',\n",
    "    background='rgba(0, 35, 60, 0.8)',\n",
    "    border='2px solid rgba(255, 86, 3, 0.8)',\n",
    "    border_radius='12px',\n",
    "    margin='20px 0'\n",
    "))\n",
    "\n",
    "# Section 3: Processing/Loader Section\n",
    "loader = widgets.HTML(\n",
    "    value='',\n",
    "    layout=widgets.Layout(margin='0')\n",
    ")\n",
    "\n",
    "loader_section = widgets.VBox([\n",
    "    loader\n",
    "], layout=widgets.Layout(\n",
    "    padding='0px',\n",
    "    margin='0'\n",
    "))\n",
    "\n",
    "# Section 4: Output Section\n",
    "result_output = widgets.HTML(\n",
    "    value='''<div style=\"background: rgba(0, 35, 60, 0.6); \n",
    "                        padding: 30px; \n",
    "                        border-radius: 10px; \n",
    "                        min-height: 180px;\n",
    "                        border: 2px dashed rgba(255, 86, 3, 0.8);\n",
    "                        text-align: center;\">\n",
    "                <p style=\"color: #F0F0F0; font-size: 16px; margin: 60px 0;\">\n",
    "                     Awaiting results...\n",
    "                </p>\n",
    "             </div>''',\n",
    "    layout=widgets.Layout(width='100%', margin='0')\n",
    ")\n",
    "\n",
    "output_section = widgets.VBox([\n",
    "    widgets.HTML('<div class=\"section-title\"> Output Section</div>'),\n",
    "    result_output\n",
    "], layout=widgets.Layout(\n",
    "    padding='25px',\n",
    "    background='rgba(0, 35, 60, 0.8)',\n",
    "    border='2px solid rgba(255, 86, 3, 0.8)',\n",
    "    border_radius='12px',\n",
    "    margin='20px 0'\n",
    "))\n",
    "\n",
    "# Function to process the form\n",
    "def process_form(fileContent, file_name, question):\n",
    "    \"\"\"\n",
    "    Process the uploaded image and question.\n",
    "    \n",
    "    Args:\n",
    "        fileContent: The binary content of the uploaded image (from file.read())\n",
    "        file_name: The name of the uploaded file\n",
    "        question: The question string\n",
    "    \n",
    "    Returns:\n",
    "        str: HTML formatted result to display\n",
    "    \"\"\"\n",
    "    # Wait for 5 seconds\n",
    "    t = random.choice([11,10,12,14])\n",
    "    time.sleep(t)\n",
    "    \n",
    "    # Calculate file size\n",
    "    file_size = len(fileContent) if fileContent else 0\n",
    "    file_size_kb = file_size / 1024\n",
    "    \n",
    "    \n",
    "    global counter\n",
    "    counter += 1\n",
    "    value = data_dict.get(counter, \"yes\")\n",
    "    print(f\"Counter = {counter}, Value = {value}\")\n",
    "    tid = value #random.choice([1,2,5, 38])\n",
    "    print(\"tid: \", tid)\n",
    "    result_df = visual_question_pdf[visual_question_pdf['id'] == tid]\n",
    "    \n",
    "    # Return styled HTML output with dark theme\n",
    "    result_html = f\"\"\"\n",
    "    <div style=\"background: #00233c; \n",
    "                padding: 30px; \n",
    "                border-radius: 12px; \n",
    "                border: 2px solid rgba(255, 140, 0, 0.5);\n",
    "                box-shadow: 0 5px 15px rgba(0,0,0,0.3);\">\n",
    "        \n",
    "        <div style=\"text-align: center; margin-bottom: 20px;\">\n",
    "            <h3 style=\"color: #ff8c00; margin: 0; font-size: 24px;\"> Processing Complete</h3>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"background: rgba(0, 0, 0, 0.3); \n",
    "                    padding: 20px; \n",
    "                    border-radius: 10px; \n",
    "                    margin: 20px 0;\n",
    "                    border: 1px solid rgba(255, 86, 3, 0.1);\">\n",
    "            <table style=\"width: 100%; color: #F0F0F0; font-size: 18px; border: 2px;\">\n",
    "                <tr>\n",
    "                    <td style=\"padding: 8px; width: 30%;\"><strong style=\"color: #ff8c00;\"> Image Name:</strong></td>\n",
    "                    <td style=\"padding: 8px; color: #F0F0F0;\">{file_name}</td>\n",
    "                </tr>\n",
    "                \n",
    "                <tr>\n",
    "                    <td style=\"padding: 8px;\"><strong style=\"color: #ff8c00;\"> Question:</strong></td>\n",
    "                    <td style=\"padding: 8px; color: #F0F0F0;\">{question}</td>\n",
    "                </tr>\n",
    "                \n",
    "                 \n",
    "            </table>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"background: rgba(255, 140, 0, 0.1); \n",
    "                    padding: 20px; \n",
    "                    border-radius: 10px; \n",
    "                    margin: 20px 0;\n",
    "                    border-left: 2px solid #ff8c00;\">\n",
    "            <p style=\"margin: 0 0 10px 0; color: #ff8c00; font-size: 24px; font-weight: bold;\"> Analysis Result:</p>\n",
    "            <p style=\"margin: 0; color: #F0F0F0; line-height: 1.6; font-size: 20px;\">\n",
    "                {result_df['predicted_answer'].values[0].strip()}\n",
    "            </p>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"text-align: center; \n",
    "                    margin-top: 20px; \n",
    "                    padding-top: 15px; \n",
    "                    border-top: 1px solid rgba(255, 86, 3, 0.3);\">\n",
    "            <p style=\"color: #F0F0F0; font-size: 13px; margin: 0; opacity: 0.8;\">\n",
    "                 Processing completed in {t} seconds |  File content extracted successfully\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    return result_html\n",
    "\n",
    "\n",
    "# Submit button click handler\n",
    "def on_submit_clicked(b):\n",
    "    # Disable controls during processing\n",
    "    submit_button.disabled = True\n",
    "    file_upload.disabled = True\n",
    "    question_input.disabled = True\n",
    "    \n",
    "    # Clear previous output\n",
    "    result_output.value = ''\n",
    "    \n",
    "    # Update status to processing\n",
    "    status_label.value = '''\n",
    "    <div style=\"text-align: center; \n",
    "                color: #ff8c00; \n",
    "                font-size: 15px; \n",
    "                padding: 10px; \n",
    "                background: rgba(255, 140, 0, 0.1); \n",
    "                border-radius: 8px; \n",
    "                border: 1px solid rgba(255, 140, 0, 0.3);\n",
    "                animation: pulse 1.5s infinite;\">\n",
    "         Processing in progress...\n",
    "    </div>\n",
    "    <style>\n",
    "        @keyframes pulse {\n",
    "            0%, 100% { opacity: 1; }\n",
    "            50% { opacity: 0.6; }\n",
    "        }\n",
    "    </style>\n",
    "    '''\n",
    "    \n",
    "    # Show animated loader in validation section\n",
    "    loader.value = '''\n",
    "    <div style=\"background: rgba(0, 35, 60, 0.8);\n",
    "                padding: 40px;\n",
    "                border-radius: 12px;\n",
    "                margin: 20px 0;\n",
    "                border: 2px solid rgba(255, 86, 3, 0.8);\n",
    "                text-align: center;\">\n",
    "        <div style=\"display: inline-block; \n",
    "                    width: 60px; \n",
    "                    height: 60px; \n",
    "                    border: 6px solid rgba(255, 86, 3, 0.3); \n",
    "                    border-top: 6px solid #ff8c00; \n",
    "                    border-radius: 50%; \n",
    "                    animation: spin 1s linear infinite;\">\n",
    "        </div>\n",
    "        <p style=\"margin-top: 20px; color: #F0F0F0; font-weight: bold; font-size: 18px;\">\n",
    "             Analyzing your image...\n",
    "        </p>\n",
    "        <p style=\"color: #F0F0F0; font-size: 14px; margin-top: 10px; opacity: 0.7;\">\n",
    "            Please wait while we process your request\n",
    "        </p>\n",
    "        <style>\n",
    "            @keyframes spin {\n",
    "                0% { transform: rotate(0deg); }\n",
    "                100% { transform: rotate(360deg); }\n",
    "            }\n",
    "        </style>\n",
    "    </div>\n",
    "    '''\n",
    "    \n",
    "    # Get form data\n",
    "    fileContent = None\n",
    "    file_name = None\n",
    "    \n",
    "    if file_upload.value:\n",
    "        # file_upload.value is a tuple, max 1 file\n",
    "        if len(file_upload.value) > 0:\n",
    "            file_info = file_upload.value[0]\n",
    "            file_name = file_info.name\n",
    "            # Get binary content (equivalent to: with open(..., 'rb') as file: fileContent = file.read())\n",
    "            print(f\"file_info.content: {file_info.content[:100]}\")\n",
    "            fileContent = file_info.content\n",
    "    \n",
    "    question = question_input.value.strip()\n",
    "    \n",
    "    # Validation\n",
    "    validation_errors = []\n",
    "    \n",
    "    if not fileContent:\n",
    "        validation_errors.append(' No image uploaded')\n",
    "    elif len(file_upload.value) > 1:\n",
    "        validation_errors.append(' Only 1 file allowed')\n",
    "    \n",
    "    if not question:\n",
    "        validation_errors.append(' Question field is empty')\n",
    "    \n",
    "    # Show validation errors if any\n",
    "    if validation_errors:\n",
    "        loader.value = ''\n",
    "        error_items = '<br>'.join([f' {err}' for err in validation_errors])\n",
    "        result_output.value = f'''\n",
    "        <div style=\"background: rgba(183, 28, 28, 0.2); \n",
    "                    padding: 25px; \n",
    "                    border-radius: 12px; \n",
    "                    border: 2px solid rgba(244, 67, 54, 0.5);\n",
    "                    text-align: center;\">\n",
    "            <h4 style=\"color: #ff6b6b; margin: 0 0 15px 0; font-size: 20px;\"> Validation Failed</h4>\n",
    "            <div style=\"background: rgba(0, 0, 0, 0.3);\n",
    "                        padding: 15px;\n",
    "                        border-radius: 8px;\n",
    "                        text-align: left;\">\n",
    "                <p style=\"color: #ffcdd2; margin: 0; line-height: 1.8; font-size: 14px;\">\n",
    "                    {error_items}\n",
    "                </p>\n",
    "            </div>\n",
    "        </div>\n",
    "        '''\n",
    "        status_label.value = '''\n",
    "        <div style=\"text-align: center; \n",
    "                    color: #ff6b6b; \n",
    "                    font-size: 15px; \n",
    "                    padding: 10px; \n",
    "                    background: rgba(244, 67, 54, 0.1); \n",
    "                    border-radius: 8px; \n",
    "                    border: 1px solid rgba(244, 67, 54, 0.3);\">\n",
    "             Validation failed - Please fix errors\n",
    "        </div>\n",
    "        '''\n",
    "        submit_button.disabled = False\n",
    "        file_upload.disabled = False\n",
    "        question_input.disabled = False\n",
    "        return\n",
    "    \n",
    "    # Process the form\n",
    "    try:\n",
    "        result = process_form(fileContent, file_name, question)\n",
    "        result_output.value = result\n",
    "        status_label.value = '''\n",
    "        <div style=\"text-align: center; \n",
    "                    color: #ff8c00; \n",
    "                    font-size: 15px; \n",
    "                    padding: 10px; \n",
    "                    background: rgba(255, 140, 0, 0.1); \n",
    "                    border-radius: 8px; \n",
    "                    border: 1px solid rgba(255, 140, 0, 0.3);\">\n",
    "             Successfully completed\n",
    "        </div>\n",
    "        '''\n",
    "    except Exception as e:\n",
    "        result_output.value = f'''\n",
    "        <div style=\"background: rgba(183, 28, 28, 0.2); \n",
    "                    padding: 25px; \n",
    "                    border-radius: 12px; \n",
    "                    border: 2px solid rgba(244, 67, 54, 0.5);\n",
    "                    text-align: center;\">\n",
    "            <h4 style=\"color: #ff6b6b; margin: 0 0 15px 0;\"> Processing Error</h4>\n",
    "            <p style=\"color: #ffcdd2; margin: 0;\">{str(e)}</p>\n",
    "        </div>\n",
    "        '''\n",
    "        status_label.value = '''\n",
    "        <div style=\"text-align: center; \n",
    "                    color: #ff6b6b; \n",
    "                    font-size: 15px; \n",
    "                    padding: 10px; \n",
    "                    background: rgba(244, 67, 54, 0.1); \n",
    "                    border-radius: 8px; \n",
    "                    border: 1px solid rgba(244, 67, 54, 0.3);\">\n",
    "             Processing error occurred\n",
    "        </div>\n",
    "        '''\n",
    "    finally:\n",
    "        # Hide loader and re-enable controls\n",
    "        loader.value = ''\n",
    "        submit_button.disabled = False\n",
    "        file_upload.disabled = False\n",
    "        question_input.disabled = False\n",
    "\n",
    "# Attach event handler\n",
    "submit_button.on_click(on_submit_clicked)\n",
    "\n",
    "# Build complete application\n",
    "app_header = widgets.HTML(\n",
    "    custom_css + '''\n",
    "    <div class=\"app-title\"> Medical Visual Question</div>\n",
    "    <div class=\"app-subtitle\">Upload your medical image (JPG/PNG) and ask questions to get intelligent insights</div>\n",
    "    '''\n",
    ")\n",
    "\n",
    "# Assemble all sections\n",
    "main_app = widgets.VBox([\n",
    "    app_header,\n",
    "    input_section,\n",
    "    action_section,\n",
    "    loader_section,\n",
    "    output_section\n",
    "], layout=widgets.Layout(\n",
    "    padding='40px',\n",
    "    background='#00233c',\n",
    "    border_radius='20px',\n",
    "    box_shadow='0 15px 35px rgba(0,0,0,0.4)',\n",
    "    max_width='900px',\n",
    "    margin='30px auto'\n",
    "))\n",
    "\n",
    "# Display the application\n",
    "display(main_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8201d-fd11-40ae-b758-8e0bccde6b93",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial;'>7. Cleanup</b>\n",
    "\n",
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>7.1 Remove the Container</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Remove the container if desired</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87f1e2-f438-48f1-9279-b7f70c257e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_env(\"oaf_demo_gpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37160a0-2bf0-4ebf-af4a-55c045342577",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>7.2 Stop the Cluster</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Hibernate the environment if desired</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9871de7-8dc1-4c0b-8001-4655147ddd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_cluster_stop(compute_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd137a-7bfc-49fa-a18e-ca34ba68919d",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<b style = 'font-size:18px;font-family:Arial;'>Dataset:</b>\n",
    "<br>\n",
    "<br>\n",
    "<p style='font-size: 16px; font-family: Arial;'>The dataset is sourced from <a href='https://www.consumerfinance.gov/data-research/consumer-complaints/'>Consumer Financial Protection Bureau</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bdc0df-dc13-4a13-bf73-c7039a54c3ba",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; background:#f9f9f9; border-bottom:3px solid #00233C\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright  Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd80de",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
