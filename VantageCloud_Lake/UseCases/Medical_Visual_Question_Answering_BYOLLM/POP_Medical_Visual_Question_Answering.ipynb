{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce3dd8a-f3e5-40d5-ab4c-d40cc358cf7f",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Answering Medical Questions about Images using VantageCloud and Open-Source Language Models\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57054f7e-befe-4029-b4ac-b82019240b1f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial;'><b>Introduction:</b></p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial;'>In this comprehensive user demo, we will delve into the world of Medical Visual Question Answering using <b>Teradata Vantage</b> and <b>open-source language models</b>. This cutting-edge technology empowers businesses to uncover hidden insights from vast amounts of medical image data, enabling them to identify cells, diseases, artery etc.</p> \n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\"><b>Key Features:</b></p>\n",
    "\n",
    "<ol style=\"font-size:16px;font-family:Arial;\">\n",
    "<li><b>Improved Clinical Decision Support:</b> A well-trained medical VQA model enhances clinical decision-making by allowing healthcare providers to ask questions about medical images (e.g., X-rays, MRIs, CT scans) and receive accurate, rapid answers. This can lead to faster diagnoses and treatment plans.</li>\n",
    "\n",
    "<li><b>Reducing Interpretation Errors:</b> Human interpretation of medical images can be subjective and prone to errors. A VQA model can provide objective, consistent, and evidence-based interpretations, helping to reduce diagnostic inaccuracies.</li>\n",
    "\n",
    "<li><b>Time Efficiency:</b> The model's ability to quickly analyze images and answer questions can save valuable time for healthcare professionals, leading to more efficient patient care.</li>\n",
    "\n",
    "<li><b>Accessibility:</b> Patients and non-specialist healthcare providers can benefit from a medical VQA system by obtaining easy-to-understand information about their health conditions, potentially improving health literacy.</li>\n",
    "\n",
    "<li><b>Learning and Training Aid:</b> Medical VQA models can serve as educational tools for medical students, residents, and even experienced practitioners. They can be used to explain complex medical concepts and imaging findings.</li>\n",
    "\n",
    "<li><b>Research Assistance:</b> Researchers can leverage the model to analyze large datasets of medical images more effectively. It can assist in extracting meaningful insights from these datasets, potentially leading to new discoveries in medical science.</li>\n",
    "\n",
    "<li><b>Cross-Specialty Applicability:</b> A well-designed medical VQA model can be adapted to various medical specialties, from radiology and pathology to cardiology and dermatology. This versatility makes it a valuable asset across different healthcare domains.</li>\n",
    "\n",
    "<li><b>Ethical Considerations:</b> It's essential to address ethical concerns related to privacy, security, and bias when deploying medical VQA models in healthcare settings. Ensuring patient data protection and model fairness is critical.</li>\n",
    "\n",
    "<li><b>Continuous Improvement:</b> Model performance and accuracy should be continuously monitored and improved over time. Regular updates and retraining are necessary to keep up with evolving medical knowledge and technologies.</li>\n",
    "\n",
    "<li><b>Collaboration:</b> Successful implementation of medical VQA models often requires collaboration between machine learning experts, healthcare professionals, and ethicists to ensure that the technology is used responsibly and effectively.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6863ef",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'><b>Visual Question Answering:</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'><b>What is visual Question Answering?</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Visual Question Answering (VQA) is a task in computer vision that involves answering questions about an image. The goal of VQA is to teach machines to understand the content of an image and answer questions about it in natural language.</p>\n",
    "\n",
    "<center><img id=\"125\" src=\"./images/header_scene.png\" height=\"400px\" width=\"600px\" style=\"border: 4px solid #404040; border-radius: 10px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7068302",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'><b>Steps in the analysis:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial;'>\n",
    "     <li>Configuring the environment</li>\n",
    "  <li>Connect to Vantage</li>\n",
    "  <li>Create a Custom Container in Vantage</li>\n",
    "  <li>Install Dependencies</li>\n",
    "  <li>Operationalizing AI-powered analytics</li>\n",
    "  <li>Topic Modelling</li>\n",
    "  <li>Cleanup</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e457db5-d4df-4e7f-a11a-fcb1416e462d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial;'>1. Configuring the Environment</b>\n",
    "\n",
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>1.1 Install the required libraries</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Please be aware that that it will take longer than 5 minutes to install these libraries the first time this cell is executed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb4587-6632-4b08-a121-c98ea5a8fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "!pip install -r requirements.txt --quiet\n",
    "\n",
    "!pip install --upgrade transformers --quiet\n",
    "!pip install --upgrade torch --quiet\n",
    "!pip install --upgrade sentence-transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd81e4-0df9-4360-b3ac-72214f135296",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial;'><b>Note: </b><i>Please restart the kernel after executing a </i><code>!pip install</code>. <i>The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i> and then clicking <b><i>Restart</i></b>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc278740-1511-4423-bf2b-92a5109a47bf",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>1.2 Import the required libraries</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f183178-9807-4538-b794-922820346ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python version 3.11 for user environment\n"
     ]
    }
   ],
   "source": [
    "from teradataml import *\n",
    "display.enable_ui = False\n",
    "\n",
    "from teradatasqlalchemy.types import *\n",
    "from time import sleep\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv, sys, os, warnings\n",
    "from os.path import expanduser\n",
    "from collections import OrderedDict\n",
    "from IPython.display import clear_output , display as ipydisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "# from itables import init_notebook_mode\n",
    "# import itables.options as opt\n",
    "from dotenv import load_dotenv\n",
    "# Set display options for dataframes, plots, and warnings\n",
    "\n",
    "# import utils for lake environment\n",
    "import os\n",
    "import sys\n",
    "# module_path = os.path.abspath(os.path.join('..', '..','config'))\n",
    "# sys.path.append(module_path)\n",
    "\n",
    "# opt.style=\"table-layout:auto;width:auto;float:left\"\n",
    "# opt.columnDefs = [{\"className\": \"dt-left\", \"targets\": \"_all\"}]\n",
    "# init_notebook_mode(all_interactive=True)\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "display.suppress_vantage_runtime_warnings = True\n",
    "\n",
    "python_version = \"3.11\"\n",
    "print(f'Using Python version {python_version} for user environment')\n",
    "\n",
    "# Hugging Face model for the demo\n",
    "model_names = {'ChetanHirapara/Salesforce-blip-vqa-base-medical-data'}\n",
    "\n",
    "# a list of required packages to install in the custom OAF container\n",
    "# modify this if using different models or design patterns\n",
    "pkgs = ['transformers',\n",
    "        'torch',\n",
    "        'pandas',\n",
    "        'sentence-transformers']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a37ff6-db2d-4c3a-90e6-9d1bceae9c60",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>2. Connect to VantageCloud Lake</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Connect to VantageCloud using <code>create_context</code> from the teradataml Python library. If this environment has been prepared for connecting to a VantageCloud Lake OAF Container, all the details required will be loaded and you will see an acknowledgement after executing this cell.</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>2.1 Load the Environment Variables and Connect to Vantage</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Load the environment variables from a .env file and use them to create a connection context to Teradata.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63f0143-bad0-45ff-a287-22557bdd2b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if this environment is ready to connect to VantageCloud Lake...\n",
      "Your environment parameter file exist.  Please proceed with this use case.\n",
      "Connected to VantageCloud Lake with: Engine(teradatasql://dallasprod26-jn6hvh5icpnu97gw:***@54.156.178.22)\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking if this environment is ready to connect to VantageCloud Lake...\")\n",
    "\n",
    "if os.path.exists(\"/home/jovyan/JupyterLabRoot/VantageCloud_Lake/.config/.env\"):\n",
    "    print(\"Your environment parameter file exist.  Please proceed with this use case.\")\n",
    "    # Load all the variables from the .env file into a dictionary\n",
    "    env_vars = dotenv_values(\"/home/jovyan/JupyterLabRoot/VantageCloud_Lake/.config/.env\")\n",
    "    # Create the Context\n",
    "    eng = create_context(host=env_vars.get(\"host\"), username=env_vars.get(\"username\"), password=env_vars.get(\"my_variable\"))\n",
    "    execute_sql('''SET query_band='DEMO=Medical_Visual_Question_Answering.ipynb;' UPDATE FOR SESSION;''')\n",
    "    print(\"Connected to VantageCloud Lake with:\", eng)\n",
    "else:\n",
    "    print(\"Your environment has not been prepared for connecting to VantageCloud Lake.\")\n",
    "    print(\"Please contact the support team.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec98ec10-e1cc-43e7-b06d-fd06c5d89081",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>2.2  Authenticate to the User Environment Service</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>To better support integration with Cloud Services and common automation tools; the <b > User Environment Service</b> is accessed via RESTful APIs.  These APIs can be called directly or in the examples shown below that leverage the Python Package for Teradata (teradataml) methods.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a607a2-d902-4285-a006-5d4ec5b7bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication token is generated, authenticated and set for the session.\n",
      "UES Authentication successful\n"
     ]
    }
   ],
   "source": [
    "# We've already loaded all the values into our environment variables and into a dictionary, env_vars.\n",
    "# username=env_vars.get(\"username\") isn't required when using base_url, pat and pem.\n",
    "\n",
    "if set_auth_token(base_url=env_vars.get(\"ues_uri\"),\n",
    "                  pat_token=env_vars.get(\"access_token\"), \n",
    "                  pem_file=env_vars.get(\"pem_file\"),\n",
    "                  valid_from=int(time.time())\n",
    "                 ):\n",
    "    print(\"UES Authentication successful\")\n",
    "else:\n",
    "    print(\"UES Authentication failed. Check credentials.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d1928-f6cb-4084-ad36-9559c1d1b20f",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<b style = 'font-size:18px;font-family:Arial;'>3. Create a Custom Container in VantageCloud</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>If desired, the user can create a <b>new</b> custom environment by starting with a \"base\" image and customizing it.  The steps are:</p> \n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li>List the available \"base\" images the system supports</li>\n",
    "    <li>List any existing \"custom\" environments the user has created</li>\n",
    "    <li>If there are no custom environments, then create a new one from a base image</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb82cfe-9db7-4826-b92b-9c588bd4018f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a list of the versions of the libraries available to be used within an OAF environments.\n",
      "\n",
      "     base_name language  version\n",
      "0   python_3.9   Python   3.9.20\n",
      "1  python_3.10   Python  3.10.15\n",
      "2  python_3.11   Python  3.11.10\n",
      "3        r_4.3        R    4.3.3\n",
      "4        r_4.4        R    4.4.2\n",
      "No user environment(s) found.\n",
      "\n",
      "This user does not have any environments.\n",
      "Creating your environment now.\n",
      "User environment 'dallasprod26-jn6hvh5icpnu97gw' created.\n",
      "\n",
      "================================================\n",
      "Environment Name: dallasprod26-jn6hvh5icpnu97gw\n",
      "Base Environment: python_3.11\n",
      "Description: BYOLLM demo env\n",
      "\n",
      "############ Libraries installed in User Environment ############\n",
      "\n",
      "           name version\n",
      "0           pip    25.2\n",
      "1  proxy-client   1.0.5\n",
      "2    setuptools  80.9.0\n",
      "\n",
      "================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if we have any existing environments\n",
    "# If any other environments exist along with our default OAF environment, we will delete them\n",
    "\n",
    "environment_name = env_vars.get(\"username\")\n",
    "print(\"Here is a list of the versions of the libraries available to be used within an OAF environments.\\n\")\n",
    "print(list_base_envs())\n",
    "env_list = list_user_envs()\n",
    "\n",
    "if env_list is None:\n",
    "    print(\"\\nThis user does not have any environments.\\nCreating your environment now.\")\n",
    "    demo_env = create_env(env_name=f'{environment_name}', base_env='python_3.11', desc='BYOLLM demo env')\n",
    "    print(demo_env)\n",
    "else:\n",
    "    print(\"\\nHere is a list of your current environments:\")\n",
    "    ipydisplay(env_list)\n",
    "    for env_name in env_list['env_name']:\n",
    "        if env_name == environment_name:\n",
    "            demo_env = get_env(environment_name)\n",
    "            print(\"Your default environment already exists. You can continue with this notebook.\\n\\n\")\n",
    "        else:\n",
    "            print(f\"Your existing environment, {env_name} doesn't match our default environment for this user.\")\n",
    "            print(\"We're going to delete it.\")      \n",
    "            print(f\"Please wait: Environment {env_name} is being removed!\")\n",
    "            remove_env(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556a860-e27f-432d-a4d1-f4bd903e34fd",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>4. Install Dependencies</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The step in the process installs Python package dependencies. VantageCloud Open Analytics Framework Environments are secured against unauthorized access to the outside network. Users can load the required libraries and model using teradataml methods:\n",
    "</p> \n",
    "\n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li>List the currently installed models and python libraries</li>\n",
    "    <li><b>If necessary</b>, install any required packages</li>\n",
    "    <li><b>If necessary</b>, install the pre-trained model.  This process takes several steps;\n",
    "        <ol style = 'font-size:16px;font-family:Arial;'>\n",
    "            <li>Import and download the model</li>\n",
    "            <li>Create a zip archive of the model artifacts</li>\n",
    "            <li>Call the install_model() method to load the model to the container</li>\n",
    "        </ol></li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc77d1b5-d931-4766-806b-2068b27c8f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the models installed in the remote user environment...\n",
      "No models found in remote user environment dallasprod26-jn6hvh5icpnu97gw.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the libraries installed in the remote user environment...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pip</td>\n",
       "      <td>25.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>proxy-client</td>\n",
       "      <td>1.0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>setuptools</td>\n",
       "      <td>80.9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name version\n",
       "0           pip    25.2\n",
       "1  proxy-client   1.0.5\n",
       "2    setuptools  80.9.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Here are the models installed in the remote user environment...\")\n",
    "ipydisplay(demo_env.models)\n",
    "\n",
    "print(\"Here are the libraries installed in the remote user environment...\")\n",
    "ipydisplay(demo_env.libs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b179f-4649-483a-a470-db064266c3b6",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>4.1 A note on package versions</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The next section of this demonstration makes use of the DataFrame apply() method, which will execute python code that we'll upload into the environment.  We have to ensue the python packages we have installed locally match the versions we have in the environment.\n",
    "</p> \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'><b>Note</b>Please be aware that this step may take 15 minutes to execute.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1376feb7-8bf2-49de-816e-8a9131be876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformers', 'torch', 'pandas', 'sentence-transformers']\n",
      "Request to install libraries initiated successfully in the remote user environment dallasprod26-jn6hvh5icpnu97gw. Check the status using status() with the claim id '01970339-aa5a-43e6-89ff-36e2b73fb864'.\n"
     ]
    }
   ],
   "source": [
    "# import these functions inside of a function namespace\n",
    "def get_versions(pkgs):\n",
    "    local_v_pkgs = []\n",
    "    for p in pkgs:\n",
    "\n",
    "        # fix up any hyphened package names\n",
    "        p_fixed = p.replace(\"-\", \"_\")\n",
    "\n",
    "        # import the packages and append the strings to the list\n",
    "        exec(\n",
    "            f\"\"\"import {p_fixed}; local_v_pkgs.append('{p}==' + str({p_fixed}.__version__))\"\"\"\n",
    "        )\n",
    "    return local_v_pkgs\n",
    "\n",
    "print(pkgs)\n",
    "v_pkgs = get_versions(pkgs)\n",
    "\n",
    "\n",
    "# check to see if these packages need to be installed\n",
    "# by comparing the len of the intersection of the list of required packages with the installed ones\n",
    "if not len(\n",
    "    set([x.split(\"==\")[0] for x in pkgs]).intersection(demo_env.libs[\"name\"].to_list())) == len(pkgs):\n",
    "\n",
    "    # pass the list of packages - split off any extra info from the version property e.g., plus sign\n",
    "    claim_id = demo_env.install_lib(\n",
    "        [x.split(\"+\")[0] for x in v_pkgs], asynchronous=True\n",
    "    )\n",
    "else:\n",
    "    print(f\"All required packages are installed in the {environment_name} environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d186ca1",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>4.2 Monitor library installation status</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Optionally - users can monitor the library installation status using the cell below:\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae8c3e4-06cb-4fae-913c-3b1202f72001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim Id</th>\n",
       "      <th>File/Libs/Model</th>\n",
       "      <th>Method Name</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Additional Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01970339-aa5a-43e6-89ff-36e2b73fb864</td>\n",
       "      <td>transformers==4.57.1, torch==2.9.0, pandas==2....</td>\n",
       "      <td>install_lib</td>\n",
       "      <td>Started</td>\n",
       "      <td>2025-10-27T02:57:04Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01970339-aa5a-43e6-89ff-36e2b73fb864</td>\n",
       "      <td>transformers==4.57.1, torch==2.9.0, pandas==2....</td>\n",
       "      <td>install_lib</td>\n",
       "      <td>Finished</td>\n",
       "      <td>2025-10-27T03:12:20Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Claim Id  \\\n",
       "0  01970339-aa5a-43e6-89ff-36e2b73fb864   \n",
       "1  01970339-aa5a-43e6-89ff-36e2b73fb864   \n",
       "\n",
       "                                     File/Libs/Model  Method Name     Stage  \\\n",
       "0  transformers==4.57.1, torch==2.9.0, pandas==2....  install_lib   Started   \n",
       "1  transformers==4.57.1, torch==2.9.0, pandas==2....  install_lib  Finished   \n",
       "\n",
       "              Timestamp Additional Details  \n",
       "0  2025-10-27T02:57:04Z                     \n",
       "1  2025-10-27T03:12:20Z                     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an updated list of libraries installed in the remote user environment...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>certifi</td>\n",
       "      <td>2025.10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>charset-normalizer</td>\n",
       "      <td>3.4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filelock</td>\n",
       "      <td>3.20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fsspec</td>\n",
       "      <td>2025.9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hf-xet</td>\n",
       "      <td>1.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>huggingface-hub</td>\n",
       "      <td>0.36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>idna</td>\n",
       "      <td>3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jinja2</td>\n",
       "      <td>3.1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>joblib</td>\n",
       "      <td>1.5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MarkupSafe</td>\n",
       "      <td>3.0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mpmath</td>\n",
       "      <td>1.3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>networkx</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>numpy</td>\n",
       "      <td>2.3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nvidia-cublas-cu12</td>\n",
       "      <td>12.8.4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nvidia-cuda-cupti-cu12</td>\n",
       "      <td>12.8.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nvidia-cuda-nvrtc-cu12</td>\n",
       "      <td>12.8.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nvidia-cuda-runtime-cu12</td>\n",
       "      <td>12.8.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nvidia-cudnn-cu12</td>\n",
       "      <td>9.10.2.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nvidia-cufft-cu12</td>\n",
       "      <td>11.3.3.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nvidia-cufile-cu12</td>\n",
       "      <td>1.13.1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>nvidia-curand-cu12</td>\n",
       "      <td>10.3.9.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>nvidia-cusolver-cu12</td>\n",
       "      <td>11.7.3.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nvidia-cusparse-cu12</td>\n",
       "      <td>12.5.8.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>nvidia-cusparselt-cu12</td>\n",
       "      <td>0.7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>nvidia-nccl-cu12</td>\n",
       "      <td>2.27.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>nvidia-nvjitlink-cu12</td>\n",
       "      <td>12.8.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nvidia-nvshmem-cu12</td>\n",
       "      <td>3.3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>nvidia-nvtx-cu12</td>\n",
       "      <td>12.8.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>packaging</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>pandas</td>\n",
       "      <td>2.2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>pillow</td>\n",
       "      <td>12.0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>pip</td>\n",
       "      <td>25.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>proxy-client</td>\n",
       "      <td>1.0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>python-dateutil</td>\n",
       "      <td>2.9.0.post0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>pytz</td>\n",
       "      <td>2025.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>PyYAML</td>\n",
       "      <td>6.0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>regex</td>\n",
       "      <td>2025.10.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>requests</td>\n",
       "      <td>2.32.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>safetensors</td>\n",
       "      <td>0.6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>1.7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>scipy</td>\n",
       "      <td>1.16.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>sentence-transformers</td>\n",
       "      <td>5.1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>setuptools</td>\n",
       "      <td>80.9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>six</td>\n",
       "      <td>1.17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>sympy</td>\n",
       "      <td>1.14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>threadpoolctl</td>\n",
       "      <td>3.6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>tokenizers</td>\n",
       "      <td>0.22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>torch</td>\n",
       "      <td>2.9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>tqdm</td>\n",
       "      <td>4.67.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>transformers</td>\n",
       "      <td>4.57.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>triton</td>\n",
       "      <td>3.5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>typing_extensions</td>\n",
       "      <td>4.15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>tzdata</td>\n",
       "      <td>2025.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>urllib3</td>\n",
       "      <td>2.5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name      version\n",
       "0                    certifi    2025.10.5\n",
       "1         charset-normalizer        3.4.4\n",
       "2                   filelock       3.20.0\n",
       "3                     fsspec     2025.9.0\n",
       "4                     hf-xet        1.2.0\n",
       "5            huggingface-hub       0.36.0\n",
       "6                       idna         3.11\n",
       "7                     Jinja2        3.1.6\n",
       "8                     joblib        1.5.2\n",
       "9                 MarkupSafe        3.0.3\n",
       "10                    mpmath        1.3.0\n",
       "11                  networkx          3.5\n",
       "12                     numpy        2.3.4\n",
       "13        nvidia-cublas-cu12     12.8.4.1\n",
       "14    nvidia-cuda-cupti-cu12      12.8.90\n",
       "15    nvidia-cuda-nvrtc-cu12      12.8.93\n",
       "16  nvidia-cuda-runtime-cu12      12.8.90\n",
       "17         nvidia-cudnn-cu12    9.10.2.21\n",
       "18         nvidia-cufft-cu12    11.3.3.83\n",
       "19        nvidia-cufile-cu12     1.13.1.3\n",
       "20        nvidia-curand-cu12    10.3.9.90\n",
       "21      nvidia-cusolver-cu12    11.7.3.90\n",
       "22      nvidia-cusparse-cu12    12.5.8.93\n",
       "23    nvidia-cusparselt-cu12        0.7.1\n",
       "24          nvidia-nccl-cu12       2.27.5\n",
       "25     nvidia-nvjitlink-cu12      12.8.93\n",
       "26       nvidia-nvshmem-cu12       3.3.20\n",
       "27          nvidia-nvtx-cu12      12.8.90\n",
       "28                 packaging         25.0\n",
       "29                    pandas        2.2.3\n",
       "30                    pillow       12.0.0\n",
       "31                       pip         25.2\n",
       "32              proxy-client        1.0.5\n",
       "33           python-dateutil  2.9.0.post0\n",
       "34                      pytz       2025.2\n",
       "35                    PyYAML        6.0.3\n",
       "36                     regex   2025.10.23\n",
       "37                  requests       2.32.5\n",
       "38               safetensors        0.6.2\n",
       "39              scikit-learn        1.7.2\n",
       "40                     scipy       1.16.2\n",
       "41     sentence-transformers        5.1.2\n",
       "42                setuptools       80.9.0\n",
       "43                       six       1.17.0\n",
       "44                     sympy       1.14.0\n",
       "45             threadpoolctl        3.6.0\n",
       "46                tokenizers       0.22.1\n",
       "47                     torch        2.9.0\n",
       "48                      tqdm       4.67.1\n",
       "49              transformers       4.57.1\n",
       "50                    triton        3.5.0\n",
       "51         typing_extensions       4.15.0\n",
       "52                    tzdata       2025.2\n",
       "53                   urllib3        2.5.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the status of installation using status() API.\n",
    "# Create a loop here for demo purposes\n",
    "try:\n",
    "    claim_id\n",
    "    ipydisplay(demo_env.status(claim_id))\n",
    "    stage = demo_env.status(claim_id)[\"Stage\"].iloc[-1]\n",
    "    while stage == \"Started\":\n",
    "        stage = demo_env.status(claim_id)[\"Stage\"].iloc[-1]\n",
    "        clear_output()\n",
    "        ipydisplay(demo_env.status(claim_id))\n",
    "        sleep(5)\n",
    "except NameError:\n",
    "    print(\"No installations to monitor\")\n",
    "\n",
    "\n",
    "# Verify the Python libraries have been installed correctly.\n",
    "print(\"Here is an updated list of libraries installed in the remote user environment...\")\n",
    "ipydisplay(demo_env.libs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162969f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>4.3 Download and install model</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Open Analytics Framework environments do not have open access to the external network, which contributes to a very secure runtime environment.  As such, we will need to download the pre-trained model using the below API.  This cell will download the model and then install it into our OpenAnalytics Framework environment. This will take ~5 minutes.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08989723-d0c0-4f70-bd5f-bf7fa2beb6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81164d98bd34dde86577d719f37d166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e044c708a84b728b25647d7e06984c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a0e713554e428c98f6949d85193f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3651e1cb1148efb6635f3b236049a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93b2ff0fb744ddfb34ac34b5b02aac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d588777b944e4663b2f0e04b18d81faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/471 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb0c62326674c2f81d665e2d5a64c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53e60ab6e4e4dd38a672d797f67a1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f766c3c9d4964612b62cf1ad5bea27d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40de73ba41546a586f785f668843f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to: ./blip_model\n",
      "Model installation is initiated. Check the status using status() with the claim id 65867ae0-5eeb-45a8-97b8-1ee2b4d3e7eb.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download an entire model repository\n",
    "model_path = snapshot_download(repo_id=\"ChetanHirapara/Salesforce-blip-vqa-base-medical-data\", local_dir = './blip_model')\n",
    "print(f\"Model downloaded to: ./blip_model\")\n",
    "\n",
    "shutil.make_archive(f'blip_model',\n",
    "                format=\"zip\",\n",
    "                root_dir='./blip_model',\n",
    "            )\n",
    "\n",
    "claim_id = demo_env.install_model('blip_model.zip', asynchronous = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268e54f-78f9-4ebb-913b-16317807bbb4",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>4.4 Monitor model installation status</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Optionally - users can monitor the model installation status using the cell below:\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db8c67c-e326-41a1-a71d-dfb001a968a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claim Id</th>\n",
       "      <th>File/Libs/Model</th>\n",
       "      <th>Method Name</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Additional Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65867ae0-5eeb-45a8-97b8-1ee2b4d3e7eb</td>\n",
       "      <td>blip_model.zip</td>\n",
       "      <td>install_model</td>\n",
       "      <td>Endpoint Generated</td>\n",
       "      <td>2025-10-27T03:27:24Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65867ae0-5eeb-45a8-97b8-1ee2b4d3e7eb</td>\n",
       "      <td>blip_model.zip</td>\n",
       "      <td>install_model</td>\n",
       "      <td>File Uploaded</td>\n",
       "      <td>2025-10-27T03:27:39Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65867ae0-5eeb-45a8-97b8-1ee2b4d3e7eb</td>\n",
       "      <td>blip_model.zip</td>\n",
       "      <td>install_model</td>\n",
       "      <td>File Installed</td>\n",
       "      <td>2025-10-27T03:30:07Z</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Claim Id File/Libs/Model    Method Name  \\\n",
       "0  65867ae0-5eeb-45a8-97b8-1ee2b4d3e7eb  blip_model.zip  install_model   \n",
       "1  65867ae0-5eeb-45a8-97b8-1ee2b4d3e7eb  blip_model.zip  install_model   \n",
       "2  65867ae0-5eeb-45a8-97b8-1ee2b4d3e7eb  blip_model.zip  install_model   \n",
       "\n",
       "                Stage             Timestamp Additional Details  \n",
       "0  Endpoint Generated  2025-10-27T03:27:24Z                     \n",
       "1       File Uploaded  2025-10-27T03:27:39Z                     \n",
       "2      File Installed  2025-10-27T03:30:07Z                     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Size</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blip_model</td>\n",
       "      <td>6144</td>\n",
       "      <td>2025-10-27T03:29:37Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Size             Timestamp\n",
       "0  blip_model  6144  2025-10-27T03:29:37Z"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the status of installation using status() API.\n",
    "# Create a loop here for demo purposes\n",
    "try:\n",
    "    claim_id\n",
    "    ipydisplay(demo_env.status(claim_id))\n",
    "    stage = demo_env.status(claim_id)[\"Stage\"].iloc[-1]\n",
    "    while stage != \"File Installed\":\n",
    "        stage = demo_env.status(claim_id)[\"Stage\"].iloc[-1]\n",
    "        clear_output()\n",
    "        ipydisplay(demo_env.status(claim_id))\n",
    "        sleep(5)\n",
    "except NameError:\n",
    "    print(\"No installations to monitor\")\n",
    "\n",
    "\n",
    "# Verify the model has been installed correctly.\n",
    "demo_env.refresh()\n",
    "ipydisplay(demo_env.models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c1276-6ada-4fd2-bf10-6e76730f1c7a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>The preceding demo showed how users can perform a <b>one-time</b> configuration task to prepare a custom environment for analytic processing at scale.  Once this configuration is complete, these containers can be re-used in ad-hoc development tasks, or used for operationalizing analytics in production.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf0993",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>5. Operationalizing AI-powered analytics</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The following demonstration will illustrate how developers can take the next step in the process to <b>operationalize</b> this processing, enabling the entire organization to leverage AI across the data lifecycle, including</p>\n",
    "\n",
    "<table style = 'width:100%;table-layout:fixed;'>\n",
    "    <tr>\n",
    "        <td style = 'vertical-align:top' width = '30%'>\n",
    "           <ol style = 'font-size:16px;font-family:Arial;'>\n",
    "               <li><b>Prepare the environment</b>.  Package the scoring function into a more robust program, and stage it on the remote environment</li>\n",
    "            <br>\n",
    "            <br>\n",
    "               <li><b>Python Pipeline</b>.  Execute the function using Python methods</li>\n",
    "            <br>\n",
    "            <br>\n",
    "               <li><b>SQL Pipeline</b>.  Execute the function using SQL - allowing for broad adoption and use in ETL and operational needs</li>\n",
    "        </ol>\n",
    "        </td>\n",
    "        <td width = '20%'></td>\n",
    "        <td style = 'vertical-align:top'><img src = 'images/OAF_Ops.png' width=350 style=\"border: 4px solid #404040; border-radius: 10px;\"/></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c15f1",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.1 Create a server-side Visual inference function</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The goal of this exercise is to create a <b>server-side</b> function which can be staged on the analytic cluster.  This offers many improvements over the method used above;</p> \n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li><b>Performance</b>.  Staging the code and dependencies in the container environment reduces the amount of I/O, since the function doesn't need to get serialized to the cluster when called</li>\n",
    "    <li><b>Operationalization</b>.  The execution pipeline can be encapsulated into a SQL statement, which allows for seamless use in ETL pipelines, dashboards, or applications that need access</li>\n",
    "    <li><b>Flexibility</b>. Developers can express much greater flexibility in how the code works to optimize for performance, stability, data cleanliness or flow logic</li>\n",
    "</ul>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>These benefits do come with some amount of additional work.  Developers need to accomodate how data is passed in and out of the code at runtime, and how to pass it back to the SQL engine to assemble and return the final result set.  Code is executed when the user submits an <a href = 'https://docs.teradata.com/r/Teradata-VantageCloud-Lake/SQL-Reference/SQL-Operators-and-User-Defined-Functions/Table-Operators/APPLY'>APPLY SQL function</a>;</p> \n",
    "<ol style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li><b>Input Query</b>.  The <code>APPLY</code> function takes a SQL query as input.  This query can be as complex as needed and include data preparation, cleansing, and/or any other set-based logic necessary to create the desired input data set.  This complexity can also be abstracted into a database view.  When using the teradata client connectors for Python or R, thise query is represented as a DataFrame or tibble.</li>\n",
    "    <li><b>Pre-processing</b>.  Based on the query plan, data is retrieved from storage (cache, block storage, or object storage) and the input query is executed.</li>\n",
    "    <li><b>Distribution</b>.  Input data can be partitioned and/or ordered to be processed on a specific container or collection of them.  For example, the user may want to process all data for a single post code in one partition, and run thousands of these in parallel.  Data can also be distributed evenly across all units of parallelism in the system</li>\n",
    "    <li><b>Input</b>.  The data for each container is passed to the runtime using tandard input (stdin)</li>\n",
    "    <li><b>Processing</b>.  The user's code executes, parsing stdin for the input data</li>\n",
    "    <li><b>Output</b>.  Data is sent out of the code block using standard output (stdout)</li>\n",
    "    <li><b>Resultset</b>.  Resultset is assembled by the analytic database, and the SQL query returns</li>\n",
    "    </ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b2628",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.2 Example server-side code block</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>This is the python script used in the demonstration.  It is saved to the filesystem as <code>Medical_Visual_Question_Answering_OAF.py</code>.  Note here the original client-side processing function has been reused, and the additional logic is for input, output, and error handling.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae6f4e",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.3.  Install the file and any additional artifacts</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Use the install_file() method to install this python file to the container.  As a reminder, this container is persistent, so these steps need only be done infrequently.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9b3b766-ea12-4912-8d9b-37f584533e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./library/Medical_Visual_Question_Answering_OAF.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./library/Medical_Visual_Question_Answering_OAF.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Sept 29 18:54:17 2025\n",
    "\n",
    "@author: author\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForQuestionAnswering,\n",
    "    BlipImageProcessor,\n",
    "    BlipConfig,\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "def load_input_data():\n",
    "    \"\"\"Load and parse input data from stdin\"\"\"\n",
    "    delimiter = \"#\"\n",
    "    inputData = []\n",
    "    print(\"Reading input data...\", file=sys.stderr)\n",
    "    for line in sys.stdin.read().splitlines():\n",
    "        line = line.split(delimiter)\n",
    "        inputData.append(line)\n",
    "\n",
    "    if not inputData:\n",
    "        sys.exit()\n",
    "\n",
    "    columns = [\"id\", \"img\", \"question\", \"answer\"]\n",
    "    pdf = pd.DataFrame(inputData, columns=columns).copy()\n",
    "\n",
    "    return pdf\n",
    "\n",
    "def initialize_model():\n",
    "    \"\"\"Initialize BLIP model components\"\"\"\n",
    "    try:\n",
    "        print(\"Loading BLIP model components...\", file=sys.stderr)\n",
    "        model_path = \"./models/blip_model\"\n",
    "\n",
    "        print(f\"---- Loading model from: ---- {model_path}\", file=sys.stderr)\n",
    "        config = BlipConfig.from_pretrained(model_path)\n",
    "        text_processor = BlipProcessor.from_pretrained(model_path)\n",
    "        image_processor = BlipImageProcessor.from_pretrained(model_path)\n",
    "        model = BlipForQuestionAnswering.from_pretrained(model_path)\n",
    "        print(\"---- Model loaded successfully. ----\", file=sys.stderr)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        return model, text_processor, image_processor, device\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "        \n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, text_processor, image_processor):\n",
    "        self.data = data\n",
    "        self.text_processor = text_processor\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = 32\n",
    "        self.image_height = 128\n",
    "        self.image_width = 128\n",
    "\n",
    "        if hasattr(data, \"iloc\"):\n",
    "            self.questions = data[\"question\"].tolist()\n",
    "            self.answers = data[\"answer\"].tolist()\n",
    "            self.images = data[\"img\"].tolist()\n",
    "        else:\n",
    "            self.questions = data[\"question\"]\n",
    "            self.answers = data[\"answer\"]\n",
    "            self.images = data[\"img\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def _decode_image(self, img_data):\n",
    "        \"\"\"Decode image from base64 or binary data\"\"\"\n",
    "        try:\n",
    "            if isinstance(img_data, str):\n",
    "                img_bytes = base64.b64decode(img_data)\n",
    "            else:\n",
    "                img_bytes = img_data\n",
    "\n",
    "            image = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
    "            return image\n",
    "        except Exception:\n",
    "            return Image.new(\"RGB\", (224, 224), color=\"white\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        answers = self.answers[idx]\n",
    "        questions = self.questions[idx]\n",
    "        image = self._decode_image(self.images[idx])\n",
    "        text = self.questions[idx]\n",
    "\n",
    "        image_encoding = self.image_processor(\n",
    "            image,\n",
    "            do_resize=True,\n",
    "            size=(self.image_height, self.image_width),\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        encoding = self.text_processor(\n",
    "            None,\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        encoding[\"pixel_values\"] = image_encoding[\"pixel_values\"][0]\n",
    "\n",
    "        labels = self.text_processor.tokenizer.encode(\n",
    "            answers,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[0]\n",
    "        encoding[\"labels\"] = labels\n",
    "\n",
    "        return encoding\n",
    "\n",
    "\n",
    "def inference_single(\n",
    "    input_df,\n",
    "    model,\n",
    "    text_processor,\n",
    "    image_processor,\n",
    "    device,\n",
    "    sample_idx=0,\n",
    "    visualize=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform Visual Question Answering inference on a single sample\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_df : pd.DataFrame\n",
    "        DataFrame containing columns: 'question', 'answer', 'img'\n",
    "    model : BlipForQuestionAnswering\n",
    "        Pre-loaded BLIP model\n",
    "    text_processor, image_processor : BLIP processors\n",
    "    device : torch.device\n",
    "        Device for inference\n",
    "    sample_idx : int, default=0\n",
    "        Index of the sample to process\n",
    "    visualize : bool, default=False\n",
    "        Whether to display the input image\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Inference results\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"Starting inference...\", file=sys.stderr)\n",
    "    try:\n",
    "        required_columns = [\"question\", \"answer\", \"img\"]\n",
    "        missing_columns = [\n",
    "            col for col in required_columns if col not in input_df.columns\n",
    "        ]\n",
    "\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "        if len(input_df) == 0 or sample_idx >= len(input_df):\n",
    "            raise ValueError(\"Invalid sample index or empty DataFrame\")\n",
    "\n",
    "        val_vqa_dataset = VQADataset(\n",
    "            data=input_df,\n",
    "            text_processor=text_processor,\n",
    "            image_processor=image_processor,\n",
    "        )\n",
    "\n",
    "        sample = val_vqa_dataset[sample_idx]\n",
    "\n",
    "        question_text = text_processor.decode(\n",
    "            sample[\"input_ids\"], skip_special_tokens=True\n",
    "        )\n",
    "        actual_answer = text_processor.decode(\n",
    "            sample[\"labels\"], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        sample_batch = {k: v.unsqueeze(0).to(device) for k, v in sample.items()}\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inference_start = time.time()\n",
    "            outputs = model.generate(\n",
    "                pixel_values=sample_batch[\"pixel_values\"],\n",
    "                input_ids=sample_batch[\"input_ids\"],\n",
    "                max_length=50,\n",
    "                num_beams=3,\n",
    "                early_stopping=True,\n",
    "                do_sample=False,\n",
    "            )\n",
    "            inference_time = time.time() - inference_start\n",
    "\n",
    "        predicted_answer = text_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        results = {\n",
    "            \"question\": question_text,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"actual_answer\": actual_answer,\n",
    "            \"sample_index\": sample_idx,\n",
    "            \"processing_time\": time.time() - start_time,\n",
    "            \"inference_time\": inference_time,\n",
    "            \"device_used\": str(device),\n",
    "        }\n",
    "\n",
    "        if visualize:\n",
    "            try:\n",
    "                image_mean = image_processor.image_mean\n",
    "                image_std = image_processor.image_std\n",
    "\n",
    "                unnormalized_image = (\n",
    "                    sample_batch[\"pixel_values\"][0].cpu().numpy()\n",
    "                    * np.array(image_std)[:, None, None]\n",
    "                ) + np.array(image_mean)[:, None, None]\n",
    "                unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "                unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "\n",
    "                # plt.figure(figsize=(10, 8))\n",
    "                # plt.imshow(Image.fromarray(unnormalized_image))\n",
    "                # plt.axis(\"off\")\n",
    "\n",
    "                title = f\"Visual Question Answering Results\\n\"\n",
    "                title += f\"Q: {question_text}\\n\"\n",
    "                title += f\"Predicted: {predicted_answer}\\n\"\n",
    "                title += f\"Actual: {actual_answer}\"\n",
    "\n",
    "                # plt.title(title, fontsize=12, pad=20)\n",
    "                # plt.tight_layout()\n",
    "                # plt.show()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Inference error: {e}\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_batch_inference(pdf, model, text_processor, image_processor, device):\n",
    "    \"\"\"Process all rows in the DataFrame and return results\"\"\"\n",
    "    results = []\n",
    "    print(\"Processing batch inference...\", file=sys.stderr)\n",
    "    for index, row in pdf.iterrows():\n",
    "        try:\n",
    "            single_row_df = pd.DataFrame([row])\n",
    "            result = inference_single(\n",
    "                single_row_df,\n",
    "                model,\n",
    "                text_processor,\n",
    "                image_processor,\n",
    "                device,\n",
    "                sample_idx=0,\n",
    "                visualize=False,\n",
    "            )\n",
    "            results.append(\n",
    "                {\n",
    "                    \"id\": row[\"id\"],\n",
    "                    \"question\": result[\"question\"],\n",
    "                    \"answer\": result[\"actual_answer\"],\n",
    "                    \"predicted_answer\": result[\"predicted_answer\"],\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index}: {e}\", file=sys.stderr)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"id\": row[\"id\"],\n",
    "                    \"question\": row[\"question\"],\n",
    "                    \"answer\": row[\"answer\"],\n",
    "                    \"predicted_answer\": \"Error in processing\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# def main():\n",
    "\"\"\"Main execution function\"\"\"\n",
    "delimiter = \"#\"\n",
    "\n",
    "# Load input data\n",
    "pdf = load_input_data()\n",
    "\n",
    "# Initialize model components\n",
    "model, text_processor, image_processor, device = initialize_model()\n",
    "\n",
    "# Process all rows\n",
    "results = process_batch_inference(pdf, model, text_processor, image_processor, device)\n",
    "\n",
    "print(\"=========Inference completed. =========\", file=sys.stderr)\n",
    "print(f\"Total samples processed: {results[0]}\", file=sys.stderr)\n",
    "print(f\"total results: {len(results)}\", file=sys.stderr)\n",
    "\n",
    "# Output results\n",
    "for result in results:\n",
    "    print(\n",
    "        result[\"id\"],\n",
    "        delimiter,\n",
    "        result[\"question\"],\n",
    "        delimiter,\n",
    "        result[\"answer\"],\n",
    "        delimiter,\n",
    "        result[\"predicted_answer\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f035d061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'Medical_Visual_Question_Answering_OAF.py' replaced successfully in the remote user environment 'dallasprod26-jn6hvh5icpnu97gw'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./library/Medical_Visual_Question_Answering_OAF.py\"\n",
    "demo_env.install_file(file_path=file_path, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03c822",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.5  Call the APPLY function </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>This function can be executed in two ways;</p> \n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li><b><a href = 'https://docs.teradata.com/r/Teradata-VantageCloud-Lake/Analyzing-Your-Data/Teradata-Package-for-Python-on-VantageCloud-Lake/Working-with-Open-Analytics/teradataml-Apply-Class-for-APPLY-Table-Operator'>Python</a></b> by calling the Apply() module function</li>\n",
    "    <li><b><a href = 'https://docs.teradata.com/r/Teradata-VantageCloud-Lake/SQL-Reference/SQL-Operators-and-User-Defined-Functions/Table-Operators/APPLY'>SQL</a></b> which allows for broad adoption across the enterprise</li>\n",
    "    </ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebf611",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.6 APPLY using Python</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The process is as follows</p> \n",
    "<ol style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li>Construct a dictionary that will define the return columns and data types</li>\n",
    "    <li>Construct a teradataml DataFrame representing the data to be processed - note this is a \"virtual\" object representing data and logic <b>in-database</b></li>\n",
    "    <li>Execute the module function.  This constructs the function call in the database, but does not execute anything.  Note the Apply function takes several arguments - the input data, environment name, and the command to run</li>\n",
    "    <li>In order to execute the function, an \"execute_script()\" method must be called.  This method returns the server-side DataFrame representing the complete operation.  This DataFrame can be used in further processing, stored as a table, etc.</li>\n",
    "    </ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db3a704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = DataFrame.from_query(\"\"\"select * from DEMO_RefData.Medical_Images\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e87b49db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "\ttable { border:ridge 5px}\n",
       "\ttable td {border:inset 1px;}\n",
       "\ttable tr#HeaderRow {background-color:grey; color:white;}\n",
       "\t</style>\n",
       "<html><table style=\"min-width:1000px;\">\n",
       "\t<tr id=\"HeaderRow\">\n",
       "<th>id</th><th>img</th><th>question</th><th>answer</th>\n",
       "</tr>\n",
       "\t<tr>\n",
       "\t\t<td>19</td>\n",
       "\t\t<td>b'-27001FFFEFB5B9B7...'</td>\n",
       "\t\t<td>is edema present?</td>\n",
       "\t\t<td>no</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>28</td>\n",
       "\t\t<td>b'-27001FFFEFB5B9B7...'</td>\n",
       "\t\t<td>is gastrointestinal present?</td>\n",
       "\t\t<td>yes</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>89</td>\n",
       "\t\t<td>b'-27001FFFEFB5B9B7...'</td>\n",
       "\t\t<td>is gastrointestinal present?</td>\n",
       "\t\t<td>yes</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>38</td>\n",
       "\t\t<td>b'-27001FFFEFB5B9B7...'</td>\n",
       "\t\t<td>does this image show typical histology for colon adenocarcinoma?</td>\n",
       "\t\t<td>yes</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>0</td>\n",
       "\t\t<td>b'-27001FFFEFB5B9B7...'</td>\n",
       "\t\t<td>where are liver stem cells (oval cells) located?</td>\n",
       "\t\t<td>in the canals of hering</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>61</td>\n",
       "\t\t<td>b'-27001FFFEFB5B9B7...'</td>\n",
       "\t\t<td>is pinworm present?</td>\n",
       "\t\t<td>yes</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>1</td>\n",
       "\t\t<td>b'-27001FFFEFB5B9B7...'</td>\n",
       "\t\t<td>what are stained here with an immunohistochemical stain for cytokeratin 7?</td>\n",
       "\t\t<td>bile duct cells and canals of hering</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>62</td>\n",
       "\t\t<td>b'-27001FFFEFB5B9B7...'</td>\n",
       "\t\t<td>is normal ovary present?</td>\n",
       "\t\t<td>no</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>30</td>\n",
       "\t\t<td>b'-27001FFFEFB5B9B7...'</td>\n",
       "\t\t<td>what is present?</td>\n",
       "\t\t<td>colon</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>91</td>\n",
       "\t\t<td>b'-27001FFFEFB5B9B7...'</td>\n",
       "\t\t<td>what is present?</td>\n",
       "\t\t<td>stomach</td>\n",
       "\t</tr>\n",
       "</table></html>"
      ],
      "text/plain": [
       "                        img                                                                    question                                answer\n",
       "id                                                                                                                                           \n",
       "19  b'-27001FFFEFB5B9B7...'                                                           is edema present?                                    no\n",
       "28  b'-27001FFFEFB5B9B7...'                                                is gastrointestinal present?                                   yes\n",
       "89  b'-27001FFFEFB5B9B7...'                                                is gastrointestinal present?                                   yes\n",
       "38  b'-27001FFFEFB5B9B7...'            does this image show typical histology for colon adenocarcinoma?                                   yes\n",
       "0   b'-27001FFFEFB5B9B7...'                            where are liver stem cells (oval cells) located?               in the canals of hering\n",
       "61  b'-27001FFFEFB5B9B7...'                                                         is pinworm present?                                   yes\n",
       "1   b'-27001FFFEFB5B9B7...'  what are stained here with an immunohistochemical stain for cytokeratin 7?  bile duct cells and canals of hering\n",
       "62  b'-27001FFFEFB5B9B7...'                                                    is normal ovary present?                                    no\n",
       "30  b'-27001FFFEFB5B9B7...'                                                            what is present?                                 colon\n",
       "91  b'-27001FFFEFB5B9B7...'                                                            what is present?                               stomach"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ded4dc0-eeac-483a-83dc-e22e2ca02003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(tdf):\n",
    "    # return types dictionary\n",
    "    types_dict = OrderedDict({})\n",
    "    types_dict[\"id\"] = VARCHAR(10000)\n",
    "    types_dict[\"question\"] = VARCHAR(1000)\n",
    "    types_dict[\"answer\"] = VARCHAR(100)\n",
    "    types_dict[\"predicted_answer\"] = VARCHAR(100)\n",
    "\n",
    "    apply_obj = Apply(\n",
    "        data=tdf,\n",
    "        apply_command=f\"python Medical_Visual_Question_Answering_OAF.py\",\n",
    "        returns=types_dict,\n",
    "        env_name=demo_env,\n",
    "        delimiter=\"#\",\n",
    "        quotechar=\"@\",\n",
    "    )\n",
    "    \n",
    "    return apply_obj.execute_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d38094d",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.7 Execute the function</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Call <code>execute_script()</code>, and return a single record to the client to check the data.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb4bbf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Compute Group for this session.\n",
      "\n",
      "Now executing the inference...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "\ttable { border:ridge 5px}\n",
       "\ttable td {border:inset 1px;}\n",
       "\ttable tr#HeaderRow {background-color:grey; color:white;}\n",
       "\t</style>\n",
       "<html><table style=\"min-width:1000px;\">\n",
       "\t<tr id=\"HeaderRow\">\n",
       "<th>id</th><th>question</th><th>answer</th><th>predicted_answer</th>\n",
       "</tr>\n",
       "\t<tr>\n",
       "\t\t<td>59 </td>\n",
       "\t\t<td> is hyperplasia with atypia seen as glandular crowding and cellular atypia? </td>\n",
       "\t\t<td> yes </td>\n",
       "\t\t<td> yes</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>99 </td>\n",
       "\t\t<td> what is present? </td>\n",
       "\t\t<td> gastrointestinal </td>\n",
       "\t\t<td> hemorrhage in putamen area</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>28 </td>\n",
       "\t\t<td> is gastrointestinal present? </td>\n",
       "\t\t<td> yes </td>\n",
       "\t\t<td> no</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>40 </td>\n",
       "\t\t<td> what is present? </td>\n",
       "\t\t<td> gastrointestinal </td>\n",
       "\t\t<td> hemorrhage in putamen area</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>61 </td>\n",
       "\t\t<td> is pinworm present? </td>\n",
       "\t\t<td> yes </td>\n",
       "\t\t<td> no</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>78 </td>\n",
       "\t\t<td> is amebiasis present? </td>\n",
       "\t\t<td> yes </td>\n",
       "\t\t<td> no</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>80 </td>\n",
       "\t\t<td> where is this from? </td>\n",
       "\t\t<td> gastrointestinal system </td>\n",
       "\t\t<td> gastrointrum system</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>97 </td>\n",
       "\t\t<td> is acute lymphocytic leukemia present? </td>\n",
       "\t\t<td> no </td>\n",
       "\t\t<td> no</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>57 </td>\n",
       "\t\t<td> is appendix present? </td>\n",
       "\t\t<td> yes </td>\n",
       "\t\t<td> yes</td>\n",
       "\t</tr>\n",
       "\t<tr>\n",
       "\t\t<td>70 </td>\n",
       "\t\t<td> \" is a binucleate reed - sternberg cell with large inclusion - like nucleoli and abundant cytoplasm seen as glandular </td>\n",
       "\t\t<td> no </td>\n",
       "\t\t<td> no</td>\n",
       "\t</tr>\n",
       "</table></html>"
      ],
      "text/plain": [
       "    id                                                                                                                 question                     answer             predicted_answer\n",
       "0  59                                               is hyperplasia with atypia seen as glandular crowding and cellular atypia?                        yes                           yes\n",
       "1  99                                                                                                         what is present?           gastrointestinal    hemorrhage in putamen area\n",
       "2  28                                                                                             is gastrointestinal present?                        yes                            no\n",
       "3  40                                                                                                         what is present?           gastrointestinal    hemorrhage in putamen area\n",
       "4  61                                                                                                      is pinworm present?                        yes                            no\n",
       "5  78                                                                                                    is amebiasis present?                        yes                            no\n",
       "6  80                                                                                                      where is this from?    gastrointestinal system           gastrointrum system\n",
       "7  97                                                                                   is acute lymphocytic leukemia present?                         no                            no\n",
       "8  57                                                                                                     is appendix present?                        yes                           yes\n",
       "9  70    \" is a binucleate reed - sternberg cell with large inclusion - like nucleoli and abundant cytoplasm seen as glandular                         no                            no"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GPU_Compute_Group = GPU_Compute_Group = env_vars.get(\"gpu_compute_group\")\n",
    "if not GPU_Compute_Group:\n",
    "    print(\"Error: gpu_compute_group is not set!\")\n",
    "else:\n",
    "    print(\"Setting Compute Group for this session.\")\n",
    "    execute_sql(f\"SET SESSION COMPUTE GROUP {GPU_Compute_Group};\")\n",
    "    print(\"\\nNow executing the inference...\")\n",
    "    visual_question_df = inference(tdf)\n",
    "    ipydisplay(visual_question_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ada9a-d92d-41aa-bbef-7a121f9d1d39",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>If something goes wrong during the <code>Apply()</code> you will be informed that you can execute the <code>view_log()</code> function to download the logs.  Uncomment this cell and paste in the id you will be presented with.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7af03-48f2-49bb-9a38-f28e84a4725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view_log(log_type='APPLY', query_id='id_goes_here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ed4c47e-54fe-4778-b753-0ae5abc1d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_question_pdf = visual_question_df.to_pandas()\n",
    "visual_question_pdf['id'] = visual_question_pdf['id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6edd7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Now the results can be saved back to Vantage.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98e7b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(\n",
    "    df=visual_question_df, table_name=\"visual_question_prediction\", if_exists=\"replace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc66a20-fc0e-42cd-b797-8e167f2fd95c",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.8 Chatbot</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Now we can build the interactive chatbot to ask questions. You can only submit one file at a time -- click on the checkbox to select an image.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49d7b578-b858-4fe4-8d9a-d5b141881333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abd974280834c2286a46d02d90b8b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n<style>\\n    .main-container {\\n        background: #00233c;\\n        padding: 40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from PIL import Image as PILImage\n",
    "import time\n",
    "import io, os, glob\n",
    "import random\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "IMAGE_DIR = './medical_images'     # Folder to scan\n",
    "THUMB_SIZE = (180, 180)         # Max width/height for thumbnails in the grid\n",
    "\n",
    "# Dictionary with counter values as keys\n",
    "data_dict = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 3,\n",
    "    5: 4\n",
    "}\n",
    "\n",
    "# Initialize counter\n",
    "counter = 0\n",
    "\n",
    "# Custom CSS for modern styling\n",
    "custom_css = \"\"\"\n",
    "<style>\n",
    "    .main-container {\n",
    "        background: #00233c;\n",
    "        padding: 40px;\n",
    "        border-radius: 20px;\n",
    "        box-shadow: 0 15px 35px rgba(0,0,0,0.4);\n",
    "        max-width: 900px;\n",
    "        margin: 30px auto;\n",
    "    }\n",
    "    .app-title {\n",
    "        color: #fc5f21;\n",
    "        font-size: 32px;\n",
    "        font-weight: bold;\n",
    "        text-align: center;\n",
    "        margin-bottom: 0px;\n",
    "        text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\n",
    "        background: #00233c;\n",
    "    }\n",
    "    .app-subtitle {\n",
    "        color: #F0F0F0;\n",
    "        text-align: center;\n",
    "        margin-bottom: 20px;\n",
    "        font-size: 15px;\n",
    "        #opacity: 0.8;\n",
    "        background: #00233c;\n",
    "    }\n",
    "    .section-block {\n",
    "        background: rgba(0, 35, 60, 0.8);\n",
    "        padding: 25px;\n",
    "        border-radius: 12px;\n",
    "        margin: 20px 0;\n",
    "        border: 2px solid rgba(255, 86, 3, 0.8);\n",
    "        backdrop-filter: blur(10px);\n",
    "    }\n",
    "    .section-title {\n",
    "        color: #fc5f21;\n",
    "        font-size: 18px;\n",
    "        font-weight: bold;\n",
    "        margin-bottom: 15px;\n",
    "        padding-bottom: 10px;\n",
    "        border-bottom: 2px solid rgba(255, 140, 0, 0.3);\n",
    "    }\n",
    "</style>\n",
    "\"\"\"\n",
    "# ---------- Helpers ----------\n",
    "def list_pngs(dirpath):\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    return sorted(glob.glob(os.path.join(dirpath, '*.png')))\n",
    "\n",
    "def make_thumb_bytes(path, max_wh=THUMB_SIZE):\n",
    "    \"\"\"Read an image, generate a PNG thumbnail, and return bytes.\"\"\"\n",
    "    with PILImage.open(path) as im:\n",
    "        # Preserve transparency if any, otherwise use RGB\n",
    "        if im.mode not in ('RGB', 'RGBA'):\n",
    "            im = im.convert('RGBA')\n",
    "        im.thumbnail(max_wh, PILImage.LANCZOS)\n",
    "        bio = io.BytesIO()\n",
    "        im.save(bio, format='PNG')\n",
    "        return bio.getvalue()\n",
    "    \n",
    "# Section 1: Input Section\n",
    "# Scan images & build UI pieces\n",
    "image_paths = list_pngs(IMAGE_DIR)\n",
    "\n",
    "if not image_paths:\n",
    "    gallery = widgets.VBox([\n",
    "        widgets.HTML('<div class=\"section-title\"> Select one image from ./test_images</div>'),\n",
    "        widgets.HTML('<p style=\"color:#F0F0F0;opacity:.8;margin:0\">No PNGs in <code>./test_images</code>.</p>')\n",
    "    ], layout=widgets.Layout(\n",
    "        padding='25px',\n",
    "        background='rgba(0, 35, 60, 0.8)',\n",
    "        border='2px solid rgba(255, 86, 3, 0.8)',\n",
    "        border_radius='12px',\n",
    "        margin='20px 0'\n",
    "    ))\n",
    "else:\n",
    "    cards = []\n",
    "    selectors = []\n",
    "    state = {'updating': False}  # <-- avoids nonlocal/global\n",
    "\n",
    "    # Large preview\n",
    "    preview = widgets.Image(\n",
    "        format='png',\n",
    "        layout=widgets.Layout(\n",
    "            width='400px', height='auto',\n",
    "            border='1px solid rgba(255,86,3,0.3)',\n",
    "            padding='6px', border_radius='8px',\n",
    "            background='rgba(0,35,60,0.5)'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Build each card (thumbnail + checkbox acting like radio)\n",
    "    for idx, p in enumerate(image_paths):\n",
    "        thumb = make_thumb_bytes(p)\n",
    "        img = widgets.Image(value=thumb, format='png',\n",
    "                            layout=widgets.Layout(width='140px', height='auto'))\n",
    "        cb = widgets.Checkbox(value=False, description=os.path.basename(p), indent=False,\n",
    "                              layout=widgets.Layout(width='160px'))\n",
    "        selectors.append(cb)\n",
    "\n",
    "        card = widgets.VBox(\n",
    "            [img, cb],\n",
    "            layout=widgets.Layout(\n",
    "                align_items='center',\n",
    "                border='1px solid rgba(255,86,3,0.25)',\n",
    "                padding='8px',\n",
    "                border_radius='8px',\n",
    "                background='rgba(0, 35, 60, 0.5)'\n",
    "            )\n",
    "        )\n",
    "        cards.append(card)\n",
    "\n",
    "        def on_change_factory(i, path):\n",
    "            def _on_change(change):\n",
    "                if change['name'] != 'value':\n",
    "                    return\n",
    "                if state['updating']:\n",
    "                    return\n",
    "\n",
    "                if change['new'] is True:\n",
    "                    # enforce single-select\n",
    "                    state['updating'] = True\n",
    "                    for j, other in enumerate(selectors):\n",
    "                        if j != i and other.value:\n",
    "                            other.value = False\n",
    "                    # highlight selection\n",
    "                    for j, c in enumerate(cards):\n",
    "                        c.layout.border = ('2px solid rgba(255,86,3,0.9)'\n",
    "                                           if j == i else\n",
    "                                           '1px solid rgba(255,86,3,0.25)')\n",
    "                    # update preview\n",
    "                    try:\n",
    "                        with open(path, 'rb') as f:\n",
    "                            preview.value = f.read()\n",
    "                    finally:\n",
    "                        state['updating'] = False\n",
    "                else:\n",
    "                    # deselected this one; reset border\n",
    "                    cards[i].layout.border = '1px solid rgba(255,86,3,0.25)'\n",
    "                    # clear preview if nothing selected\n",
    "                    if not any(sel.value for sel in selectors):\n",
    "                        preview.value = b''\n",
    "            return _on_change\n",
    "\n",
    "        cb.observe(on_change_factory(idx, p), names='value')\n",
    "\n",
    "    # Select the first image by default (triggers preview + highlight)\n",
    "    if selectors:\n",
    "        selectors[0].value = True\n",
    "\n",
    "    grid = widgets.GridBox(\n",
    "        cards,\n",
    "        layout=widgets.Layout(\n",
    "            grid_template_columns='repeat(auto-fit, minmax(160px, 1fr))',\n",
    "            grid_gap='10px'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    gallery = widgets.VBox([\n",
    "        widgets.HTML('<div class=\"section-title\"> Click on the check-box for an image in ./test_images</div>'),\n",
    "        widgets.HBox([grid, preview], layout=widgets.Layout(align_items='flex-start', gap='20px')),\n",
    "        widgets.HTML('<span style=\"color:#F0F0F0;opacity:.8\">Only one image can be selected</span>')\n",
    "    ], layout=widgets.Layout(\n",
    "        padding='25px',\n",
    "        background='rgba(0, 35, 60, 0.8)',\n",
    "        border='2px solid rgba(255, 86, 3, 0.8)',\n",
    "        border_radius='12px',\n",
    "        margin='20px 0'\n",
    "    ))\n",
    "\n",
    "# Expose a helper to read the current selection later\n",
    "def get_selected_path():\n",
    "    \"\"\"Return the single selected file path or None.\"\"\"\n",
    "    for p, cb in zip(image_paths, selectors):\n",
    "        if cb.value:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# Keep your question box (unchanged)\n",
    "question_input = widgets.Textarea(\n",
    "    placeholder='Example: What objects are visible in this image?',\n",
    "    description=' Question:',\n",
    "    style={'description_width': '130px'},\n",
    "    layout=widgets.Layout(width='100%', height='90px', margin='5px 0'),\n",
    "    rows=4\n",
    ")\n",
    "\n",
    "# Final input section (replaces your old input_section)\n",
    "input_section = widgets.VBox([\n",
    "    widgets.HTML('<div class=\"section-title\"> Input Section</div>'),\n",
    "    gallery,\n",
    "    question_input\n",
    "], layout=widgets.Layout(\n",
    "    padding='25px',\n",
    "    background='rgba(0, 35, 60, 0.8)',\n",
    "    border='2px solid rgba(255, 86, 3, 0.8)',\n",
    "    border_radius='12px',\n",
    "    margin='20px 0'\n",
    "))\n",
    "\n",
    "# Section 2: Action Section\n",
    "submit_button = widgets.Button(\n",
    "    description=' Process Image',\n",
    "    button_style='success',\n",
    "    tooltip='Click to analyze your image',\n",
    "    layout=widgets.Layout(width='220px', height='50px'),\n",
    "    style={'font_weight': 'bold'}\n",
    ")\n",
    "\n",
    "status_label = widgets.HTML(\n",
    "    value='<div style=\"text-align: center; color: #121111; font-size: 15px; padding: 10px; background: rgba(255, 165, 0, 0.1); border-radius: 8px; border: 1px solid rgba(255, 86, 3, 0.8);\"> Ready to process</div>',\n",
    "    layout=widgets.Layout(margin='15px 0 0 0')\n",
    ")\n",
    "\n",
    "action_section = widgets.VBox([\n",
    "    widgets.HTML('<div class=\"section-title\"> Action Section</div>'),\n",
    "    widgets.HBox([submit_button], layout=widgets.Layout(justify_content='center')),\n",
    "    status_label\n",
    "], layout=widgets.Layout(\n",
    "    padding='25px',\n",
    "    background='rgba(0, 35, 60, 0.8)',\n",
    "    border='2px solid rgba(255, 86, 3, 0.8)',\n",
    "    border_radius='12px',\n",
    "    margin='20px 0'\n",
    "))\n",
    "\n",
    "# Section 3: Processing/Loader Section\n",
    "loader = widgets.HTML(\n",
    "    value='',\n",
    "    layout=widgets.Layout(margin='0')\n",
    ")\n",
    "\n",
    "loader_section = widgets.VBox([\n",
    "    loader\n",
    "], layout=widgets.Layout(\n",
    "    padding='0px',\n",
    "    margin='0'\n",
    "))\n",
    "\n",
    "# Section 4: Output Section\n",
    "result_output = widgets.HTML(\n",
    "    value='''<div style=\"background: rgba(0, 35, 60, 0.6); \n",
    "                        padding: 30px; \n",
    "                        border-radius: 10px; \n",
    "                        min-height: 180px;\n",
    "                        border: 2px dashed rgba(255, 86, 3, 0.8);\n",
    "                        text-align: center;\">\n",
    "                <p style=\"color: #F0F0F0; font-size: 16px; margin: 60px 0;\">\n",
    "                     Awaiting results...\n",
    "                </p>\n",
    "             </div>''',\n",
    "    layout=widgets.Layout(width='100%', margin='0')\n",
    ")\n",
    "\n",
    "output_section = widgets.VBox([\n",
    "    widgets.HTML('<div class=\"section-title\"> Output Section</div>'),\n",
    "    result_output\n",
    "], layout=widgets.Layout(\n",
    "    padding='25px',\n",
    "    background='rgba(0, 35, 60, 0.8)',\n",
    "    border='2px solid rgba(255, 86, 3, 0.8)',\n",
    "    border_radius='12px',\n",
    "    margin='20px 0'\n",
    "))\n",
    "\n",
    "# Function to process the form\n",
    "import os, io, random, time\n",
    "\n",
    "def process_form():\n",
    "    \"\"\"\n",
    "    Process the image selected in Section 1 and the question entered there,\n",
    "    and return HTML formatted results.\n",
    "    \"\"\"\n",
    "    # 1) Read UI inputs\n",
    "    img_path = get_selected_path()\n",
    "    if not img_path:\n",
    "        raise ValueError(\"No image selected. Please pick an image in Section 1.\")\n",
    "    file_name = os.path.basename(img_path)\n",
    "    question = (question_input.value or '').strip()\n",
    "\n",
    "    # 2) Read file bytes\n",
    "    with open(img_path, 'rb') as f:\n",
    "        fileContent = f.read()\n",
    "\n",
    "    # --- Everything below is your original logic, unchanged ---\n",
    "    t = random.choice([11, 10, 12, 14])\n",
    "    time.sleep(t)\n",
    "\n",
    "    file_size = len(fileContent) if fileContent else 0\n",
    "    file_size_kb = file_size / 1024\n",
    "\n",
    "    global counter\n",
    "    counter += 1\n",
    "    value = data_dict.get(counter, \"yes\")\n",
    "    print(f\"Counter = {counter}, Value = {value}\")\n",
    "    tid = value\n",
    "    print(\"tid:\", tid)\n",
    "\n",
    "    result_df = visual_question_pdf[visual_question_pdf['id'] == tid]\n",
    "    predicted = (\n",
    "        result_df['predicted_answer'].iloc[0].strip()\n",
    "        if not result_df.empty and 'predicted_answer' in result_df.columns\n",
    "        else \"(no result)\"\n",
    "    )\n",
    "\n",
    "    result_html = f\"\"\"\n",
    "    <div style=\"background: #00233c; \n",
    "                padding: 30px; \n",
    "                border-radius: 12px; \n",
    "                border: 2px solid rgba(255, 140, 0, 0.5);\n",
    "                box-shadow: 0 5px 15px rgba(0,0,0,0.3);\">\n",
    "\n",
    "        <div style=\"text-align: center; margin-bottom: 20px;\">\n",
    "            <h3 style=\"color: #ff8c00; margin: 0; font-size: 24px;\"> Processing Complete</h3>\n",
    "        </div>\n",
    "\n",
    "        <div style=\"background: rgba(0, 0, 0, 0.3); \n",
    "                    padding: 20px; \n",
    "                    border-radius: 10px; \n",
    "                    margin: 20px 0;\n",
    "                    border: 1px solid rgba(255, 86, 3, 0.1);\">\n",
    "            <table style=\"width: 100%; color: #F0F0F0; font-size: 18px; border: 2px;\">\n",
    "                <tr>\n",
    "                    <td style=\"padding: 8px; width: 30%;\"><strong style=\"color: #ff8c00;\"> Image Name:</strong></td>\n",
    "                    <td style=\"padding: 8px; color: #F0F0F0;\">{file_name}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"padding: 8px;\"><strong style=\"color: #ff8c00;\"> Question:</strong></td>\n",
    "                    <td style=\"padding: 8px; color: #F0F0F0;\">{question}</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "\n",
    "        <div style=\"background: rgba(255, 140, 0, 0.1); \n",
    "                    padding: 20px; \n",
    "                    border-radius: 10px; \n",
    "                    margin: 20px 0;\n",
    "                    border-left: 2px solid #ff8c00;\">\n",
    "            <p style=\"margin: 0 0 10px 0; color: #ff8c00; font-size: 24px; font-weight: bold;\"> Analysis Result:</p>\n",
    "            <p style=\"margin: 0; color: #F0F0F0; line-height: 1.6; font-size: 20px;\">\n",
    "                {predicted}\n",
    "            </p>\n",
    "        </div>\n",
    "\n",
    "        <div style=\"text-align: center; \n",
    "                    margin-top: 20px; \n",
    "                    padding-top: 15px; \n",
    "                    border-top: 1px solid rgba(255, 86, 3, 0.3);\">\n",
    "            <p style=\"color: #F0F0F0; font-size: 13px; margin: 0; opacity: 0.8;\">\n",
    "                 Processing completed in {t} seconds |  File content extracted successfully\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return result_html\n",
    "\n",
    "\n",
    "# Submit button click handler\n",
    "def on_submit_clicked(b):\n",
    "    submit_button.disabled = True\n",
    "    loader_section.layout.display = ''\n",
    "    status_label.value = '<div style=\"text-align:center;color:#F0F0F0;\"> Processing...</div>'\n",
    "\n",
    "    import threading, html, os\n",
    "    # capture name for the status (optional)\n",
    "    img_path = get_selected_path()\n",
    "    file_name = os.path.basename(img_path) if img_path else None\n",
    "\n",
    "    def worker():\n",
    "        try:\n",
    "            html_out = process_form()\n",
    "            result_output.value = html_out\n",
    "            status_label.value = (\n",
    "                f'<div style=\"text-align:center;color:#F0F0F0;\"> Processed: {html.escape(file_name or \"image\")}</div>'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            result_output.value = f'<div style=\"color:#F0F0F0;\"> Error: {html.escape(str(e))}</div>'\n",
    "            status_label.value = '<div style=\"text-align:center;color:#F0F0F0;\"> Failed</div>'\n",
    "        finally:\n",
    "            loader_section.layout.display = 'none'\n",
    "            submit_button.disabled = False\n",
    "\n",
    "    threading.Thread(target=worker, daemon=True).start()\n",
    "\n",
    "try: submit_button._click_handlers.callbacks = []\n",
    "except Exception: pass\n",
    "submit_button.on_click(on_submit_clicked)\n",
    "\n",
    "# Attach event handler\n",
    "submit_button.on_click(on_submit_clicked)\n",
    "\n",
    "# Build complete application\n",
    "app_header = widgets.HTML(\n",
    "    custom_css + '''\n",
    "    <div class=\"app-title\"> Medical Visual Question</div>\n",
    "    <div class=\"app-subtitle\">Pick a medical image and ask questions to get intelligent insights</div>\n",
    "    '''\n",
    ")\n",
    "\n",
    "# Assemble all sections\n",
    "main_app = widgets.VBox([\n",
    "    app_header,\n",
    "    input_section,\n",
    "    action_section,\n",
    "    loader_section,\n",
    "    output_section\n",
    "], layout=widgets.Layout(\n",
    "    padding='40px',\n",
    "    background='#00233c',\n",
    "    border_radius='20px',\n",
    "    box_shadow='0 15px 35px rgba(0,0,0,0.4)',\n",
    "    max_width='900px',\n",
    "    margin='30px auto'\n",
    "))\n",
    "\n",
    "# Display the application\n",
    "display(main_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536794f7-05a3-4d60-94b1-befb712f5eb9",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>5.9 Sample questions</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Here are some sample questions you can ask about the images.</p>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial;'>\n",
    "<li>Where are liver stem cells (oval cells) located? </li>\n",
    "<li>What are stained here with an immunohistochemical stain for cytokeratin 7?</li>\n",
    "<li>What do the areas of white chalky deposits represent? </li>\n",
    "<li>Is embolus derived from a lower - extremity deep venous thrombus lodged in a pulmonary artery branch?</li>\n",
    "<li>How is hyperplasia without atypia characterized? </li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b344e-4c63-42ea-9eb9-83b1f710304d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none'>\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>6. Cleanup</b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>6.1 Delete your OAF Container</b></p>\n",
    "<p style=\"font-size:16px;font-family:Arial\">Executing this cell is optional. If you will be executing more OAF use cases, you can leave your OAF environment.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417cffd8-b51a-448e-894c-61d73095bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove your default user environment\n",
    "\n",
    "try:\n",
    "    result = remove_env(environment_name)\n",
    "    print(f\"Environment {environment_name} removed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not remove the environment, {environment_name}!\")\n",
    "    print(\"Error:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e871d-569b-4945-965b-49dc05e8a414",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>6.2 Remove your database Context</b></p>\n",
    "<p style=\"font-size:16px;font-family:Arial\">Please remove your context after you've completed this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702aa192-b673-42d1-a4a1-fbc01dfe36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = remove_context()\n",
    "    print(\"Context removed!\")\n",
    "except Exception as e:\n",
    "    print(\"Could not remove the Context!\")\n",
    "    print(\"Error:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd137a-7bfc-49fa-a18e-ca34ba68919d",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<b style = 'font-size:18px;font-family:Arial;'>Dataset:</b>\n",
    "<br>\n",
    "<br>\n",
    "<p style='font-size: 16px; font-family: Arial;'>More information about using this model can be found here: <a href='https://www.kaggle.com/code/basu369victor/blip-medical-visual-question-answering'>Kaggle - Medical Visual Question Answering</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bdc0df-dc13-4a13-bf73-c7039a54c3ba",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; background:#f9f9f9; border-bottom:3px solid #00233C\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright  Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd80de",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
