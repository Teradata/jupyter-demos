{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9944e564-a16d-44fb-b9e0-0dd16a45eb1f",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Shipping Delivery Delay Prediction using Open Table Format Tables\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed6f6f-da47-40ca-8877-6849fc54060a",
   "metadata": {},
   "source": [
    "<p style=\"font-size:20px;font-family:Arial\"><b>Introduction</b></p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial\">\n",
    "The rapid growth in global shipping and e-commerce demand has significantly outpaced the operational capabilities of many logistics providers. While order volumes continue to rise, improvements in delivery infrastructure, capacity planning, and real-time visibility have not kept pace. As a result, delayed deliveries have become an increasingly common and critical risk across multiple industries.<br>\n",
    "One of the primary contributors to this challenge is the fragmentation of enterprise systems and data. Logistics and retail organizations often rely on multiple, disconnected systems—such as order management, warehouse management, transportation management, and partner platforms—that maintain data in different formats and across different locations. This lack of standardization and integration limits end-to-end visibility, slows decision-making, and increases the risk of inconsistencies across the supply chain.<br>\n",
    "In the e-commerce retail sector, delivery delays have a direct and cascading impact on the supply chain. Late shipments disrupt inventory replenishment cycles, delay order fulfillment, and reduce operational efficiency. Beyond internal disruptions, these delays also negatively affect customer experience. Buyers expect fast and reliable delivery, and failure to meet these expectations leads to dissatisfaction, loss of trust, and reduced customer loyalty—ultimately harming the retailer’s credibility and long-term growth.\n",
    "</p>\n",
    "<p style=\"font-size:18px;font-family:Arial\"><b>Why Teradata</b></p>\n",
    "<p style=\"font-size:16px;font-family:Arial\">\n",
    " Teradata addresses these challenges by providing a <b>unified analytics platform capable of performing large-scale, in-database analysis on massive volumes of data</b>. Its massively parallel processing (MPP) architecture enables complex analytics—such as delivery performance analysis, delay prediction, and supply chain optimization—to be executed directly where the data resides, eliminating the need for costly data movement and improving time to insight.<br>   \n",
    "    In addition, Teradata natively <b>supports Open Table Formats (OTF)</b> such as Apache Iceberg, allowing it to read and analyze data stored in open, cloud-native formats alongside traditional relational tables. This capability enables organizations to seamlessly combine structured enterprise data with high-volume operational and logistics data stored in data lakes, all within a single SQL and governance framework. By unifying fragmented datasets and enabling scalable in-database analytics across both native and OTF tables, Teradata empowers organizations to gain real-time visibility, improve delivery reliability, and make faster, data-driven decisions across the end-to-end supply chain.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c7c2b-0216-451c-b5e5-8972c51fbe63",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>1. Connect to VantageCloud</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Connect to VantageCloud using <code>create_context</code> from the teradataml Python library. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2016c4-a193-4fdd-8ba7-28fb5a769935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from teradataml import *\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "display.suppress_vantage_runtime_warnings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a1034-78fe-4866-9b31-45972607ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking if this environment is ready to connect to VantageCloud Lake...\")\n",
    "\n",
    "if os.path.exists(\"/home/jovyan/JupyterLabRoot/VantageCloud_Lake/.config/.env\"):\n",
    "    print(\"Your environment parameter file exist.  Please proceed with this use case.\")\n",
    "    # Load all the variables from the .env file into a dictionary\n",
    "    env_vars = dotenv_values(\"/home/jovyan/JupyterLabRoot/VantageCloud_Lake/.config/.env\")\n",
    "    # Create the Context\n",
    "    eng = create_context(host=env_vars.get(\"host\"), username=env_vars.get(\"username\"), password=env_vars.get(\"my_variable\"))\n",
    "    execute_sql('''SET query_band='DEMO=VCL_Shipping_Delivery_Delay_Prediction_using_OTF.ipynb;' UPDATE FOR SESSION;''')\n",
    "    print(\"Connected to VantageCloud Lake with:\", eng)\n",
    "else:\n",
    "    print(\"Your environment has not been prepared for connecting to VantageCloud Lake.\")\n",
    "    print(\"Please contact the support team.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b84d8d-b9a5-44ef-8782-9833282f0494",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'> <b> 2. Data exploring  </b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1806498e-f17b-4e7f-a967-7bb0418c002f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "The following tables are present in the database\n",
    "    <ul style = 'font-size:16px;font-family:Arial'>  \n",
    "        <li><b>orderitems</b>:This table contains a mapping between orders placed by customers and the table of products purchased</li>\n",
    "        <li><b>orders</b>:This table contains data on orders placed by each user. This table is divided into two tables train_orders which have 3 additional columns related to delivery details namely order_status,order_delivered_timestamp and order_estimated_delivery_date</li>\n",
    "        <li><b>payments</b>: This table contains payments made by each user, containing payment details and transaction value</li>\n",
    "        </ul>\n",
    "<p style = 'font-size:16px;font-family:Arial'>        The following tables are OTF format in cloud\n",
    "    <ul style = 'font-size:16px;font-family:Arial'> \n",
    "        <li><b>products</b> :This table contains a list of products sold on ecccomercce and contained in transactions</li>\n",
    "        <li><b>customers</b>: This table contains data on customers who make product transactions.\n",
    "        </li>\n",
    "        </ul>\n",
    "<p style = 'font-size:16px;font-family:Arial'> Let's look at the detailed contents of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be6588-40c9-4598-930c-eead4818f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_orderitems=DataFrame(in_schema(\"DEMO_Shipment\",\"orderitems\"))\n",
    "tdf_orderitems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02818578-01aa-4545-b633-95cef35c7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_order=DataFrame(in_schema(\"DEMO_Shipment\",\"orders\"))\n",
    "tdf_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ad0bf-1a6e-4c50-bc36-fa94af8686be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_pay=DataFrame(in_schema(\"DEMO_Shipment\",\"payments\"))\n",
    "tdf_pay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf140d55-29a5-4a0e-9a6b-5608ae68e3c1",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>2.1 Connecting Open Table Format tables</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "An open table format (OTF) is a standardized, engine-agnostic way of organizing and managing large analytical datasets in data lakes, separating storage from compute while ensuring consistent access across multiple processing engines. It is needed to address challenges such as data silos, proprietary lock-in, lack of transactional consistency, and the inability to safely share data across tools and platforms. Open table formats like Apache Iceberg provide database-like capabilities—ACID transactions, schema and partition evolution, and time travel—on low-cost object storage. Teradata supports open table formats by natively reading and analyzing OTF tables alongside traditional relational tables, enabling high-performance, in-database analytics on large volumes of data without data movement. This allows organizations to unify structured enterprise data with lake-based data under a single SQL, security, and governance framework, accelerating insights while preserving openness and flexibility.<br>    Connecting to open table format tables require additional authentication object and datalake definition. Please refer to Getting_Started_OTF for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f0ac9-6ef5-42e4-8957-1e50c94482dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_in_schema_tbl = in_schema(schema_name=\"demo_shipping\",\n",
    "                           table_name=\"customers\",\n",
    "                           datalake_name=\"iceberg_glue\")\n",
    "c_datalake_df = DataFrame(cust_in_schema_tbl)\n",
    "c_datalake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c288f-d56f-4f32-a5d1-c2b18fe01124",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_in_schema_tbl = in_schema(schema_name=\"demo_shipping\",\n",
    "                           table_name=\"products\",\n",
    "                           datalake_name=\"iceberg_glue\")\n",
    "p_datalake_df = DataFrame(prod_in_schema_tbl)\n",
    "p_datalake_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f90ca-5e9e-4af5-ab2b-ae73ae1a563d",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'> <b> 3. Creating Shipment dataset </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'> As our goal is to build a binary classification machine learning model that can predict delays in logistics with high accuracy; We will combine all the tables available both in the Vantage tables and OTF tables to create a dataset which we will then use to create our prediction model.\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>General steps we will follow:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <li>Combine both tables in Vantage and OTF to create a dataset</li>    \n",
    "    <li>Feature engineering to create target variable as will as other columns</li>\n",
    "    <li>Creating Fit tables for Scaling, OneHotEncoding and LableEncoding</li>\n",
    "    <li>Applying ColumnTransformer to create final dataset, creating train and test datasets</li>\n",
    "    <li>Creating prediction model</li>\n",
    "    <li>Model evaluation and explainability</li>\n",
    "</ol>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a3d8a-298f-4acc-ada4-c08d8ad5d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship = DataFrame.from_query('''SELECT\n",
    "    o.*,\n",
    "    c.  customer_zip_code_prefix ,c.customer_city,c.customer_state,\n",
    "    i.product_id,i.seller_id ,i.price ,i.shipping_charges,\n",
    "    p.payment_sequential ,p.payment_type,p.payment_installments,p.payment_value,\n",
    "    pr.product_category_name,pr.product_weight_g,pr.product_length_cm,pr.product_height_cm,pr.product_width_cm \n",
    "FROM DEMO_Shipment.orders o\n",
    "LEFT JOIN iceberg_glue.demo_shipping.customers c\n",
    "    ON o.customer_id = c.customer_id\n",
    "LEFT JOIN DEMO_Shipment.orderitems i\n",
    "    ON o.order_id = i.order_id\n",
    "LEFT JOIN DEMO_Shipment.payments p\n",
    "    ON o.order_id = p.order_id\n",
    "LEFT JOIN iceberg_glue.demo_shipping.products pr\n",
    "    ON i.product_id = pr.product_id;''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b95749-e5de-461e-b8fd-385752c0c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020fa31-802b-4785-a465-31cc5b5263ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving intermediate table\n",
    "copy_to_sql(tdf_ship, table_name=\"shipment_dataset\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d0686-bfc6-4eb5-9667-95160defc8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship = DataFrame(\"shipment_dataset\")\n",
    "tdf_ship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924b73a-3401-49ea-b263-3b205367787a",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'> <b> 4. Feature Engineering </b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4af7e-1724-48b6-887c-1fd154f16321",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>As a first step let us check how many nulls we have in our data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533bde9-adc3-4b96-b1c8-ba7ce1cb4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking NULLs in data\n",
    "colsum = ColumnSummary(data=tdf_ship,\n",
    "                        target_columns=[':']\n",
    "                       )\n",
    "cs = colsum.result.filter(items = ['ColumnName', 'Datatype', 'NullCount','NullPercentage'])\n",
    "cs[cs['NullPercentage'] > 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deaaad0-35e1-44cc-8eb7-6791e020e582",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Dropping the rows with approved date as null.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb34a5-ed76-416a-8d8a-47fb9e60ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship = tdf_ship.dropna(how='any', subset=['order_approved_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5eb46d-7d2a-4f69-bbb3-f9a2d65d70c5",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'> <b> 4.1 Creating Target Variable </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'> \n",
    "We will create target column which is 'is the shipment delayed or not' as 'is_late_s' . To calculate this we will check all the records which have order_status as 'delivered' and where the order_delivered_timestamp > order_estimated_delivery_date.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909687af-e8f2-4346-8235-42c1978eb62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Late delivery is if the \n",
    "#order_delivered_timestamp feature is greater than the order_estimated_delivery_date feature for delivered orders\n",
    "tdf_ship = tdf_ship.assign(is_late_s = case([(tdf_ship['order_status'] == 'delivered' and (tdf_ship.order_delivered_timestamp > tdf_ship.order_estimated_delivery_date),\n",
    "                          1)], else_= 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee107d9-c2c3-417f-88f0-6298dd6e04fe",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'> <b> 4.2 Adding calculated columns </b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b> Product Volume </b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Added the prod_vol feature which is the multiplication of product_length_cm, product_height_cm, and product_width_cm.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d185f4-693c-42d6-a89f-efbb561d8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship=tdf_ship.assign(prod_vol = tdf_ship.product_length_cm*tdf_ship.product_height_cm*tdf_ship.product_width_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e95f10-a95b-4236-843d-30c02822d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking percentile values of prod_vol\n",
    "d1=tdf_ship.assign(True, percentile_25=tdf_ship.prod_vol.percentile(0.25, interpolation=None),\n",
    "            percentile_75=tdf_ship.prod_vol.percentile(0.75, interpolation=None))\n",
    "p_25p=d1.get_values()[0][0].item()\n",
    "p_75p=d1.get_values()[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede6119-e939-470f-9a63-03d12680277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship = tdf_ship.assign(vol_category = case([(tdf_ship.prod_vol >= p_75p,'large'),\n",
    "                                              ((tdf_ship.prod_vol >= p_25p),'medium'),\n",
    "                                              (tdf_ship.prod_vol < p_25p ,'small')\n",
    "                                             ], else_= 'small'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09743f-31f4-441d-bedf-779d36780759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cbdf56-1cb7-4b8e-95d8-3ae607f3c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f1aaf3-ff1c-41df-9e04-d859800ed367",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b> Day Of Week </b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Day Of Week is a representation of the days of the week in number 0-6. We will use day_of_week() function available in teradataml library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17923367-808f-4250-99ae-62974c41bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship=tdf_ship.assign(purchase_day_of_week = tdf_ship.order_purchase_timestamp.day_of_week(),\n",
    "                      approved_day_of_week = tdf_ship.order_approved_at.day_of_week())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94487ab6-d812-4540-be18-9a8be783b024",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b> Purchase Hour and Approved Hour</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will extarct hour from the purchase_hour and approved_hour. We will use extract() function available in teradataml library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdaf836-e025-468b-b1c3-466655dbebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship=tdf_ship.assign(purchase_hour = tdf_ship.order_purchase_timestamp.extract('HOUR'),\n",
    "                      approved_hour = tdf_ship.order_approved_at.extract('HOUR'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7a37a1-2e2d-4a8c-ac31-92e3fcdc4e57",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b> Price Category</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will categorise the price in 3bins based on the 75th pecentile and 25th percentile of the price.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cf57e-9e36-4d47-a012-280e01faae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking percentile values of price\n",
    "d2=tdf_ship.assign(True, percentile_25=tdf_ship.price.percentile(0.25, interpolation=None),\n",
    "            percentile_75=tdf_ship.price.percentile(0.75, interpolation=None))\n",
    "\n",
    "ship_25p=d2.get_values()[0][0].item()\n",
    "ship_75p=d2.get_values()[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76397f1b-9cec-42bc-898f-71a219051977",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship = tdf_ship.assign(price_category = case([(tdf_ship.price >= ship_75p,'expensive'),\n",
    "                                              (tdf_ship.price >= ship_25p,'affordable'),\n",
    "                                              (tdf_ship.price < ship_25p,'budget')\n",
    "                                             ], else_= 'budget'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b6a579-b705-442c-b026-fc1e005ff011",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b> Weight Category</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will categorise the weight in 3bins based on the 75th pecentile and 25th percentile of the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc0afca-d170-4380-b167-ce0202182135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking percentile values of price\n",
    "d3=tdf_ship.assign(True, percentile_25=tdf_ship.product_weight_g.percentile(0.25, interpolation=None),\n",
    "            percentile_75=tdf_ship.product_weight_g.percentile(0.75, interpolation=None))\n",
    "\n",
    "wt_25p=d3.get_values()[0][0].item()\n",
    "wt_75p=d3.get_values()[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e9eca-2d53-41fe-8c93-0c3a9bd20d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship = tdf_ship.assign(weigth_category = case([(tdf_ship.product_weight_g >= wt_75p,'heavy'),\n",
    "                                              (tdf_ship.product_weight_g >= wt_25p,'medium'),\n",
    "                                              (tdf_ship.product_weight_g < wt_25p,'light')\n",
    "                                             ], else_= 'light'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa702bf4-760f-4e7f-ad8b-e85a458e966e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b> Shipping Charges Category</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will categorise shipping charges in 3bins based on the 75th pecentile and 25th percentile of the shipping charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace990f-85ab-4c36-9bd6-646a819f9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking percentile values of price\n",
    "d4=tdf_ship.assign(True, percentile_25=tdf_ship.shipping_charges.percentile(0.25, interpolation=None),\n",
    "            percentile_75=tdf_ship.shipping_charges.percentile(0.75, interpolation=None))\n",
    "\n",
    "s_25p=d4.get_values()[0][0].item()\n",
    "s_75p=d4.get_values()[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a88883-7a40-49f2-9ef2-ebdac35f04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship = tdf_ship.assign(freight_category = case([(tdf_ship.shipping_charges >= s_75p,'expensive'),\n",
    "                                              (tdf_ship.shipping_charges >= s_25p,'medium'),\n",
    "                                               (tdf_ship.shipping_charges < s_25p,'budget')   \n",
    "                                             ], else_= 'budget'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517bd8c7-5f4c-4c1e-899e-963f0823d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181477e9-0cc8-4eb8-bf0d-6a3668769810",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_ship.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dfb218-f5fd-49ea-a5a7-aba0f5d5888d",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b> Zip code counts binning Category</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will categorise the late delivery on the zip codes and categorize them in 3bins based on the 75th pecentile and 25th percentile of the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068a179-7d3a-45bf-a77e-ab594023d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking zip code counts \n",
    "zip = tdf_ship[tdf_ship['is_late_s'] == 1].select(['customer_zip_code_prefix','order_id']) \\\n",
    "                                           .groupby(['customer_zip_code_prefix']).agg({'order_id' : ['count']}) \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c15c5e-0daf-4776-90ff-289fb8bbb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking percentile of zip code counts \n",
    "d_z=zip.assign(True, percentile_25=zip.count_order_id.percentile(0.25, interpolation=None),\n",
    "            percentile_75=zip.count_order_id.percentile(0.75, interpolation=None))\n",
    "\n",
    "#75th & 25th percentile values\n",
    "zip_25p=d_z.get_values()[0][0].item()\n",
    "zip_75p=d_z.get_values()[0][1].item()\n",
    "\n",
    "#binning the zip code in one of the category\n",
    "zip = zip.assign(zip_category = case([(zip.count_order_id >= zip_75p,'often'),\n",
    "                                       (zip.count_order_id >= zip_25p,'quite'),\n",
    "                                        (zip.count_order_id < zip_25p,'rarely')\n",
    "                                             ], else_= 'rarely'))\n",
    "\n",
    "zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df89046-ebb1-4833-8c74-3ae4610b3f20",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b> Customer City counts binning Category</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will categorise the late delivery on the customer cities and categorize them in 3bins based on the 75th pecentile and 25th percentile of the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b664de-6c49-45e4-9813-97d0315ca861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking city counts \n",
    "city = tdf_ship[tdf_ship['is_late_s'] == 1].select(['customer_city','order_id']).groupby(['customer_city']). \\\n",
    "       agg({'order_id' : ['count']})\n",
    "\n",
    "#taking percentile of city counts\n",
    "d_c=city.assign(True, percentile_25=city.count_order_id.percentile(0.25, interpolation=None),\n",
    "            percentile_75=city.count_order_id.percentile(0.75, interpolation=None))\n",
    "\n",
    "#75th & 25th percentile values\n",
    "city_25p=d_c.get_values()[0][0].item()\n",
    "city_75p=d_c.get_values()[0][1].item()\n",
    "\n",
    "#binning the city in one of the category\n",
    "city = city.assign(city_category = case([(city.count_order_id >= city_75p,'often'),\n",
    "                                       (city.count_order_id >= city_25p,'quite')\n",
    "                                             ], else_= 'rarely'))\n",
    "\n",
    "city"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c2ecf-98ff-480c-956b-8f09187c2168",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b> Seller counts binning Category</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will categorise the late delivery by sellers and categorize them in 3bins based on the 75th pecentile and 25th percentile of the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcc541-cf6b-4776-b658-badd199f52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking seller counts \n",
    "seller = tdf_ship[tdf_ship['is_late_s'] == 1].select(['seller_id','order_id']).groupby(['seller_id']). \\\n",
    "       agg({'order_id' : ['count']})\n",
    "\n",
    "#taking percentile of seller counts\n",
    "d_s=seller.assign(True, percentile_25=seller.count_order_id.percentile(0.25, interpolation=None),\n",
    "            percentile_75=seller.count_order_id.percentile(0.75, interpolation=None))\n",
    "\n",
    "#75th & 25th percentile values\n",
    "seller_25p=d_s.get_values()[0][0].item()\n",
    "seller_75p=d_s.get_values()[0][1].item()\n",
    "\n",
    "#binning the seller in one of the category\n",
    "seller = seller.assign(seller_category = case([(seller.count_order_id >= seller_75p,'often'),\n",
    "                                       (seller.count_order_id >= seller_25p,'quite'),\n",
    "                                        (seller.count_order_id < seller_25p,'rarely')       \n",
    "                                             ], else_= 'rarely'))\n",
    "\n",
    "seller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90395e92-0e55-4780-abf4-acbf52c47a87",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b> Product counts binning Category</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will categorise the late delivery of products and categorize them in 3bins based on the 75th pecentile and 25th percentile of the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef22d2-1f8a-45b0-8b43-8bdb9abb93ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#taking product counts \n",
    "product = tdf_ship[tdf_ship['is_late_s'] == 1].select(['product_id','order_id']).groupby(['product_id']). \\\n",
    "       agg({'order_id' : ['count']})\n",
    "\n",
    "#taking percentile of product counts\n",
    "d_p=product.assign(True, percentile_25=product.count_order_id.percentile(0.25, interpolation=None),\n",
    "            percentile_75=product.count_order_id.percentile(0.75, interpolation=None))\n",
    "\n",
    "#75th & 25th percentile values\n",
    "product_25p=d_p.get_values()[0][0].item()\n",
    "product_75p=d_p.get_values()[0][1].item()\n",
    "\n",
    "#binning the product in one of the category\n",
    "product = product.assign(seller_category = case([(product.count_order_id >= seller_75p,'often'),\n",
    "                                       (product.count_order_id >= seller_25p,'quite')\n",
    "                                             ], else_= 'rarely'))\n",
    "\n",
    "product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c66b7-5617-416c-ab99-d36e8248b184",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Joining all of these (zip,city, seller, product) with shipment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd1b07-a0a1-4984-9323-40820efb7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = tdf_ship.join(other = zip,on = \"customer_zip_code_prefix\", how = \"left\", rprefix =\"zip\")\\\n",
    "               .join(other = city,on = \"customer_city\", how = \"left\", rprefix =\"city\")\\\n",
    "               .join(other = seller,on = \"seller_id\", how = \"left\", rprefix =\"seller\")\\\n",
    "               .join(other = product,on = \"product_id\", how = \"left\", rprefix =\"product\")\n",
    "tdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3a14e-58fd-4d7c-8e41-ea88d3ad627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf= tdf.assign(drop_columns = True\n",
    "                ,order_id=tdf.order_id\n",
    "                ,customer_state=tdf.customer_state.cast(type_=VARCHAR(20))\n",
    "                ,price=tdf.price\n",
    "                ,shipping_charges=tdf.shipping_charges\n",
    "                ,payment_seq=tdf.payment_sequential\n",
    "                ,payment_type=tdf.payment_type\n",
    "                ,payment_installments=tdf.payment_installments\n",
    "                ,payment=tdf.payment_value\n",
    "                ,product_type=tdf.product_category_name\n",
    "                ,product_wt=tdf.product_weight_g\n",
    "                ,is_late_s=tdf.is_late_s\n",
    "                ,prod_vol=tdf.prod_vol\n",
    "                ,vol_cat=tdf.vol_category.cast(type_=VARCHAR(20))\n",
    "                ,approved_day_of_week=tdf.approved_day_of_week\n",
    "                ,purchase_day_of_week=tdf.purchase_day_of_week\n",
    "                ,approved_hour=tdf.approved_hour\n",
    "                ,purchase_hour=tdf.purchase_hour\n",
    "                ,price_cat=tdf.price_category.cast(type_=VARCHAR(20))\n",
    "                ,weigth_cat=tdf.weigth_category.cast(type_=VARCHAR(20))\n",
    "                ,freight_cat=tdf.freight_category.cast(type_=VARCHAR(20))\n",
    "                ,zip_cat=tdf.zip_category.cast(type_=VARCHAR(20))\n",
    "                ,city_cat=tdf.city_category.cast(type_=VARCHAR(20))\n",
    "                ,seller_cat=tdf.seller_category.cast(type_=VARCHAR(20))\n",
    "                ,product_cat=tdf.product_seller_category.cast(type_=VARCHAR(20))\n",
    "               )\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf23e5-6bea-49ae-9404-88450d6b9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving intermediate table\n",
    "copy_to_sql(tdf, table_name=\"data_preprocess\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00a143-7ad1-4c48-8002-3ff6f51a93e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_pre = DataFrame(\"data_preprocess\")\n",
    "tdf_pre.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed088923-891b-486e-a9d7-20cd80d1d737",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'> <b> 4.3 Encoding of categorical columns </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>First we get the list of columns which are categories and calculate the disticnt counts in each of them.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b7aa6-f9cf-407a-913c-82d58d5683cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of column names with data type 'str'\n",
    "ohe_col_list = [col.split()[0] for col in str(tdf_pre.dtypes).split('\\n') if col.split()[1] == 'str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f03175-772d-4891-aa6c-20f06ab88ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts of all str columns\n",
    "tdf_pre.select(ohe_col_list).agg('unique')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f55c46-058b-4ea6-9b1c-f4500c5e1125",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b> One-Hot Encoding</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'> Due to number of distinct values in customer_state(27) and product_type(70); we will do label encoding for these, for others we will do one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1d1b5-e24a-4e41-93c3-0fdb04467c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_col_list.remove(\"order_id\")\n",
    "ohe_col_list.remove(\"customer_state\")\n",
    "ohe_col_list.remove(\"product_type\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a15c0a-0c70-4ab8-9ad0-3b8eb612e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e86c8-efea-4ba3-b45d-f66264eda849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding and label encoding\n",
    "# create fit object to encode categorical columns\n",
    "hot_fit = OneHotEncodingFit(data=tdf_pre,\n",
    "                                is_input_dense=True,\n",
    "                                target_column=ohe_col_list,\n",
    "                                category_counts=[4,2,3,3,2,2,3,3,2],\n",
    "                                approach=\"auto\",\n",
    "                                other_column=\"other\")\n",
    " \n",
    "# Print the result DataFrame.\n",
    "hot_fit.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4174aa1-070e-4f8b-ae3c-33a505ec25f1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b> Label Encoding</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23eff1-d47f-43b0-92fb-d437cc7fa3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_fit = OrdinalEncodingFit(target_column=['customer_state','product_type'],\n",
    "                                 data=tdf_pre,\n",
    "                                 default_value=-1\n",
    "                                )\n",
    "\n",
    "ordinal_fit.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f1fe5-028b-401a-9564-74ebde28d5db",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'> <b> 4.4 Scaling of numerical columns </b></p>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d6715-22d3-4508-8ee6-6de426b6ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_list = [col.split()[0] for col in str(tdf_pre.dtypes).split('\\n') if col.split()[1] not in ('str','int')]\n",
    "scale_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1246b8ad-6c19-4678-abb6-ae175926cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_fit = ScaleFit(data=tdf_pre,\n",
    "                       target_columns=scale_list,\n",
    "                       scale_method=\"RANGE\",\n",
    "                       miss_value=\"KEEP\",\n",
    "                       global_scale=False)\n",
    "scale_fit.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951cd249-09e3-4fdc-b76e-9901bb70c11e",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'> <b> 4.5 Applying all the fit tables</b> \n",
    "    <p style = 'font-size:16px;font-family:Arial'> We will apply all the fit tables created in above steps using the ColumnTransformer function and create the final dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330889a5-e039-49f8-9d96-af7e4b16bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = ColumnTransformer(input_data=tdf_pre,\n",
    "                         scale_fit_data=scale_fit.output,\n",
    "                         onehotencoding_fit_data=hot_fit.result,\n",
    "                         ordinalencoding_fit_data=ordinal_fit.result\n",
    "                                        )\n",
    "tdf = out1.result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23383125-0493-4788-86d0-40889c966235",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100db456-a89c-40f5-ad8a-2df99e6f59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_col=tdf.columns\n",
    "sel_col = list(set(all_col) - set(ohe_col_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09c278-7fe6-4c6e-988e-20a09cfd76dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_final = tdf.select(sel_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208bad94-3562-482f-b790-17c13a9dfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af255b70-453a-4799-8ca9-e49584c64950",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eee9a1-e0f6-4edd-85dd-9f0a1891507a",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'> <b> 5. Create train and test data  </b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1981052-1c31-4814-ab0c-599e261acbb9",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Now we have transformed our data and it is fit to be used in machine learning models, let us split the whole dataset into train and test sets for model training and scoring. We will use <b>TrainTestSplit</b> function for this task.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5393e1f9-a8c9-4151-9009-11232a0f5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTestSplit_out = TrainTestSplit(\n",
    "                                    data = tdf_final,\n",
    "                                    id_column = \"order_id\",\n",
    "                                    stratify_column = \"is_late_s\",\n",
    "                                    train_size = 0.75,\n",
    "                                    test_size = 0.25,\n",
    "                                    seed = 21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297797d-42b2-4e6a-a330-411ba67b03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 2 virtual dataframes\n",
    "df_train = TrainTestSplit_out.result[TrainTestSplit_out.result['TD_IsTrainRow'] == 1].drop(['TD_IsTrainRow'], axis = 1)\n",
    "df_test = TrainTestSplit_out.result[TrainTestSplit_out.result['TD_IsTrainRow'] == 0].drop(['TD_IsTrainRow'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd18c4f-c336-4547-a21c-562c403a58ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(df_train, table_name=\"train_shipment_ds\", if_exists=\"replace\",primary_index =\"order_id\")\n",
    "copy_to_sql(df_test, table_name=\"test_shipment_ds\", if_exists=\"replace\",primary_index =\"order_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d689517-5168-4c52-8220-4d05df8b9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=DataFrame(\"train_shipment_ds\")\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1de1e4-ccfb-4928-9233-f36b3404537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363fd95-1a46-472c-b6e4-b597350d5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=DataFrame(\"test_shipment_ds\")\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1402fb-23df-485b-a5a5-1b58f0ac9687",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>6. InDb Model Training and Scoring</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>For our model we will use logistic regression.<br>\n",
    "  <b>Logistic regression</b> is a statistical algorithm used for binary classification problems. It is a type of supervised learning algorithm that predicts the probability of an input belonging to a certain class (e.g., positive or negative) based on its features.<br>Logistic regression works by modeling the relationship between the input features and the probability of belonging to a certain class using a logistic function. The logistic function takes the input feature values and maps them onto a probability scale between 0 and 1, which represents the probability of belonging to the positive class.<br>\n",
    "    The <b>GLM </b>function is a generalized linear model (GLM) that performs regression and classification analysis on data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b52fbb-da88-4d61-85af-9e8e2c5dddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list=train_data.columns\n",
    "col_list.remove(\"order_id\")\n",
    "col_list.remove(\"is_late_s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b7c5ba-9469-4a89-977f-a74e84fd5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_model = GLM(data = train_data,\n",
    "                input_columns = col_list, \n",
    "                response_column = 'is_late_s',\n",
    "                family = 'Binomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f3ff0-fcc7-4c28-81f4-71e79d46cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_model.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed54b9c2-f872-4929-92be-dc493633382b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We have created our model, let's do the predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d76216d-dbcb-4257-960f-cdeaf8a95d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_prediction = TDGLMPredict(newdata = test_data,\n",
    "                           id_column = 'order_id',\n",
    "                           object = glm_model.result,\n",
    "                           accumulate = 'is_late_s',\n",
    "                           family = 'Binomial',   \n",
    "                           output_prob=True,\n",
    "                           output_responses = ['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb6ebd-fb3a-474f-8922-74e027f7b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_glm = glm_prediction.result.assign(prediction = glm_prediction.result.prediction.cast(type_ = BYTEINT))\n",
    "out_glm = out_glm.assign(prediction = out_glm.prediction.cast(type_ = VARCHAR(2)))\n",
    "out_glm = out_glm.assign(is_late_s = out_glm.is_late_s.cast(type_ = VARCHAR(2)))\n",
    "out_glm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5cff8-894b-4fba-a645-122d99617da5",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The output above shows prob_1, i.e. shipment will be delayed and prob_0, i.e. shipment will not be delayed. The prediction column uses these probabilities to give a class label, i.e. prediction column.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92aa56-548b-4d6a-ae94-51e84f26e9b7",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>6.2 Evaluation of Logistic Regression Model</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will use the <b>ClassificationEvaluator</b> function to evaluate the trained glm model on test data. This will let us know how well our model has performed on unseen data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f77524-8fdc-4503-b69f-01114845e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassificationEvaluator_glm = ClassificationEvaluator(\n",
    "                                                        data = out_glm,\n",
    "                                                        observation_column = 'is_late_s',\n",
    "                                                        prediction_column = 'prediction',\n",
    "                                                        labels = ['0', '1']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c9af6-b899-4bc1-8ef0-2065c7096aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ClassificationEvaluator_glm.output_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a141b-2862-4972-881d-f0de88b31a63",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>6.3 Model Explainability</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>SHAP computes the contribution of each feature in a prediction as as average marginal contribution of the feature value across all possible coalitions. TD_SHAP also computes mean absolute contribution of each feature as global explanation (using OUT clause) which can be used as a measure of feature importance.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52d7f5-9c0f-4e4a-8907-5c9dea11a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Shap_out = Shap(data=train_data, \n",
    "                    object=glm_model.result, \n",
    "                    id_column='order_id',\n",
    "                    training_function=\"TD_GLM\", \n",
    "                    model_type=\"Classification\",\n",
    "                    input_columns=col_list)\n",
    "#Print the result DataFrame.\n",
    "Shap_out.output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1459da6-41b0-42e5-bcc0-bd9cdc20713c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d46112-ecc7-4091-a8c9-b1d88a7bd2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Shap_out.output_data.to_pandas()\n",
    "df_t = df.T\n",
    "df_t=df_t.rename(columns={'index': 'Feature', 0: 'Importance'})\n",
    "df_sort=df_t.sort_values(by='Importance', ascending=False)\n",
    "df_sort.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e39ac2-30a3-4a84-b666-616a233835e7",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>7. Cleanup</b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "We need to clean up our work tables to prevent errors next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921638e2-2e89-4190-ab7d-e47733c73ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['shipment_dataset' ,'data_preprocess' ,'train_shipment_ds' ,'test_shipment_ds']\n",
    "\n",
    "# Loop through the list of tables and execute the drop table command for each table\n",
    "for table in tables:\n",
    "    try:\n",
    "        db_drop_table(table_name = table)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6241c-a0c9-4023-9901-728d6c837466",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf0e545-b1fe-41ad-b822-276db2117910",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'> <b> 8. Conclusion </b> </p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this notebook, we explored how open table formats enable scalable, flexible analytics on large volumes of data stored in modern data lake architectures. We demonstrated how data stored in open formats can be seamlessly integrated with enterprise analytics workflows, eliminating data silos and reducing the need for complex data movement. By leveraging Teradata’s ability to perform high-performance, in-database analytics and natively read open table format tables, organizations can analyze structured and lake-based data together using a single SQL and governance framework. This approach provides a strong foundation for building scalable analytics, advanced AI use cases, and data-driven decision-making, while maintaining openness, performance, and enterprise-grade reliability.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75485b-b4e3-4415-ae26-554149520897",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid #91A0Ab\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2026. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
