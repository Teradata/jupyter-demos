{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579a1566-b499-4a28-b093-c5e20c4926aa",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Combining Structured and Unstructured Data for Predictive Modeling – a recipe\n",
    " <br>       \n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 150px; height: auto; margin-top: 20pt;\">\n",
    "  <br>\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7929266f-507f-4f93-86ca-14e03df3813a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Introduction:</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "    In real-world machine learning, the most useful signals don’t always live in neat, structured tables. Sure, you’ve got sensor readings and transaction logs—but a ton of valuable context often hides in messier stuff like operator notes, support tickets, or log files. The trick is: once you combine those two worlds—structured and unstructured—you can build smarter, more insightful models.\n",
    "<br><br>\n",
    "That’s exactly what this blog post is about: a hands-on recipe for predicting equipment outages by blending sensor data with free-text operator logs. We’ll show how to turn unstructured text into meaningful features using embeddings and clustering, then plug those into a supervised model. And the best part? You can run the whole pipeline—embedding, scoring, and all—right inside Teradata Vantage.\n",
    "<br><br>\n",
    "While we focus on predictive maintenance here, this pattern is super flexible. You can use it anywhere unstructured text adds value. Think customer support tickets in banking, field reports in telecom, or even patient notes in healthcare. Here's what that might look like:\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9722ba12-7860-4396-9c69-3843ef6f1f89",
   "metadata": {},
   "source": [
    "| Industry           | Structured Data                 | Unstructured Data        | Use Case                           |\n",
    "| ------------------ | ------------------------------- | ------------------------ | ---------------------------------- |\n",
    "| Manufacturing      | Sensor data, production cycles  | Operator logs            | Predictive maintenance             |\n",
    "| Banking            | Transaction logs                | Customer support tickets | Fraud detection                    |\n",
    "| Retail             | Sales metrics, inventory levels | Customer reviews         | Demand forecasting                 |\n",
    "| Healthcare         | Patient vitals, lab results     | Doctor’s notes           | Early diagnosis / readmission risk |\n",
    "| Telecommunications | Network metrics                 | Technician reports       | Network failure prediction         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a54b33-17d7-4cd1-8bc2-d92f3e71e4ca",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In this walkthrough, we’re taking historical machine data—both structured (like sensor readings) and the unstructured (like operator-written log entries)-and using it to predict whether a machine will fail in the next 24 hours.\n",
    "<br>\n",
    "    Here’s what we will do:\n",
    "</p>\n",
    "\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>First, we turn the operator logs into embeddings—dense vector representations of the text</li>\n",
    "    <li>Then we use KMeans to find common patterns (or “topic prototypes”) in those embeddings</li>\n",
    "    <li>We turn those topics into features by measuring how close each log is to each cluster center</li>\n",
    "    <li>Next, we combine those new features with the classic sensor data and train a model to predict outages</li>\n",
    "    <li>Finally, we put it all together into a full inference pipeline that runs end-to-end inside the database—no data movement, no glue code</li>\n",
    "</ul>\n",
    "<p style = 'font-size:16px;font-family:Arial'>It all comes together in a clean, scalable setup. Here’s a sketch of what that looks like (Note that we already assume that the features from the structured data have been calculated, as well as the outage events have been defined based on a certain SLA/KPI):</p>\n",
    "\n",
    "<center><img src=\"images/flowchart_embeddingsfeatures.png\" alt=\"workflow_topictrend\" style=\"background-color: #f0f0f0; border: 4px solid #404040; border-radius: 10px;\">\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This method provides an efficient way to not only categorize messages by topic but also track how these topics evolve over time, offering actionable insights into changing customer concerns, emerging issues, and overall trends.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db2784-5e71-4520-83df-625baecbf429",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial;'><b>Dataset Overview</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "To show how you can combine structured and unstructured data for predictive maintenance, we’re using a synthetic dataset. Real-world data is often locked away due to privacy or operational reasons, so we built a dataset that mimics the messiness and patterns you'd typically run into in industrial settings.\n",
    "<br><br>\n",
    "We’ve got 10,000 rows, split into 8,000 for training and 2,000 for testing.\n",
    "</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Each row has:</p>\n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "  <li><b>15 numeric features</b> —your usual suspects from sensors: temp, pressure, RPM, vibration, etc.</li>\n",
    "  <li><b>One text column</b>, maintenance_log_aug, where operators log what they saw or did, in free-form comments.</li>\n",
    "</ul>\n",
    "\n",
    "<ul style = 'font-size:16px;font-family:Arial;'>As you might expect, the log entries are all over the place:\n",
    "<li>Weird formatting</li>\n",
    "<li>Typos like <code>successefully</code> or <code>prblms</code></li>\n",
    "<li>Some super detailed, others barely say anything</li>\n",
    "<li>Inconsistent language and terminology</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>That makes traditional NLP techniques like bag-of-words kind of useless. So instead, we turn those logs into dense vector embeddings using a pre-trained Hugging Face model (exported to ONNX for in-database use). These embeddings are much better at capturing the real meaning of the text—even if it’s messy.\n",
    "<br><br>\n",
    "Next, we’ll show how to turn those embeddings into clean, interpretable features and plug them into a supervised model alongside the sensor data.</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9550e8-557d-4efb-b6e4-032f0bf3f3d5",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>1. Connect to Vantage, Import python packages and explore the dataset</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6334d5-89a0-4989-968c-41acf5038a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc3405-c79d-4bd9-8913-7ce41f41e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install teradataml --upgrade --quiet\n",
    "!pip install teradatasqlalchemy --upgrade --quiet\n",
    "!pip install teradataml-plus --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac5ff0-a422-4b54-88a9-719d5022d25c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial;'><b>Note: </b><i>The above libraries have to be installed. Restart the kernel after executing these cells to bring the installed libraries into memory. The simplest way to restart the Kernel is by typing <b> 0 0</b></i> (zero zero) and pressing <i>Enter</i>.</p>\n",
    "</div>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9da9ae-c4aa-41af-9add-c2aa4518ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import tdmlplus\n",
    "import teradataml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc, classification_report\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from teradataml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c10306-04b9-425e-ade1-c74fb300bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/tab_widget.py # imports a function `display_dataframes_in_tabs`\n",
    "list_relevant_tables = [] # we will be adding names of relevant tables progressivley into this list to display "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf2a87d-4eb0-4f61-9b7e-cecf70168f28",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:18px;font-family:Arial;'> 1.1 Connect to Vantage</b>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30236b-f5b8-4498-a71b-939ced858640",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i /home/jovyan/JupyterLabRoot/UseCases/startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975b67a-76e7-4932-a0cd-6b2b5b1061d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql('''SET query_band='DEMO=Combine_structured_unstructured_predictive_modelling;' UPDATE FOR SESSION; ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbbfd71-bacb-4bfc-a7ba-5c27a729c7e3",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;'><b>Getting Data for This Demo</b></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a109c9-9430-491b-9bd7-e56982df313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/_dataload.ipynb # takes about 20 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3c91a-69b6-4438-bbc3-05ccd33e262e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>In addition, we want to check if our database has already got the required functionality to generate embeddings.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e02215-64e0-4fe6-a157-f87d59f347a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VCL = False # current system is VCE/VCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28605d5e-cb94-4487-bb0e-f7e0577a74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VCL:\n",
    "    results = execute_sql(\"help database mldb\").fetchall()\n",
    "else:\n",
    "    results = execute_sql(\"help user mldb\").fetchall()\n",
    "\n",
    "embeddings_functions = [x[0] for x in results if x[0].startswith(\"ONNXEmbeddings\")]\n",
    "if len(embeddings_functions) >0:#\n",
    "    print(\"\\n\".join(embeddings_functions))\n",
    "    print(\"---------------------\\nONNXEmbeddings is installed\")\n",
    "else:\n",
    "    print(\"ONNXEmbeddings is not installed. Please Upgrade to BYOM version 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5fd943-bf04-4e0c-ab40-f6956d1ebad2",
   "metadata": {},
   "source": [
    "# Inspect the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9fcc99-5657-4506-9c19-0cf4cbf3c8b2",
   "metadata": {},
   "source": [
    "Before jumping into modeling, we take a quick look at the input data using a handy little utility called `display_dataframes_in_tabs()`. It shows our key tables as interactive tabs right in the Jupyter notebook—super useful for sanity checks.\n",
    "\n",
    "We’ve got two main datasets: **pump\\_failure\\_train** and **pump\\_failure\\_test**. Both have the same structure:\n",
    "\n",
    "* `row_id`: A unique ID for each row\n",
    "* `outage_next_24h`: Our target—1 means there was an outage in the next 24 hours, 0 means everything was fine\n",
    "* 15 numeric features from sensors (things like temperature, pressure, etc.)\n",
    "* `maintenance_log_aug`: A free-text field where operators wrote down what they saw or did\n",
    "\n",
    "Pretty straightforward. This is the raw material we’ll be working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125aa131-5962-4582-a5de-b83cee1e08f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_relevant_tables+=[\"pump_failure_train\", \"pump_failure_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b5505-c9a9-475a-9c03-03e7bae719f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframes_in_tabs(list_relevant_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bedb777-3251-4e6d-9e1b-69e21e78b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_train = DataFrame(in_schema(username,\"pump_failure_train\"))\n",
    "DF_test = DataFrame(in_schema(username,\"pump_failure_test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9f924-5c83-4295-b0a8-ba6b6541e70c",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>2. Load HuggingFace Model</b>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>To generate embeddings, we need an ONNX model capable of transforming text into vector representations. We use a pretrained model from\n",
    "<a href=\"https://huggingface.co/Teradata/gte-base-en-v1.5\" target=\"_blank\">Teradata's Hugging Face repository</a>   , such as gte-base-en-v1.5. The model and its tokenizer are downloaded and stored in Vantage tables as BLOBs using the save_byom function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028892c-ac8c-4382-b85f-86d8c98155ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bge-small-en-v1.5\"\n",
    "number_dimensions_output = 384\n",
    "model_file_name = \"model.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5431afe-e4ff-42e2-9f6d-1316a74df95a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Download Model from Teradata HuggingFace Page\n",
    "hf_hub_download(repo_id=f\"Teradata/{model_name}\", filename=f\"tokenizer.json\", local_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef8bb25-23cc-47bd-b614-a9a4d56afe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the command line syntax as it is more reliable then the python function\n",
    "!hf download Teradata/{model_name} onnx/{model_file_name} --local-dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b10e2e-7b49-4455-88cb-1e02a66b8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db_drop_table(\"embeddings_models\")\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    db_drop_table(\"embeddings_tokenizers\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef6d63-2bf0-4ac6-ab80-f47b5d8d62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Models into Vantage\n",
    "# a) Embedding model\n",
    "save_byom(model_id = model_name, # must be unique in the models table\n",
    "               model_file = f\"onnx/{model_file_name}\",\n",
    "               table_name = 'embeddings_models' )\n",
    "# b) Tokenizer\n",
    "save_byom(model_id = model_name, # must be unique in the models table\n",
    "              model_file = 'tokenizer.json',\n",
    "              table_name = 'embeddings_tokenizers') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28696ddb-4f4e-4bde-bf8e-b984343fb895",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframes_in_tabs([\"embeddings_models\",\"embeddings_tokenizers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924ef09-f2e8-4e0a-a782-60fc297220b5",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial;'>3. Create the Embeddings</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12588bd6-b543-4052-9535-79fd86df64dd",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:18px;font-family:Arial;'>3.1 Generate Embeddings with ONNXEmbeddings</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660dcc00-1cdd-4038-b8d5-7857fc16674c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "Now it's time to generate the embeddings using <b>ONNXEmbeddings</b>.<br>We run the ONNXEmbeddings function to generate embeddings for a small subset of records. The model is <b>loaded into the cache memory on each node</b>, and Teradata's <b>Massively Parallel Processing (MPP)</b> architecture ensures that embeddings are computed in parallel using <b>ONNX Runtime</b> on each node.  <br>Having said that, generating embeddings for the entire training set can be time-consuming, especially when working on a system with limited resources. In the <b>ClearScape Analytics experience</b>, only a <b>4 AMP system</b> with constrained RAM and CPU power is available. To ensure smooth execution, we test embedding generation on a small sample and use <b>pre-calculated embeddings</b> for the remainder of demo. In a real-life scenario you would tyipically encounter multiple hundred AMPs with much more compute power!<br>Also have a look at the most important input parameters of this <b>ONNXEmbeddings</b> function.\n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "<li><b>InputTable</b>: The source table containing the text to be embedded. </li>\n",
    "<li><b>ModelTable</b>: The table storing the ONNX model.                    </li>\n",
    "<li><b>TokenizerTable</b>: The table storing the tokenizer JSON file.       </li>\n",
    "<li><b>Accumulate</b>: Specifies additional columns to retain in the output </li>  \n",
    "<li><b>OutputFormat</b>: Specifies the data format of the output embeddings (<b>FLOAT32(768)</b>, matching the model's output dimension).</li>\n",
    "</ul>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "Since embedding generation is computationally expensive, we only process <b>10 records for testing</b> and rely on precomputed embeddings for further analysis.  \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06243f67-6315-4f85-aea4-88222e782948",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure.byom_install_location = \"mldb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d368623-fd0c-405a-b3f3-c2cf9bce8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_table = f\"SELECT TOP 10 t.row_id, t.maintenance_log_aug as txt FROM {username}.pump_failure_train t\" # we only create 10 embeddings to test the function\n",
    "DF_sample10 = DataFrame.from_query(input_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffefbe-e67f-4c44-9715-3599d334452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = DataFrame.from_query(f\"select * from {username}.embeddings_models where model_id = '{model_name}'\")\n",
    "my_tokenizer = DataFrame.from_query(f\"select model as tokenizer from {username}.embeddings_tokenizers where model_id = '{model_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8857de3-b474-4c35-a5a4-5aa29d05d7f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF_embeddings_sample = ONNXEmbeddings(\n",
    "    newdata = DF_sample10,\n",
    "    modeldata = my_model, \n",
    "    tokenizerdata = my_tokenizer, \n",
    "    accumulate = [\"row_id\", \"txt\"],\n",
    "    model_output_tensor = \"sentence_embedding\",\n",
    "    output_format = f'FLOAT32({number_dimensions_output})',\n",
    "    enable_memory_check = False\n",
    ").result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8fda35-da8f-4db6-8ee0-7f9c4a241066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DF_embeddings_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26afdff2-aae2-4b93-9377-ae68e65f9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_relevant_tables.append(\"operator_log_embeddings_train\")\n",
    "DF_train_embeddings = DataFrame(\"operator_log_embeddings_train\")\n",
    "DF_test_embeddings = DataFrame(\"operator_log_embeddings_test\")\n",
    "DF_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9734275e-0308-4eb2-b508-f5f294929ca5",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial;'>4. Prototype Distances as Features: Making Embeddings Work for ML</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18541a9e-9230-4d4d-a1f2-d7da13570d5f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "After converting the free-text maintenance logs into <b>384-dimensional embeddings</b> using a pre-trained transformer model, we’re left with a nontrivial question: \n",
    "<b>how do we combine these dense, abstract vectors with our 15 structured sensor features</b> in a way that’s efficient, interpretable, and still predictive?\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "You might expect us to simply append the embedding vectors to the sensor data and move on. \n",
    "And technically, that would work—many modern ML models can ingest hundreds of features without complaint. \n",
    "But the imbalance between 15 structured features and 384 text-derived ones poses practical issues. \n",
    "It increases training time, introduces the risk of overfitting, and—perhaps most critically—renders the model harder to interpret and trust. \n",
    "So, like many, we turned to <b>dimensionality reduction</b>.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "At this point, you're probably thinking of <b>PCA</b>, maybe <b>autoencoders</b>. \n",
    "These are the usual suspects for cutting down dimensionality. \n",
    "But here's where we do something unconventional: <b>we use KMeans clustering.</b>\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "At first glance, KMeans doesn’t sound like a dimensionality reduction method at all. \n",
    "Clustering is usually about grouping, not compressing. \n",
    "But in our case, we flip the usual logic. \n",
    "We don’t care about the cluster assignments. \n",
    "Instead, for each embedding vector, we compute its <b>distance to each cluster centroid</b>—turning a 384-dimensional embedding into a set of, say, 15 distances. \n",
    "These distance values then become our new features.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "<b>Why would we do that?</b><br>\n",
    "First, it gives us <b>explicit control over dimensionality</b>: the number of clusters we choose is the number of features we generate. \n",
    "Second, and more importantly, it gives us a way to <b>interpret</b> the resulting features. \n",
    "Each centroid represents a prototype region of the text space. \n",
    "By looking at the log entries closest to each cluster center, we can understand what kind of language or issue that cluster \"captures\"—whether it's communication errors, maintenance confirmations, or vague operator remarks.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "You might be surprised, but this isn’t as ad hoc as it sounds. \n",
    "There’s a line of work that treats centroids in vector space as <b>prototypes</b>, and uses distances to them as features—seen in areas like <b>Bag-of-Visual-Words</b> for image classification (explained in the box below, feel free to skip it).\n",
    "</p>\n",
    "\n",
    "<div style=\"border-left:5px solid ;padding:10px;margin:10px 0;\">\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "<b><i>Excursus: Prototype Distances in Bag-of-Visual-Words (BoVW)</i></b><br>\n",
    "While using distances to KMeans centroids as features might seem unconventional in the context of text embeddings, \n",
    "the same idea has long been a cornerstone of traditional computer vision pipelines—particularly in the <b>Bag-of-Visual-Words (BoVW)</b> model.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "Here’s how it works:\n",
    "</p>\n",
    "<ul style=\"font-size:16px;font-family:Arial;\">\n",
    "<li>In BoVW, an image is first decomposed into many local descriptors—for example, SIFT or ORB vectors extracted from key points in the image. \n",
    "Each descriptor is a high-dimensional vector representing a small patch of the image. \n",
    "These vectors are then clustered using KMeans, where the resulting centroids represent prototypical “visual words.” \n",
    "This creates a fixed “vocabulary” of visual patterns.</li>\n",
    "\n",
    "<li>To represent a new image, each of its local descriptors is assigned to the nearest centroid, \n",
    "and the image is then encoded as a histogram of how often each centroid appears. \n",
    "This histogram becomes the input to downstream classifiers like SVMs.</li>\n",
    "\n",
    "<li>In more advanced versions, instead of just counting how often a centroid is assigned, researchers use soft-assignment or distance-weighted histograms, \n",
    "where the feature reflects how close the image patches are to each centroid. \n",
    "In this sense, the image is embedded in a space defined by prototype distances.</li>\n",
    "\n",
    "<li>This technique made it possible to take a highly variable, unstructured input (an image), \n",
    "and project it into a compact, interpretable, and fixed-length feature vector—exactly what we aim to achieve with log text embeddings.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "BoVW was widely used in state-of-the-art vision models before deep learning took over, \n",
    "and it still appears in lightweight or embedded ML applications today. \n",
    "The conceptual takeaway is clear: representing new data points by their relationship to learned prototypes is a powerful and general method for feature engineering—even if the domain shifts from pixels to text.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85130fed-48ad-4428-aab0-8d8d31e59130",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "While <b>PCA</b> tends to collapse semantic structure into hard-to-interpret axes \n",
    "(and often assigns large variance to a single, uninformative direction), \n",
    "clustering preserves <b>topical or behavioral structure</b> in a way that's more meaningful \n",
    "and robust to the quirks of transformer embeddings.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "Of course, this method isn’t without trade-offs. \n",
    "Distance features tend to be <b>correlated</b>, especially when clusters overlap. \n",
    "That makes them less suitable for models that assume feature independence. \n",
    "And, like PCA, <b>KMeans is unsupervised</b>—it’s not optimizing for the predictive task directly. \n",
    "But for our needs, it's a strong compromise between <b>performance</b>, <b>interpretability</b>, \n",
    "and <b>deployment feasibility</b>.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "Crucially, this approach integrates seamlessly into our environment. \n",
    "<b>Teradata Vantage</b> offers a <b>highly optimized, in-database KMeans implementation</b>. \n",
    "That means we can cluster the embedding space once and then transform new log entries into feature vectors \n",
    "entirely within the database—no extra infrastructure, no model serving, no data movement.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "So yes, using <b>KMeans</b> for dimensionality reduction might seem unconventional. \n",
    "But as we show in the next section, it provides a surprisingly effective and practical foundation \n",
    "for combining <b>unstructured</b> and <b>structured features</b> in real-world ML pipelines.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "Below is the code for fitting and persisting a cluster model with <b>15 clusters</b>.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6286e-afd3-4cc5-9bf9-8ce8fc8a7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 15 # 15 features\n",
    "kmeans_out = KMeans(\n",
    "    id_column=\"row_id\",\n",
    "    data=DF_train_embeddings,\n",
    "    target_columns=f\"emb_0:emb_{number_dimensions_output-1}\",\n",
    "    output_cluster_assignment=False,\n",
    "    num_init=10,\n",
    "    num_clusters=num_clusters,\n",
    "    iter_max=50,\n",
    "    seed= 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff4611-831c-4324-b0e5-6ce3c9efde23",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(kmeans_out.model_data, \"operator_log_clustermodel\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9b156-2021-477c-b20b-759312ecae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_relevant_tables.append(\"operator_log_clustermodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b39fa8-9c49-4021-9ef2-a3c52607e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframes_in_tabs(list_relevant_tables,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39d9f0-aaf3-4d09-88dd-b9de740ba701",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<b style = 'font-size:18px;font-family:Arial;'>4.1 Naming the Clusters</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fa620-33fe-418a-b02e-9a99db70fc87",
   "metadata": {},
   "source": [
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "To make our cluster-based features more <b>interpretable</b>, we give each cluster a short, meaningful name—\n",
    "a kind of label that hints at what the grouped messages are all about. \n",
    "Since the clusters group together similar log texts, \n",
    "we can look at the ones closest to each centroid and get a feel for the common theme.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "Here’s how we do it: we pull the <b>top 5 messages</b> nearest to each cluster center and scan them \n",
    "(either by eye or with a little help from GPT) to figure out what kind of issue or pattern the cluster represents. \n",
    "These names—like <code>sensor_malfunction_issues</code> or <code>filter_replacement_status</code>—\n",
    "become the column names in our feature set. \n",
    "That way, when we look at feature importances later, we’re not staring at <code>cluster_7</code> \n",
    "but at something that actually makes sense.\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;\">\n",
    "It’s a small step that pays off big in terms of clarity—great for explaining the model to others, \n",
    "debugging unexpected behavior, or meeting <b>interpretability requirements</b> in regulated environments.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a631c1-a42c-4fba-9d1b-a6dd494633f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute distance of each log embedding to cluster centroids using in-DB KMeans\n",
    "DF_clusterdistance = KMeansPredict(\n",
    "    data=DF_train_embeddings,\n",
    "    object=DataFrame(in_schema(username, \"operator_log_clustermodel\")),\n",
    "    output_distance=True\n",
    ").result\n",
    "\n",
    "# Step 2: Rank messages within each cluster by distance to centroid (ascending)\n",
    "DF_clusterdistance = DF_clusterdistance.assign(\n",
    "    rank_distance=DF_clusterdistance.td_distance_kmeans.window(\n",
    "        partition_columns=DF_clusterdistance.td_clusterid_kmeans,\n",
    "        order_columns=DF_clusterdistance.td_distance_kmeans\n",
    "    ).dense_rank()\n",
    ")\n",
    "\n",
    "# Step 3: Keep only top 5 closest messages per cluster\n",
    "DF_clusterdistance_top = DF_clusterdistance.loc[DF_clusterdistance.rank_distance <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6262da-fb39-4d81-945a-489fa92ae347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Join back to original logs to retrieve the text content\n",
    "DF_topmesages = DF_clusterdistance_top.join(\n",
    "    DF_train.select([\"row_id\", \"maintenance_log_aug\"]),\n",
    "    how=\"inner\",\n",
    "    on=[\"row_id = row_id\"],\n",
    "    lsuffix=\"a\"\n",
    ").select([\"td_clusterid_kmeans\", \"maintenance_log_aug\"]).drop_duplicate()\n",
    "\n",
    "# Step 5: Convert to pandas for downstream processing\n",
    "df_topmessages = DF_topmesages.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc30405-5226-4656-9d87-6e5111904bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topmessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597676b6-caea-4c2e-9ee5-781f46d778cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True if you want to dynamically generate topic names via OpenAI, you need an OpenAI key for this\n",
    "I_Have_an_OpenAI_API_Key = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7565f8-86ca-47e0-a251-e0a4a99a7e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if I_Have_an_OpenAI_API_Key:\n",
    "    import os, getpass\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OPENAI API KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362ac33-908a-4986-9e63-b7e122400616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI prompt for generating interpretable topic names\n",
    "if I_Have_an_OpenAI_API_Key:\n",
    "    prompt_template = \"\"\"Your task is to identify a common topic of 10 messages that have shown similar vector embeddings. \n",
    "    Your answer should be a single string, maximum 30 characters, only using lowercase latin alphabet and underscores. \n",
    "    The cosine similarity with this topic will be used as a column name in a database table. Only return this column name.\n",
    "    Here are the messages:\n",
    "    \n",
    "    {messages}\n",
    "    \n",
    "    ====\n",
    "    Column Name:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b46c3c6-11b5-4957-b541-d035fcc8244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if I_Have_an_OpenAI_API_Key:\n",
    "    from openai import OpenAI\n",
    "    column_names =  {}\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    for i in range (15):\n",
    "        cluster_feedback = '\\n\\n'.join(df_topmessages[df_topmessages['td_clusterid_kmeans'] == i]['maintenance_log_aug'])\n",
    "        this_prompt = prompt_template.format(messages = cluster_feedback)\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": this_prompt,\n",
    "                    }\n",
    "                ],\n",
    "                model=\"gpt-4o\",\n",
    "                temperature=0,\n",
    "                max_tokens=4096\n",
    "            )\n",
    "            column_names[i] = chat_completion.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to call OpenAI API: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a5f6ab-f404-4c36-82f4-f0a409aff28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback: Predefined topics from a previous run\n",
    "if not I_Have_an_OpenAI_API_Key :\n",
    "    column_names = {\n",
    "        0: 'emergency_stop_test_success',\n",
    "        1: 'no_unusual_noises_detected',\n",
    "        2: 'steady_power_consumption',\n",
    "        3: 'control_panel_status',\n",
    "        4: 'sensors_status_normal',\n",
    "        5: 'routine_maintenance_completed',\n",
    "        6: 'filter_replacement_status',\n",
    "        7: 'temperature_status',\n",
    "        8: 'slight_leak_detection',\n",
    "        9: 'no_leaks_or_valve_issues',\n",
    "        10: 'cleaned_debris_pump_area',\n",
    "        11: 'steady_pressure_and_lubrication',\n",
    "        12: 'slight_increase_in_noise',\n",
    "        13: 'coolant_maintenance',\n",
    "        14: 'latest_version_software_update'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566754bb-2d85-4635-a6e0-96a3146a0c56",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<b style = 'font-size:18px;font-family:Arial;'>Vector Distance-Based Feature Extraction (and Reuse)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72a7ea-a004-45da-9464-cf78426f7dfe",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;'>4.2 Using Clusters as Features</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>To actually use our clusters as features, we need to answer a simple question: <em>how similar is each log message to each cluster/topic?</em> We do this by computing the <b>cosine similarity</b> between each embedding and the cluster centroids. This tells us how strongly a given message matches each semantic pattern we discovered earlier.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The nice part is that we can do this entirely <b>inside the Teradata database</b>, using <code>VectorDistance</code> to compute similarities and <code>Pivoting</code> to turn them into individual columns—one for each cluster. No data leaves the platform; everything stays within Teradata.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>We wrapped this logic into a small reusable function, so we can apply the same transformation to both training and test data. The output is a tidy table of features, ready to be joined with the structured sensor data.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf4e7a-3f18-4b56-8207-0f467e4dca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid_features(DF_embeddings):\n",
    "    \"\"\"\n",
    "    Compute similarity features between input embeddings and cluster centroids,\n",
    "    and pivot the top-k similarities into individual columns.\n",
    "\n",
    "    Args:\n",
    "        DF_embeddings (DataFrame): A teradataml DataFrame containing row-wise embedding vectors.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with pivoted similarity features to cluster centroids.\n",
    "    \"\"\"\n",
    "    DF_VD = VectorDistance(\n",
    "        target_data=DF_embeddings,\n",
    "        target_id_column=\"row_id\",\n",
    "        reference_data=DataFrame(in_schema(username, \"operator_log_clustermodel\")),\n",
    "        ref_id_column=\"td_clusterid_kmeans\",\n",
    "        distance_measure=\"COSINE\",\n",
    "        target_feature_columns=f\"emb_0:emb_{number_dimensions_output-1}\",\n",
    "        ref_feature_columns=f\"emb_0:emb_{number_dimensions_output-1}\",\n",
    "        topk=15\n",
    "    ).result\n",
    "\n",
    "    DF_VD2 = DF_VD.assign(\n",
    "        similarity=1.0 - DF_VD.distance,\n",
    "        row_id=DF_VD.target_id\n",
    "    )\n",
    "\n",
    "    pivot_obj = Pivoting(\n",
    "        data=DF_VD2,\n",
    "        data_partition_column=\"row_id\",\n",
    "        data_order_column=\"reference_id\",\n",
    "        partition_columns=\"row_id\",\n",
    "        target_columns=\"similarity\",\n",
    "        rows_per_partition=15,\n",
    "        output_column_names=[\"logmessagesimilarity_\" + c for c in list(column_names.values())]\n",
    "    )\n",
    "\n",
    "    DF_VD_pivoted = pivot_obj.result\n",
    "\n",
    "    return DF_VD_pivoted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec56c54-f2b4-4ffe-9bf1-179c8e2ada32",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_logfeatures_train = get_centroid_features(DF_train_embeddings)\n",
    "DF_logfeatures_test = get_centroid_features(DF_test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e692d45-0c52-4b64-b631-991054994669",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Finally, we join the structured sensor features with the newly generated log similarity features - creating a unified dataset ready for supervised learning.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8edcfa3-aa5f-451f-bbd7-2652d70f17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge training data with log-derived features and drop redundant columns\n",
    "DF_train_ADS = DF_train.merge(\n",
    "    DF_logfeatures_train,\n",
    "    on=\"row_id\",\n",
    "    rsuffix=\"emb\"\n",
    ").drop(\n",
    "    columns=[\"maintenance_log_aug\", \"row_id_emb\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eca46d-d6fa-4298-8ee1-e8b0a77d6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge test data with log-derived features and drop redundant columns\n",
    "DF_test_ADS = DF_test.merge(\n",
    "    DF_logfeatures_test,\n",
    "    on=\"row_id\",\n",
    "    rsuffix=\"emb\"\n",
    ").drop(\n",
    "    columns=[\"maintenance_log_aug\", \"row_id_emb\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9ceb8-8904-415f-a8e1-5e78243538cf",
   "metadata": {},
   "source": [
    "\n",
    "<b style = 'font-size:18px;font-family:Arial;'>Testing Feature Value: Structured, Unstructured, or Both?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80d655-169e-4a5d-a0e3-530dc076fa12",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>To evaluate whether the log-derived embedding features actually improve predictive performance, we train three separate models using different feature sets: (1) classic sensor features only, (2) text embedding–based features only, and (3) a combination of both. </p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The model choice—CatBoost—is arbitrary; it’s a fast, reliable classifier that works well with heterogeneous feature types, can deal with inter-correlated features and doesn’t require much preprocessing. </p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>What matters here isn’t fine-tuning the model, but comparing performance across feature sets. This experiment helps us understand whether the unstructured log data carries additional signal beyond what’s already available in the structured telemetry. Once satisfied, we can export the trained model to ONNX and use Teradata’s Bring Your Own Model (BYOM) capabilities to run the entire inference pipeline—embedding generation, feature extraction, and model scoring— fully inside the database.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7685591-4c15-4941-9977-fa032bfce8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling data into Python memory for model training\n",
    "df_train = DF_train_ADS.to_pandas()\n",
    "df_test = DF_test_ADS.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4bb77-5370-4df2-a2a3-873051b38d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb02d3-035d-4392-9d76-c3304521b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target column\n",
    "target = \"outage_next_24h\"\n",
    "\n",
    "# Identify feature groups: classic sensor features (first 15), text-based features (the rest)\n",
    "classic_features = list(df_train.columns[1:16])\n",
    "text_features = list(df_train.columns[16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c323d-07e7-4a30-8085-8dbe4935573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training features into classic, text-only, and combined sets\n",
    "X_train_classic = df_train[classic_features]\n",
    "X_train_text = df_train[text_features]\n",
    "X_train_combined = df_train[classic_features + text_features]\n",
    "\n",
    "# Split test features in the same way\n",
    "X_test_classic = df_test[classic_features]\n",
    "X_test_text = df_test[text_features]\n",
    "X_test_combined = df_test[classic_features + text_features]\n",
    "\n",
    "# Extract training and test targets\n",
    "y_train = df_train[target]\n",
    "y_test = df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df276cf-72e8-43ad-9dda-296738c845e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reusable function to train and evaluate a CatBoost model\n",
    "def train_eval(X_train, X_test, y_train, y_test, label):\n",
    "    # Initialize CatBoost with reasonable defaults and class imbalance handling\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        eval_metric='F1',\n",
    "        scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),  # handle class imbalance\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict labels and probabilities\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Return all relevant results and the trained model\n",
    "    return {\n",
    "        'label': label,\n",
    "        'f1': f1,\n",
    "        'cm': cm,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'auc': roc_auc,\n",
    "        'report': report,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "# Run experiments with different feature sets and collect results\n",
    "results = []\n",
    "results.append(train_eval(X_train_classic, X_test_classic, y_train, y_test, 'Classic Features'))\n",
    "results.append(train_eval(X_train_text, X_test_text, y_train, y_test, 'Text Features'))\n",
    "results.append(train_eval(X_train_combined, X_test_combined, y_train, y_test, 'Combined Features'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a94052-2e5c-4592-b139-79b63fe6e9ea",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>The results speak for themselves: mixing structured and unstructured features gives us the best model performance. Using just the sensor data gets us a decent F1 score of 0.29 and an AUC of 0.80, but it struggles to catch the minority class (i.e., the actual outages). The text-only model does slightly worse on F1 (0.19), but interestingly, its AUC is still strong at 0.81—suggesting the log embeddings add something useful, even on their own.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "But the real magic happens when we put both feature sets together. The F1 score jumps to 0.51, and the AUC shoots up to 0.95. That’s a solid gain, and it means we’re catching more real outages without making a mess of the rest. So those operator comments? Turns out they’re worth keeping.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a72482-9deb-49c6-bc52-e867475fbddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Summary\n",
    "for res in results:\n",
    "    print(f\"--- {res['label']} ---\")\n",
    "    print(f\"F1 Score: {res['f1']:.4f}\")\n",
    "    print(f\"AUC: {res['auc']:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(res['cm'])\n",
    "    print(\"Classification Report:\")\n",
    "    for label, metrics in res['report'].items():\n",
    "        if isinstance(metrics, dict):\n",
    "            print(f\"  {label}: precision={metrics['precision']:.2f}, recall={metrics['recall']:.2f}, f1={metrics['f1-score']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4197201-b068-4a7e-90e9-7c4baa9b4ae1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "The ROC plot backs this up nicely—across the full range of thresholds, the combined model clearly outperforms the others. It’s a strong signal that **operator logs really do bring extra context** that complements the sensor data. Those embedding-based text features aren’t just filler—they genuinely boost the model’s ability to catch outages more reliably.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab569658-7038-4294-9ffc-b2082ad4b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "# ROC Plot\n",
    "roc_traces = [\n",
    "    go.Scatter(x=res['fpr'], y=res['tpr'], mode='lines', name=f\"{res['label']} (AUC={res['auc']:.2f})\")\n",
    "    for res in results\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"ROC Curves\",\n",
    "    xaxis=dict(title='False Positive Rate'),\n",
    "    yaxis=dict(title='True Positive Rate'),\n",
    "    width=700, height=500\n",
    ")\n",
    "fig = go.Figure(data=roc_traces, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297036bd-1250-47c0-b54e-0e6e77083d4d",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;'>4.3 Feature Importances  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f09e46-b1ae-4d15-9b22-63d464d5f8a8",
   "metadata": {},
   "source": [
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "Knowing which features matter most isn't just a nice-to-have—it’s crucial for understanding, trusting, and explaining the model. Since we’re blending sensor readings with log-based embeddings, it’s important to see what’s actually driving predictions. That’s where our earlier decision to turn high-dimensional text into interpretable prototype distances really pays off. Instead of dealing with opaque vectors, we get meaningful signals like “how much does this message sound like a known malfunction?”</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "And it works. The usual suspects like <code>motor_temp</code> and <code>rpm</code> show up at the top, but so do several log-based features—like <code>logmessagesimilarity_inspection_results</code> and <code>logmessagesimilarity_sensor_malfunction_issues</code>. This solid mix confirms that both data types pull their weight, and the model is learning from text in a way that’s not just useful, but explainable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22799b-7e13-4ad9-a1e8-42d24c2d6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the combined model (results[2] corresponds to combined features)\n",
    "importances = results[2][\"model\"].get_feature_importance()\n",
    "\n",
    "# Get corresponding feature names\n",
    "feat_names = results[2][\"model\"].feature_names_\n",
    "\n",
    "# Create a DataFrame to organize feature names and their importance values\n",
    "df_plot = pd.DataFrame({'Feature': feat_names, 'Importance': importances})\n",
    "\n",
    "# Add a color column: red for text-based features, blue for classic sensor features\n",
    "df_plot['Color'] = df_plot['Feature'].apply(\n",
    "    lambda x: 'red' if x.startswith('logmessagesimilarity') else 'blue'\n",
    ")\n",
    "\n",
    "# Sort features by importance, descending\n",
    "df_plot.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Create a horizontal bar chart of feature importances using Plotly\n",
    "fig = px.bar(\n",
    "    df_plot,\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    orientation='h',\n",
    "    color='Color',\n",
    "    color_discrete_map='identity',  # use literal color values from the DataFrame\n",
    "    category_orders={'Feature': df_plot['Feature'].tolist()},  # preserve sorted order\n",
    "    labels={'Importance': 'Importance', 'Feature': 'Feature'}\n",
    ")\n",
    "\n",
    "# Final layout tweaks and display\n",
    "fig.update_layout(title='Feature Importances', height=1000)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c732b49-c138-4240-84e9-9f54272fe52c",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>5. Deployment</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041f061-59c1-4289-9a83-79ae53542df7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Now it's time to wrap things up with deployment. In this last part, we show how to get the full pipeline running inside Teradata, even when the Python environment is gone. First, we export the trained CatBoost model to ONNX and store it in Vantage. Then, we rebuild the entire prediction flow using teradataml DataFrames—embedding generation, similarity scoring, model inference, the works. Finally, we collapse it all into one clean SQL query using a Common Table Expression (CTE), so the whole thing is ready to run entirely in-database, production-style.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d6a7c9-9b8f-469f-a57b-2e6e52760644",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>5.1 Exporting and Registering the Model with Teradata BYOM</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0c037-e682-417b-8524-cace740d92f7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Here’s where we turn the trained CatBoost model into something Teradata can run in-database. We convert it to ONNX format and upload it to a BYOM model table—so it's ready to plug straight into the inference pipeline.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b981b20-dd12-4949-b26e-035aeee75073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set BYOM install location\n",
    "configure.byom_install_location = \"mldb\"\n",
    "\n",
    "# Save CatBoost model to ONNX format\n",
    "onnxfile_catb = \"catb-outage.onnx\"\n",
    "model_catb_id = \"catb_outage\"\n",
    "\n",
    "results[2][\"model\"].save_model(\n",
    "    onnxfile_catb,\n",
    "    format=\"onnx\",\n",
    "    export_parameters={\n",
    "        'onnx_domain': 'ai.catboost',\n",
    "        'onnx_model_version': 1,\n",
    "        'onnx_doc_string': 'model for Outage prediction',\n",
    "        'onnx_graph_name': 'CatBoostModel_for_OutagePrediction'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save ONNX model to Teradata BYOM table\n",
    "model_table_name = \"outage_models\"\n",
    "try:\n",
    "    db_drop_table(model_table_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "save_byom(\n",
    "    model_id=model_catb_id,\n",
    "    model_file=onnxfile_catb,\n",
    "    table_name=model_table_name\n",
    ")\n",
    "\n",
    "# Display saved model table\n",
    "display_dataframes_in_tabs([model_table_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91ab29-5270-4ba1-a319-a2fe6c5e4fca",
   "metadata": {},
   "source": [
    "\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>5.1 Develop Inference Pipeline\n",
    "</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2d001-96d4-4746-9f92-5b34fd0d0d09",
   "metadata": {},
   "source": [
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>In the next step, we string everything together into a full pipeline—right inside Teradata. We start by simulating new inference data and run it through several in-database ML steps: generate embeddings with a Hugging Face model, compute similarity scores to our KMeans cluster centers, combine everything with the sensor features, and finally run inference with the CatBoost model in ONNX format. One neat trick here is using <code>literal_column</code> to write raw SQL for extracting the label and probability from the ONNX JSON output. All of this happens in-database, no data movement, just Python and SQL playing nicely together.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7424a76-3585-4f00-b3d5-2687121ace58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate new inference data by creating a subset of test data \n",
    "(DF_test\n",
    " .loc[DF_test.row_id < 100]\n",
    " .drop(columns=[\"outage_next_24h\"])\n",
    " .to_sql(\"pump_failure_inference\", primary_index=\"row_id\", if_exists=\"replace\")\n",
    ")\n",
    "\n",
    "# Reference the inference data\n",
    "DF_inference = DataFrame(in_schema(username, \"pump_failure_inference\"))\n",
    "\n",
    "# Step 1: Generate sentence embeddings using BYOM HuggingFace model (all processing in-database)\n",
    "DF_embeddings_inference = ONNXEmbeddings(\n",
    "    newdata=DF_inference.assign(\n",
    "        row_id=DF_inference.row_id,\n",
    "        txt=DF_inference.maintenance_log_aug,\n",
    "        drop_columns=True\n",
    "    ),\n",
    "    modeldata=my_model,\n",
    "    tokenizerdata=my_tokenizer,\n",
    "    accumulate=[\"row_id\"],\n",
    "    model_output_tensor=\"sentence_embedding\",\n",
    "    output_format=f'FLOAT32({number_dimensions_output})',\n",
    "    enable_memory_check=False\n",
    ").result\n",
    "\n",
    "# Step 2: Compute similarity features to precomputed cluster centroids with the previously defined function\n",
    "DF_logfeatures_inference = get_centroid_features(DF_embeddings_inference)\n",
    "\n",
    "# Step 3: Join similarity features with original inference features in Teradata\n",
    "DF_inference_ADS = (DF_inference\n",
    "    .merge(DF_logfeatures_inference, on=\"row_id\", rsuffix=\"emb\")\n",
    "    .drop(columns=[\"maintenance_log_aug\", \"row_id_emb\"])\n",
    ")\n",
    "\n",
    "# Step 4: Run classification inference using the deployed ONNX classification model\n",
    "DF_predictions_raw = ONNXPredict(\n",
    "    newdata=DF_inference_ADS,\n",
    "    modeldata=DataFrame(in_schema(username, model_table_name)),\n",
    "    accumulate=[\"row_id\"],\n",
    "    overwrite_cached_models=\"*\",\n",
    "    model_input_fields_map=f\"features={feat_names[0]}:{feat_names[-1]}\"\n",
    ").result\n",
    "\n",
    "from sqlalchemy.sql import literal_column as col\n",
    "\n",
    "# Step 5: Extract label and probability values from ONNX JSON output. col allows us to directly use SQL for creating new columns.\n",
    "DF_predictions_final = DF_predictions_raw.assign(\n",
    "    row_id=DF_predictions_raw.row_id,\n",
    "    json_report_json=col(\"NEW JSON (json_report)\"),\n",
    "    outage_prediction=col(\"CAST(json_report_json.JSONExtractValue('$.label[0]') AS INTEGER)\"),\n",
    "    outage_probability=col(\"CAST(json_report_json.JSONExtractValue('$.probabilities[0].value.1') AS DECIMAL(7,6))\"),\n",
    "    drop_columns=True\n",
    ").select([\"row_id\", \"outage_prediction\", \"outage_probability\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8458e7-9b81-484c-9584-f2553a9b8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_predictions_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8353227-f900-459f-ac4b-249ceb26cb06",
   "metadata": {},
   "source": [
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "<center><img src=\"images/flowchart_embeddingsfeatures.png\" alt=\"workflow_topictrend\" style=\"border: 4px solid #404040; border-radius: 10px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb8751-5929-46e1-a6f0-ce9c7601c6e1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>So far, everything runs nicely inside the database—but it still depends on an active Python session with teradataml to tie it all together. To make the pipeline truly deployable, we need something more portable. That’s where capturing the entire logic as plain SQL comes in. Using a small utility called show_CTE_query(), we can turn the full chain of transformations—embedding generation, similarity scoring, pivoting, feature joining, and inference—into one big Common Table Expression (CTE). The result is a standalone SQL query that mirrors our Python logic and can be deployed as a view, macro, or job—no need for Python to be running in the background.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5dd69-af1c-43f4-883e-a5187ac8591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_CTE_qu = DF_predictions_final.show_CTE_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ca6cf-bae5-4dd9-8826-df5a98d5e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"```sql\\n{inference_CTE_qu}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3008b6-f80e-4a90-a7e6-a56aaaf5ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame.from_query(inference_CTE_qu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62a748-c7b2-4bf3-9787-bb4ad05ed823",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Now that we’ve wrapped the whole pipeline into a single SQL query, the obvious next step is to turn it into a view. This gives us a clean, reusable object inside the database that handles everything—from text embedding to final prediction—in one go. And the best part? Anyone with the access rights can now query the view directly from tools like Teradata Studio or any SQL client, no Python required.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a7a99-0556-46ad-a516-795524f0f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the deployed inference view\n",
    "inference_view_name = \"outage_inference_v\"\n",
    "\n",
    "# Create or replace the view using the full CTE-based SQL pipeline\n",
    "execute_sql(f\"\"\"\n",
    "    REPLACE VIEW {username}.{inference_view_name} AS\n",
    "    {inference_CTE_qu}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af89df-8a57-4505-8e9a-7fd2a1ddf6e1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>To make sure our view is doing what it should, we run a few quick sanity checks: first, we query it with some known data and confirm it returns predictions. Then we clear the input table—it should return nothing. Finally, we insert fresh test records and see new predictions show up. These simple tests give us confidence that the whole thing is working reliably and ready for production, whether it's part of a bigger pipeline or just used for on-demand scoring.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf164627-7887-4dcf-8247-515a7eb3d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview inference results (expecting row_ids < 100 from initial sample)\n",
    "DataFrame(inference_view_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2550c-344f-43c1-a5fa-13603102ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the inference table\n",
    "execute_sql(f\"DELETE FROM {username}.pump_failure_inference\")\n",
    "\n",
    "# Re-check the view (should now return no rows)\n",
    "DataFrame(inference_view_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18dc82-306c-43b1-b6a1-0cf58635fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new test data into the inference table (row_ids between 1000 and 1050, without labels)\n",
    "execute_sql(f\"\"\"\n",
    "    INSERT INTO {username}.pump_failure_inference\n",
    "    SELECT * FROM ANTISELECT(\n",
    "        ON {username}.pump_failure_test\n",
    "        USING Exclude ('outage_next_24h')\n",
    "    ) t\n",
    "    WHERE row_id > 1000 AND row_id < 1050\n",
    "\"\"\")\n",
    "\n",
    "# Preview updated inference results (now expecting row_ids between 1000 and 1050)\n",
    "DataFrame(inference_view_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a83008-cffd-4c52-9a23-3ab2c6ccae41",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial;'><b>Conclusion\n",
    "</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ab247-a689-4f3b-9dd3-2be8d5a747e9",
   "metadata": {},
   "source": [
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "And that’s a wrap! What we’ve built here is a practical and powerful way to bring structured and unstructured data together for predictive modeling. By turning operator log texts into interpretable similarity features and combining them with sensor data, we’ve managed to boost model performance while keeping things explainable.</p>\n",
    "\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The best part? Everything runs fully inside Teradata. Embeddings, similarity scoring, classification—all of it happens in-database. And by exporting the model to ONNX and capturing the whole pipeline as a SQL query, we’ve made it super easy to deploy. Just wrap it in a view, and suddenly any analyst or app with SQL access can run real-time predictions—no Python or extra infrastructure needed.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;'>This pattern isn’t limited to pump failures or predictive maintenance. You can reuse it for all sorts of use cases where unstructured data matters—think fraud detection, churn prediction, service ticket triage, and more. If you've got logs, notes, or any kind of free text, this is a great way to turn them into something your models can use and your business can trust.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6e3bf-9fbb-40b6-a385-ee9ece4cebd2",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial;'>6. Cleanup</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c003e08-1ec3-430e-be1e-5a85f928c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/_dataremove.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e55d9-b45f-4676-ba1c-6a3470706a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4d1cf-d1d8-4238-8dfe-af7675ea39e1",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
