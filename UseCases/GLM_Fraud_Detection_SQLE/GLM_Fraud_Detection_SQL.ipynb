{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<header style=\"padding:1px;background:#f9f9f9;border-top:3px solid #00b2b1\"><img id=\"Teradata-logo\" src=\"https://www.teradata.com/Teradata/Images/Rebrand/Teradata_logo-two_color.png\" alt=\"Teradata\" width=\"220\" align=\"right\" />\n",
    "\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>GLM Fraud Detection with Python and Teradata SQL</b>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Introduction:</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "    In recent years we have seen a huge increase in Fraud attempts, making fraud detection necessary for Banking and Financial Institutions. Despite countless efforts and human supervision, hundreds of millions are lost due to fraud. Fraud can happen using various methods i.e., stolen credit cards, misleading accounting, phishing emails, etc. Due to small cases in large population detection of fraud is important as well as challenging.\n",
    "    <br>\n",
    "    <br>\n",
    "    This notebook provides a demonstration of \"data science workflow\" that illustrates how to leverage Vantage's <b>Advanced SQL Engine (was NewSQL Engine)</b> to build, validate and score a model at scale in Vantage without moving the data. Users can perform the large-scale operations such as feature analysis, data transformation, Model training and ML Model Scoring in the Vantage environment without having to move data.</p>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#E37C4D'><b>Steps in the analysis:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Configuring the environment</li>\n",
    "    <li>Initiate a connection to Vantage</li>\n",
    "    <li>Read the data from Vantage as a teradataml Dataframe</li>\n",
    "    <li>Clean up the dataset</li>\n",
    "    <li>Create training and testing datasets in Vantage</li>\n",
    "    <li>In-Database GLM model training</li>\n",
    "    <li>In-Database GLM model scoring</li>\n",
    "    <li>Visualize the results (ROC curve and AUC)</li>\n",
    "    <li>Cleanup</li>\n",
    "</ol>\n",
    "\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'> Accessing the Data\n",
    "<p style = 'font-size:16px;font-family:Arial'>These demos will work either with foreign tables accessed from Cloud Storage via NOS or you may import the tables to your machine. If you import data for multiple demos, you may need to use the Data Dictionary \"Manage Your Space\" routine to cleanup tables you no longer need. \n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial'>Use the link below to access the 2 options for using data from the data dictionary notebook:\n",
    "\n",
    "[Click Here to get data for this notebook](../Data_Dictionary/Data_Dictionary.ipynb#TRNG_GLMFraud)\n",
    "\n",
    "[Click Here to Manage Your Space](../Data_Dictionary/Data_Dictionary.ipynb#Manage_Your_Space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>1. Configuring the Environment</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In the section, we import the required libraries and set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from teradataml.dataframe.dataframe import DataFrame\n",
    "from teradataml.dataframe.copy_to import copy_to_sql\n",
    "from teradataml.dataframe.dataframe import in_schema\n",
    "from teradataml.context.context import create_context, remove_context\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>2. Initiate a connection to Vantage</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>You might be prompted to enter the password.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = getpass.getpass())\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>3. Read the data from Vantage as a teradaml Dataframe</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The data from <a href = 'https://www.kaggle.com/code/georgepothur/4-financial-fraud-detection-xgboost/data'>https://www.kaggle.com/code/georgepothur/4-financial-fraud-detection-xgboost/data</a> is loaded in Vantage in a table named \"transaction_data\". Check the data size and print sample rows. 63k rows and 12 columns.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Please scroll down to the end of the notebook for detailed column descriptions of the dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_data = DataFrame(in_schema('TRNG_GLMFraud','transaction_data'))\n",
    "\n",
    "print(txn_data.shape)\n",
    "txn_data.to_pandas(num_rows = 5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Here we rename a misspelt column without moving the data out of Vantage. We are renaming <b>oldbalanceOrg</b> to <b>oldbalanceOrig</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = txn_data.assign(oldbalanceOrig = txn_data.oldbalanceOrg).drop(['oldbalanceOrg'] , axis=1)\n",
    "\n",
    "new_data.to_pandas(num_rows = 5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>These transactions are made by the fraudulent agents inside a simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Below are some insights about the dataset:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>There are 92 fraud transactions i.e. 0.14% of fraud transactions in the dataset.</li>\n",
    "    <li>From these 92 fraud transactions, 47 are of type TRANSFER and 45 are of type CASH_OUT.</li>\n",
    "    <li>97.83% of fraud transations have transaction amount equal to oldbalanceOrig i.e. account cleanout.</li>\n",
    "    <li>71.74% of fraud transactions have recipient's old balance as zero.</li>\n",
    "    <li>isFlaggedFraud is correct only two times among the 92 fraud transactions.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>4. Clean up the dataset</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Based on what we discovered above, we will:</p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Remove all data that isn't 'CASH OUT' or 'TRANSFER'.</li>\n",
    "    <li>Drop \"nameOrig\" and \"nameDest\" since the origin and destination accounts don't matter.</li>\n",
    "    <li>Drop \"isFlaggedFraud\" because it has just flagged two transactions. Hence it doesn't have much significance.</li>\n",
    "</ol> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = new_data.loc[(new_data.type == 'CASH_OUT') | (new_data.type == 'TRANSFER')]\n",
    "clean_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Now our dataset is reduced to 27k records.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.drop(['nameDest', 'nameOrig', 'isFlaggedFraud'], axis = 1)\n",
    "clean_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the source data table in the database\n",
    "clean_data.to_sql('clean_data', if_exists = 'replace', primary_index='txn_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>5. Create training and testing datasets in Vantage</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>We'll perform the following steps:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Create Training and Testing datasets in the ratio 80:20</li>\n",
    "    <li>Use TD_ScaleFit to scale the dataset</li>\n",
    "    <li>Use TD_ScaleTransform to transform the training and testing datasets</li>\n",
    "</ul>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>TD_ScaleFit outputs a table of statistics to input to TD_ScaleTransform, which scales specified input table columns. TD_ScaleTransform scales specified input table columns, using TD_ScaleFit output.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Feature scaling is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clean_data_train table using SAMPLE\n",
    "\n",
    "qry = '''\n",
    "CREATE MULTISET TABLE clean_data_train \n",
    "  AS(SELECT * FROM clean_data SAMPLE 0.8) \n",
    "WITH DATA\n",
    "PRIMARY INDEX (txn_id);\n",
    "'''\n",
    "\n",
    "try:\n",
    "    eng.execute(qry)\n",
    "except:\n",
    "    eng.execute('DROP TABLE clean_data_train;')\n",
    "    eng.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''SELECT * FROM TD_ScaleFit(\n",
    "    ON clean_data_train as InputTable\n",
    "    OUT VOLATILE TABLE OutputTable(scale_train)\n",
    "    USING\n",
    "    TargetColumns('step', 'amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig')\n",
    "    ScaleMethod('STD')\n",
    ") as dt;'''\n",
    "\n",
    "eng.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''CREATE MULTISET TABLE transformed_train AS (\n",
    "    SELECT * FROM TD_ScaleTransform(\n",
    "        ON clean_data_train AS InputTable\n",
    "        ON scale_train AS FitTable DIMENSION\n",
    "        USING\n",
    "        accumulate('txn_id', 'isFraud')\n",
    "    ) AS dt\n",
    ") WITH data;'''\n",
    "\n",
    "eng.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clean_data_test table using SAMPLE\n",
    "\n",
    "qry = '''\n",
    "CREATE MULTISET TABLE clean_data_test\n",
    "  AS(SELECT * FROM clean_data\n",
    "    EXCEPT\n",
    "    SELECT * FROM clean_data_train)\n",
    "WITH DATA\n",
    "PRIMARY INDEX (txn_id); \n",
    "'''\n",
    "\n",
    "try:\n",
    "    eng.execute(qry)\n",
    "except:\n",
    "    eng.execute('DROP TABLE clean_data_test;')\n",
    "    eng.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''SELECT * FROM TD_ScaleFit(\n",
    "    ON clean_data_test as InputTable\n",
    "    OUT VOLATILE TABLE OutputTable(scale_test)\n",
    "    USING\n",
    "    TargetColumns('step', 'amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig')\n",
    "    ScaleMethod('STD')\n",
    ") as dt;'''\n",
    "\n",
    "eng.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''CREATE MULTISET TABLE transformed_test AS (\n",
    "    SELECT * FROM TD_ScaleTransform(\n",
    "        ON clean_data_test AS InputTable\n",
    "        ON scale_train AS FitTable DIMENSION\n",
    "        USING\n",
    "        accumulate('txn_id', 'isFraud')\n",
    "    ) AS dt\n",
    ") WITH data;'''\n",
    "\n",
    "eng.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_sql('select * from transformed_train', eng)\n",
    "temp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above output shows that the data has been tranformed into a scaled dataset. Scaling of data makes it easy for the model to learn and understand the problem.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>6. In-Database GLM model training</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The TD_GLM function is a generalized linear model (GLM) that performs regression and classification analysis on data sets, where the response follows an exponential family distribution.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Due to gradient-based learning, the function is highly sensitive to feature scaling. Input features should be standardized, such as using TD_ScaleFit, and TD_ScaleTransform, before using in the function. The function takes only numeric features. The categorical features must be converted to numeric values prior to training. The rows with missing (null) values are skipped by the function during training.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''CREATE VOLATILE TABLE glm_model AS (\n",
    "SELECT * from TD_GLM (\n",
    "ON transformed_train\n",
    "USING\n",
    "InputColumns('step', 'amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig')\n",
    "ResponseColumn('isFraud')\n",
    "Family('Binomial')\n",
    ") as dt) WITH DATA\n",
    "ON COMMIT PRESERVE ROWS\n",
    ";'''\n",
    "\n",
    "eng.execute(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The function output is a trained GLM model which can be input to the TD_GLMPredict function for prediction. The model also contains model statistics of MSE, Loglikelihood, AIC, and BIC.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('select * from glm_model', eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>7. In-Database GLM model scoring</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The TD_GLMPredict function predicts target values (regression) and class labels (classification) for test data using a GLM model trained by the TD_GLM function.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Similar to TD_GLM, input features should be standardized, such as using TD_ScaleFit, and TD_ScaleTransform, before using in the function. The function takes only numeric features. The categorical features must be converted to numeric values prior to prediction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''CREATE VOLATILE TABLE glm_prediction AS (\n",
    "SELECT * from TD_GLMPredict (\n",
    "ON transformed_test AS INPUTTABLE\n",
    "ON glm_model AS Model DIMENSION\n",
    "USING\n",
    "IDColumn ('txn_id')\n",
    "Accumulate('isFraud')\n",
    "OutputProb('true')\n",
    "Responses ('1','0')\n",
    ") AS dt\n",
    ") WITH DATA\n",
    "ON COMMIT PRESERVE ROWS\n",
    ";'''\n",
    "\n",
    "eng.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_sql('select * from glm_prediction;', eng)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The output above shows prob_1 i.e. transaction being fraud and prob_0 i.e. transaction being not fraud. The prediction column uses these probabilities to give a class label i.e. prediction column.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>8. Visualize the results (ROC curve and AUC)</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Calculate mean absolute error and AUC(Area Under the Curve) for Receiver Operating Characteritic Curve</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Mean Absolute Error is the summation of difference of actual and predicted value averaged over the number of observations.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_error(pred['isFraud'], pred['prob_1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>ROC curve is a graph between TPR(True Positive Rate) and FPR(False Positive Rate). The area under the ROC curve is a metric on how good the model is able to distinguish between positive and negative classes. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes. AUC above 0.75 is generally considered decent.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = roc_auc_score(pred['isFraud'], pred['prob_1'])\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(pred['isFraud'], pred['prob_1'])\n",
    "plt.plot(fpr, tpr, color='orange', label='ROC. AUC = {}'.format(str(AUC)))\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Looking at the above ROC Curve we can confidently say that the model has performed well on testing data as well. The AUC value is way above 0.75 and hence resonates with our understanding that the model is performing well.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>9. Cleanup</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.execute('DROP TABLE clean_data;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.execute('DROP TABLE clean_data_train;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.execute('drop table transformed_train;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.execute('DROP TABLE clean_data_test;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.execute('drop table transformed_test;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eng.execute('DROP TABLE glm_model;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eng.execute('DROP TABLE glm_prediction;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>Dataset:</b>\n",
    "\n",
    "- `txn_id`: transaction id\n",
    "- `step`: maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (31 days simulation).\n",
    "- `type`: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER\n",
    "- `amount`: amount of the transaction in local currency\n",
    "- `nameOrig`: customer who started the transaction\n",
    "- `oldbalanceOrig`: customer's balance before the transaction\n",
    "- `newbalanceOrig`: customer's balance after the transaction\n",
    "- `nameDest`: customer who is the recipient of the transaction\n",
    "- `oldbalanceDest`: recipient's balance before the transaction\n",
    "- `newbalanceDest`: recipient's balance after the transaction\n",
    "- `isFraud`: identifies a fraudulent transaction (1) and non fraudulent (0)\n",
    "- `isFlaggedFraud`: flags illegal attempts to transfer more than 200,000 in a single transaction\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#E37C4D'><b>Links:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Uses a dataset and feature discovery methods outlined here: <a href = 'https://www.kaggle.com/georgepothur/4-financial-fraud-detection-xgboost/notebook'>https://www.kaggle.com/georgepothur/4-financial-fraud-detection-xgboost/notebook</a></li>\n",
    "    <li>Teradataml Python reference: <a href = 'https://docs.teradata.com/r/Teradata-Package-for-Python-User-Guide/May-2022/Introduction-to-Teradata-Package-for-Python'>https://docs.teradata.com/r/Teradata-Package-for-Python-User-Guide/May-2022/Introduction-to-Teradata-Package-for-Python</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<footer style=\"padding:10px;background:#f9f9f9;border-bottom:3px solid #394851\">Copyright Â© Teradata Corporation - 2023. All Rights Reserved.</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
