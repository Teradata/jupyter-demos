{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acquired-consideration",
   "metadata": {},
   "source": [
    "<header style=\"padding:1px;background:#f9f9f9;border-top:3px solid #00b2b1\"><img id=\"Teradata-logo\" src=\"https://www.teradata.com/Teradata/Images/Rebrand/Teradata_logo-two_color.png\" alt=\"Teradata\" width=\"220\" align=\"right\" />\n",
    "\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>Energy Consumption Forecasting</b>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701a2ac-9d7c-4498-a23c-7ae3955f6a32",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Introduction:</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>For energy trading companies, forecasting electricity consumption is one key driver in building a successful business. Proper forecasting of market demand prevents losses (in case of overselling energy to market) and lost profits (in case of underestimating demand). Also, the regulator of the energy market can apply fees or even disqualify a trading company for specific periods in case of frequent inaccurate forecasts. This is why increasing the accuracy even by 0.1% can significantly improve the profitability of the energy trading company.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demo, we demonstrate how the entire lifecycle of consumption forecast can be implemented using Vantage technologies and, specifically, the combination of Bring Your Own Model (BYOM), Vantage Analytics Library (VAL) and teradataml python client library solution. This demo consists of four parts (details on Teradata's \"Analytics 1-2-3\" strategy can be found <a href = 'https://assets.teradata.com/resourceCenter/downloads/WhitePapers/Analytics-123-Enabling-Enterprise-AI-at-Scale-MD006623.pdf'>here</a>):</p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Data discovery using client python libraries</li>\n",
    "    <li>Feature Prep and Transformation using Vantage Analytic Library</li>\n",
    "    <li>Model training using the scikit-learn</li>\n",
    "    <li>Scoring the model in Vantage and analyzing the results</li>\n",
    "    </ol>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The dataset used in this demo represents electricity consumption in Norway from the 1st of January 2016 to the 31st of August 2019. Each line in this dataset reflects consumption for one hour. Apart from electricity consumption, this datamart also reflects additional data: weather from multiple sources, daylight information and labour calendar. We collected all data from open data sources.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-turkey",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;color:#E37C4D'>Utilize Vantage to Operationalize the Machine Learning Process</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'Open-source tools and techniques provide a rich ecosystem for data scientists and analysts to gain new insights into their data.  However, the process of obtaining these insights is manual, error-prone, and time-consuming process.  Most machine learning tools and platforms seek to make model training more efficient and ignore the more significant challenges with;</p>\n",
    "\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Data Discovery and Statistical Analysis</li>\n",
    "    <li>Data Preparation and Feature Engineering</li>\n",
    "    <li>Model Deployment and Evaluation At Operational Scale</li>\n",
    "    </ul>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Traditional approaches require the developer to move data <b>from</b> the sources <b>to</b> the analytics.  Even \"integrated\" analytic systems like Apache Spark provide parallel processing for analyzing data but don't optimize for loading data - neither locality nor quantity that needs to be moved.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Teradata Vantage reverses this model; and allows PUSH processing down to the individual processing nodes where the data resides.  This allows for the unprecedented scale of the analytical processing, reduced costs in data movement/egress charges, and drastically improved performance.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d444f9-e2f7-4282-a293-96802ff2db7b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Downloading and installing additional software needed</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c074f7-9866-4af0-b822-434435c361a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # '%%capture' suppresses the display of installation steps of the following packages\n",
    "# !pip install sklearn2pmml\n",
    "# !pip install jdk4py\n",
    "# !pip install teradataml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10db2ec-ce66-411d-bb03-b9262752d667",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><b>Note: </b><i>The above statements may need to be uncommented if you run the notebooks on a platform other than ClearScape Analytics Experience that does not have the libraries installed. If you uncomment those installs, be sure to restart the kernel after executing those lines to bring the installed libraries into memory. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>\n",
    "</div>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d4207-bc98-428b-93d3-7955c99a34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from jdk4py import JAVA, JAVA_HOME, JAVA_VERSION\n",
    "\n",
    "from teradataml import *\n",
    "from teradataml.analytics.valib import *\n",
    "from teradataml.analytics.Transformations import *\n",
    "from teradataml.dataframe.copy_to import copy_to_sql\n",
    "from teradataml.dataframe.dataframe import DataFrame, in_schema\n",
    "from teradataml.context.context import create_context, remove_context, get_connection\n",
    "from teradataml.options.display import display\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn2pmml.pipeline import PMMLPipeline\n",
    "from sklearn2pmml import sklearn2pmml\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "display.max_rows = 5\n",
    "\n",
    "# Modify the following to match the specific client environment settings\n",
    "configure.val_install_location = 'val'\n",
    "configure.byom_install_location = 'mldb'\n",
    "os.environ['PATH'] = os.pathsep.join([os.environ['PATH'], str(JAVA_HOME), str(JAVA)[:-5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d75d8d-d58b-4bb8-87e5-cdf1a8add76a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>1. Connect to Vantage</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>You will be prompted to provide the password. Enter your password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a453af8-1cba-4890-b6a7-3448a1bbc568",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username = 'demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdfde89-b0b9-4210-95c7-a9390569fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql('''SET query_band='DEMO=Energy_Consumption_Forecasting_Python.ipynb;' UPDATE FOR SESSION;''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0be93a-2c49-4708-a17e-3e9e1401c8fc",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Begin running steps with Shift + Enter keys. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20217e-9600-4ef7-adc5-6ba10c76a5cc",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial;color:#E37C4D'><b>Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo on cloud storage. You can either run the demo using foreign tables to access the data without any storage on your environment or download the data to local storage, which may yield faster execution. Still, there could be considerations of available storage. Two statements are in the following cell, and one is commented out. You may switch which mode you choose by changing the comment string.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a490a6-75b8-4d4d-8986-46b03fc57081",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call get_data('DEMO_Energy_cloud');\"        # Takes 1 minute\n",
    "# %run -i ../run_procedure.py \"call get_data('DEMO_Energy_local');\"        # Takes 2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c1780-0cca-4b25-8fc5-cefbf8bde51b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Next is an optional step – if you want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c657c8-2122-4426-9d38-c5a619f7817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-championship",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>2. Data Exploration</b>\n",
    "\n",
    "<table style = 'width:100%;table-layout:fixed;'>\n",
    "<tr>\n",
    "    <td style = 'vertical-align:middle' width = '50%'>\n",
    "        <p style = 'font-size:16px;font-family:Arial'>Users can access large volumes of data by connecting remotely using the teradataml client connection library.  Python methods are translated to SQL and run remotely on the Vantage system.  Only the minimal amount of data required is copied to the client, allowing users to interact with data sets of any size and scale.\n",
    "    </td>\n",
    "    <td><img src = 'images/connect_and_discover.png' width = '400'></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Create a \"Virtual DataFrame\" that points to the data set in Vantage</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(in_schema(\"DEMO_Energy\", \"consumption\"))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d83371-aeca-4efd-b43c-05cb33a2d988",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Let's investigate the data by looking at a data sample.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93212923-1012-401d-9f39-b1d6b99b6b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989392d-db19-4be6-ad75-9df5ce829a5e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The dataset above shows the hourly consumption of energy. Multiple columns are potential factors affecting energy consumption, such as is_dark, is_holiday, etc.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9839945-5088-4b72-9241-5b52c4f547a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe\n",
    "pd_df = df.to_pandas(all_rows = True)\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize = (12, 6))\n",
    "\n",
    "# Create line plot using seaborn\n",
    "sns.set_palette(['#add8e6', '#90ee90', '#00bfff'])\n",
    "sns.lineplot(data = pd_df, x = 'TD_TIMECODE', y = 'consumption')\n",
    "\n",
    "# Add x label\n",
    "plt.xlabel('Date', fontsize = 12)\n",
    "\n",
    "# Add y label\n",
    "plt.ylabel('Energy Units', fontsize = 12)\n",
    "\n",
    "# Add title\n",
    "plt.title('Energy Demand', fontsize = 16)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(labels = ['Energy'], fontsize = 12)\n",
    "\n",
    "# Add grid lines\n",
    "plt.grid(axis = 'y', alpha = 0.5)\n",
    "\n",
    "# Remove spines\n",
    "sns.despine()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66905c-809e-4381-b942-e852e3cfe22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "cols = ['cap_air_temperature', 'cap_cloud_area_fraction', 'cap_precipitation_amount']\n",
    "pd_df[cols] = scaler.fit_transform(pd_df[cols])\n",
    "\n",
    "# Create three subplots\n",
    "fig, axs = plt.subplots(nrows = 3, ncols = 1, figsize = (12, 9))\n",
    "sns.set_palette(['#add8e6', '#90ee90', '#00bfff'])\n",
    "sns.lineplot(x = 'TD_TIMECODE', y = 'cap_air_temperature', data = pd_df, ax = axs[0])\n",
    "sns.lineplot(x = 'TD_TIMECODE', y = 'cap_cloud_area_fraction', data = pd_df, ax = axs[1])\n",
    "sns.lineplot(x = 'TD_TIMECODE', y = 'cap_precipitation_amount', data = pd_df, ax = axs[2])\n",
    "\n",
    "# Set the labels, titles, and other properties for each subplot\n",
    "cols = ['cap_air_temperature', 'cap_cloud_area_fraction', 'cap_precipitation_amount']\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_ylabel('Normalized Values')\n",
    "    ax.set_title(cols[i])\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.grid()\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    fig.autofmt_xdate(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c3ad5-d5ab-4c2e-9c56-51f61c465051",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The graph of cap_air_temperature shows an inverse relationship with energy consumption, meaning that in countries with colder climates like Norway, electricity usage tends to increase as the temperature drops, likely due to increased demand for heating. Conversely, electricity usage tends to decrease when the temperature rises, potentially due to reduced need for heating.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f05c7-dc01-44b8-92c3-f6e988124531",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df['quarter'] = pd_df['TD_TIMECODE'].dt.quarter\n",
    "# create boxplots for selected columns for each quarter\n",
    "sns.boxplot(x = 'quarter', y = 'consumption', data = pd_df, palette = 'pastel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441628d1-87f0-42c3-bb39-0cf0dd885535",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above graph shows the distribution of energy consumption across quarters. It indicates that the 1st and 4th quarters across years witness high energy consumption due to cold weather, while the 3rd quarter witnesses the least energy consumption across years, indicating the summer season.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-spencer",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>3. Data Preparation</b>\n",
    "\n",
    "<table style = 'width:100%;table-layout:fixed;'>\n",
    "<tr>\n",
    "    <td style = 'vertical-align:top' width = '50%'>\n",
    "        <p style = 'font-size:16px;font-family:Arial'>The Vantage Analytic Library is a suite of powerful functions that allows for whole-data-set descriptive analysis, data transformation, hypothesis testing, and algorithmic algorithms at an extreme scale.  As with all Vantage capabilities, these functions run in parallel at the source of the data</p>\n",
    "        <ol style = 'font-size:16px;font-family:Arial'>\n",
    "            <li>Create Feature Transformation objects</li>\n",
    "            <br>\n",
    "            <li>Define the columns to be retained in the analytic data set</li>\n",
    "            <br>\n",
    "            <li>Push the transformations to the data in Vantage</li>\n",
    "            <br>\n",
    "            <li>Inspect the results</li>\n",
    "        </ol>\n",
    "    </td>\n",
    "    <td><img src = 'images/VAL_transformation.png' width = '400'></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_mapping = {1:'monday', 2:'tuesday', 3:'wednesday', 4:'thursday', 5:'friday', 6:'saturday', 7:'sunday'}\n",
    "weekday_t = OneHotEncoder(values = weekday_mapping, columns = 'weekday')\n",
    "\n",
    "hour_t = OneHotEncoder(values = [x for x in range(0,24)],  columns = 'h')\n",
    "\n",
    "rs = MinMaxScalar(columns = ['nasa_temp','cap_air_temperature', 'cap_cloud_area_fraction', 'cap_precipitation_amount'])\n",
    "\n",
    "rt = Retain(columns = ['consumption',\n",
    "                       'is_dark', 'is_light', 'is_from_light_to_dark', 'is_from_dark_to_light', \n",
    "                       'is_holiday', 'is_pre_holiday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26620753-a458-492d-bef2-3cb1c8f75701",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The transformation objects created in the previous step will be used to prepare the data for modeling. Specifically, weekday_t and hour_t will be used to convert weekday and hour columns from numeric to one-hot encoded columns. rs will be used to scale the nasa_temp using MinMaxScalar, and rt will be used to retain the specified columns. These transformations will enable the data to be used effectively in a machine learning model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_output = valib.Transform(data = df,\n",
    "                           one_hot_encode = [weekday_t, hour_t], \n",
    "                           rescale = [rs], \n",
    "                           index_columns = 'TD_TIMECODE',\n",
    "                           retain = [rt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_output.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080268d2-4cc4-4f98-b2fd-1a8a09a0bfc1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Please scroll to the right and observe that we now have columns named <b>monday-sunday</b> and <b>0_h - 23_h</b>. Also, nasa_temp has been scaled.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-instrument",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>4. Model Training</b>\n",
    "\n",
    "<table style = 'width:100%;table-layout:fixed;'>\n",
    "<tr>\n",
    "    <td style = 'vertical-align:top' width = '50%'>\n",
    "        <p style = 'font-size:16px;font-family:Arial'>With Vantage Bring Your Own Model, users can take advantage of a rich ecosystem of Machine Learning, Data Preparation, and Advanced analytical libraries available in the open-source and commercial space.  This demonstration illustrates how to utilize simple client-side training pipelines</p>\n",
    "        <ol style = 'font-size:16px;font-family:Arial'>\n",
    "            <li>Create Train and Test data sets in Vantage</li>\n",
    "            <br>\n",
    "            <li>Copy the training data to the client</li>\n",
    "            <br>\n",
    "            <li>Prepare data and train the model</li>\n",
    "            <br>\n",
    "            <li>Load the model into Vantage</li>\n",
    "        </ol>\n",
    "    </td>\n",
    "    <td><img src = 'images/BYOM_model_training.png' width = '400'></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(t_output.result.iloc[int(t_output.result.shape[0])-168:],\n",
    "            table_name = 'energy_consumption_variables_rescaled_test',\n",
    "            if_exists = 'replace')\n",
    "\n",
    "copy_to_sql(t_output.result.iloc[:int(t_output.result.shape[0])-168],\n",
    "            table_name = 'energy_consumption_variables_rescaled_train',\n",
    "            if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236efbb9-85e3-4700-82b5-50b51db7d0cf",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above step creates training and testing datasets. The last 168 hours, i.e. seven days, are used for testing, and the remaining data is used for training.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT * FROM energy_consumption_variables_rescaled_train order by TD_TIMECODE;', eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-shark",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We calculate the average consumption for the last day of the training period. We will use this number for the normalization of the target variable.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "normalize_value = int(df.tail(24).mean()['consumption'])\n",
    "normalize_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = df.drop(['TD_TIMECODE', 'consumption'], axis = 1).astype(float)\n",
    "train_y = df['consumption'] - normalize_value\n",
    "\n",
    "feature_names = list(train_x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c71e6c-6fc5-4948-9394-fc5035bfdfa3",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Dropping TD_TIMECODE and consumption columns from the training dataset is done because TD_TIMECODE is a timestamp and consumption is the target variable we want to predict, so we can't use it as a feature in our model. Normalizing the target variable by subtracting the normalized_value is done to scale the consumption values and bring them in a similar range as the other features used for prediction. This helps the model to learn the correct relationship between features and the target variable, and makes the training process more efficient.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = PMMLPipeline(steps = [('lr', LinearRegression())])\n",
    "pipeline_rf = PMMLPipeline(steps = [('random_forest', RandomForestRegressor(n_estimators = 100, max_depth = 10, random_state = 42))])\n",
    "\n",
    "pipeline_lr.fit(train_x,train_y)\n",
    "pipeline_rf.fit(train_x,train_y)\n",
    "\n",
    "sklearn2pmml(pipeline_lr, \"energy_consumption_LR.pmml\", with_repr = True)\n",
    "sklearn2pmml(pipeline_rf, \"energy_consumption_RF.pmml\", with_repr = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6805b0b-37d0-4c1b-bfd2-e763b89c94d2",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above step creates two PMML Pipelines, one with a Linear Regression object and another with a Random Forest object. These Pipelines are used to train the models using the \"fit\" method with the preprocessed training dataset. The trained pipelines are then saved as PMML files locally for future use.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f9c1a-969f-4b80-9e46-189c4323f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PMML file into Vantage\n",
    "\n",
    "model_ids = ['lr', 'rf']\n",
    "model_files = ['energy_consumption_LR.pmml', 'energy_consumption_RF.pmml']\n",
    "table_name = 'energy_models'\n",
    "\n",
    "for model_id, model_file in zip(model_ids, model_files):\n",
    "    try:\n",
    "        save_byom(model_id = model_id, model_file = model_file, table_name = table_name)\n",
    "    except Exception as e:\n",
    "        # if our model exists, delete and rewrite\n",
    "        if str(e.args).find('TDML_2200') >= 1:\n",
    "            delete_byom(model_id = model_id, table_name = table_name)\n",
    "            save_byom(model_id = model_id, model_file = model_file, table_name = table_name)\n",
    "        else:\n",
    "            raise ValueError(f\"Unable to save the model '{model_id}' in '{table_name}' due to the following error: {e}\")\n",
    "\n",
    "# Show the bank_models table\n",
    "list_byom(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502f4b01-4a91-4f10-95db-08b2adf16e64",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In the above steps, the trained PMML models are saved in a table named \"energy_models\". If a model with the same model_id already exists, it is deleted first and then the latest trained model is saved again using the save_byom method. This ensures that the most recent version of the model is always stored in the table.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-poetry",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>5. Model Scoring and Evaluation</b>\n",
    "\n",
    "<table style = 'width:100%;table-layout:fixed;'>\n",
    "<tr>\n",
    "    <td style = 'vertical-align:top' width = '50%'>\n",
    "        <p style = 'font-size:16px;font-family:Arial'>The final step in this process is to test the trained model.  The PMMLPredict function will take the stored pipeline object (including any data preparation and mapping tasks) and execute it against the data on the Vantage Nodes.  Note that we can keep many models in the model table, with versioning, last scored timestamp, or any other management data to allow for the operational management of the process.</p>\n",
    "        <ol style = 'font-size:16px;font-family:Arial'>\n",
    "            <li>Create a pointer to the model in Vantage</li>\n",
    "            <br>\n",
    "            <li>Execute the Scoring function using the model against the testing data</li>\n",
    "            <br>\n",
    "            <li>Copy the results of the test to the client - only needs to be a subset of rows if desired</li>\n",
    "            <br>\n",
    "            <li>Visualize the results</li>\n",
    "        </ol>\n",
    "    </td>\n",
    "    <td><img src = 'images/Score_and_Evaluate.png' width = '400'></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd3f5de-48ac-40e8-8815-12066e927438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a pointer to the model\n",
    "table_name = 'energy_models'\n",
    "model_id = 'lr'\n",
    "model_lr = retrieve_byom(model_id, table_name = table_name)\n",
    "df_test = DataFrame('energy_consumption_variables_rescaled_test')\n",
    "\n",
    "result_lr = PMMLPredict(\n",
    "            modeldata = model_lr,\n",
    "            newdata = df_test,\n",
    "            accumulate = ['TD_TIMECODE','consumption'],\n",
    "            ).result.to_pandas(all_rows = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31e0ce-b465-44eb-87ac-42af306d4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4881e-3107-4567-aad6-4daac51b6231",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In the above step, we use the PMMLPredict method from teradataml library to score the model in the database. The PMMLPredict function in Teradata ML-Engine allows users to score the PMML model directly on the data in the Vantage system, without having to move the data or the model outside the system. This can help to improve the efficiency and security of the scoring process.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c73a7-4832-408b-a3fb-844612a06aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a pointer to the model\n",
    "table_name = 'energy_models'\n",
    "model_id = 'rf'\n",
    "model_rf = retrieve_byom(model_id, table_name = table_name)\n",
    "df_test = DataFrame('energy_consumption_variables_rescaled_test')\n",
    "\n",
    "result_rf = PMMLPredict(\n",
    "            modeldata = model_rf,\n",
    "            newdata = df_test,\n",
    "            accumulate = ['TD_TIMECODE','consumption'],\n",
    "            ).result.to_pandas(all_rows = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10c2b9-51f9-467d-be59-4e0bab6128c6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In the above step, we use the PMMLPredict method from teradataml library to score the model in the database. The PMMLPredict function in Teradata ML-Engine allows users to score the PMML model directly on the data in the Vantage system, without having to move the data or the model outside the system. This can help to improve the efficiency and security of the scoring process.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989b9d6-cd74-4e66-a2c6-c08051f4bcc1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>6. Visualize the results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7703ce-fc14-4f5b-af26-4434509ea197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and preprocess\n",
    "df_lr = result_lr\n",
    "df_rf = result_rf\n",
    "\n",
    "# Calculate RMS errors\n",
    "rms_lr = mean_squared_error(result_lr['consumption'], result_lr['prediction'].astype(float) + normalize_value, squared = False)\n",
    "rms_rf = mean_squared_error(result_rf['consumption'], result_rf['prediction'].astype(float) + normalize_value, squared = False)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Add the first plot (linear regression)\n",
    "ax.plot(df_lr.index, df_lr['consumption'], label = f'Actual Consumption', color = 'red', linewidth = 2)\n",
    "ax.plot(df_lr.index, df_lr['prediction'].astype(float) + normalize_value, label = f'Linear Regression (RMS = {rms_lr:.2f})', color = 'blue', linestyle = '--')\n",
    "\n",
    "# Add the second plot (random forest)\n",
    "ax.plot(df_rf.index, df_rf['prediction'].astype(float) + normalize_value, label=f'Random Forest (RMS = {rms_rf:.2f})', color = 'green', linestyle = '--')\n",
    "\n",
    "# Set the axis labels, title, and legend\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Energy Consumption')\n",
    "ax.set_title('Energy Consumption Prediction')\n",
    "ax.legend()\n",
    "\n",
    "# Add gridlines\n",
    "ax.grid(axis = 'y', linestyle = '--')\n",
    "\n",
    "# Add a background color\n",
    "fig.patch.set_facecolor('#f2f2f2')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5e1b8-f01e-490b-a8f9-15f2d0a201b6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above graph displays the Root Mean Squared (RMS) error values for both Linear Regression and Random Forest models. The lower the RMS error value, the better the model's performance. As we can see, Random Forest outperforms Linear Regression in predicting energy demand, as it has a lower RMS error value. Therefore, Random Forest is more suitable for proactively predicting energy demand in our use case.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This demonstration has illustrated a simplified - but complete - overview of how a typical machine learning workflow can be improved using Vantage in conjunction with open-source tools and techniques.  This combination allows users to leverage open-source innovation with Vantage's operational scale, power, and stability.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e9e44-3119-4852-8574-9303a40fcd52",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>7. Cleanup</b>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'><b>Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Cleanup work tables to prevent errors next time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05be8b4-29f3-45c8-9f25-d9ea950c17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['energy_models', 'energy_consumption_variables_rescaled_train', 'energy_consumption_variables_rescaled_test']\n",
    "\n",
    "# Loop through the list of tables and execute the drop table command for each table\n",
    "for table in tables:\n",
    "    try:\n",
    "        db_drop_table(table_name = table)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d377fef9-b377-4b59-a313-5eb9784dba93",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#E37C4D'> <b>Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472c2ca-65d0-4116-9c45-6411fcea52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_Energy');\"        # Takes 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b0b73e-0c50-4110-be9c-05aaf1181960",
   "metadata": {},
   "source": [
    "<footer style=\"padding:10px;background:#f9f9f9;border-bottom:3px solid #394851\">©2023 Teradata. All Rights Reserved</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
