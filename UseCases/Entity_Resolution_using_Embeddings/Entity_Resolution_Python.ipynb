{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e16fe2-293b-4806-948c-56ea0b921a58",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Entity Resolution with In-Database Embeddings and Analytics\n",
    " <br>       \n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 150px; height: auto; margin-top: 20pt;\">\n",
    "  <br>\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b46448-493d-4e52-a031-24c1c7660475",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Introduction:</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "In most organizations, data originates from multiple systems — CRM, ERP, marketing platforms, and external sources — each storing information in different formats. This leads to duplicates, inconsistencies, and incomplete records across entities such as customers, suppliers, products, or locations.<br>\n",
    "    <b>Entity Resolution (ER)</b>, also known as <b>Record Linking</b>, is the process of identifying and merging records that refer to the same real-world entity. It forms the foundation of Master Data Management (MDM), enabling organizations to achieve trusted, unified, and analytics-ready data.<br>\n",
    "However, traditional ER approaches based on deterministic or rule-based matching often struggle with accuracy, scalability, and multi-lingual data, leading to high false positives and negatives. As data volumes and complexity grow, organizations need a smarter, faster, and more adaptive approach.<br>\n",
    "<br>    <b>Solution Overview</b><br>    Using <b>Teradata ClearScape Analytics</b>, organizations can modernize Entity Resolution with a combination of string similarity metrics, vector embeddings, and machine learning techniques — all executed directly within the database.<br>\n",
    "The approach enhances conventional text matching (e.g., Levenshtein, Jaro-Winkler) by incorporating semantic embeddings derived from pre-trained language models. These embeddings capture contextual meaning, allowing the system to identify similar entities even when data includes abbreviations, spelling errors, or multilingual variations.<br>\n",
    "A machine learning model (trained using Teradata Vantage, H2O, or Scikit-Learn) uses both similarity scores and embedding distances as input features to classify potential matches. Once deployed in-database, the model performs high-speed matching across millions of records — improving accuracy by 5–15% compared to traditional methods, while maintaining deterministic, repeatable results.\n",
    "    <ul style = 'font-size:16px;font-family:Arial'>Typical applications include:\n",
    "        <li>Creating unified customer, vendor, or product master records</li>\n",
    "        <li>Linking similar entities across multiple internal and external data sources</li>\n",
    "        <li>Standardizing reference data for reporting, analytics, and AI initiatives</li>\n",
    "        <li>Reducing redundancy and improving overall data governance</li>\n",
    "        </ul>\n",
    "    <p style = 'font-size:16px;font-family:Arial'><b>Why Teradata Vantage</b>\n",
    "Teradata Vantage provides a scalable, enterprise-grade environment for advanced Entity Resolution with key advantages:\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li><b>In-Database Analytics:</b> Run feature engineering, similarity computation, and ML inference directly in Vantage — no data movement required.</li>\n",
    "    <li><b>Bring Your Own Model (BYOM):</b> Seamlessly import and operationalize models trained in external frameworks like H2O or Scikit-Learn.</li>\n",
    "    <li><b>Rich Analytical Ecosystem:</b> Native support for text similarity, NLP-based vector embeddings, and machine learning functions.</li>\n",
    "    <li><b>Parallel CPU Processing:</b> Delivers high performance without the need for GPUs, even on large-scale datasets.</li>\n",
    "    <li><b>End-to-End Governance:</b> Integrated security, scalability, and audit capabilities ensure trusted data management at enterprise scale.   </li>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2067a-72ed-4ae2-9d59-1d2cca696f3e",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial;'>1. Initiate a connection to Vantage</b>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Let'start by importing required libraries and making connection to Vantage database. You will be prompted to provide the password. Enter your password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a3e60a-bc42-495e-81a8-b6ae42a48a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from teradataml import *\n",
    "import getpass\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f801308-ffc0-40c9-880a-8f1c0c238edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea50bc2-d143-4ec0-83cb-3851f1bb154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql('''SET query_band='DEMO=Entity_Resolution_Python.ipynb;' UPDATE FOR SESSION; ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a85a9-963a-4016-a7ad-6cb88570843d",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Begin running steps with Shift + Enter keys. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b43080d-1b8e-4e64-8d56-d149869b83b5",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;'><b>Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>We have provided data for this demo on cloud storage. You can either run the demo using foreign tables to access the data without any storage on your environment or download the data to local storage, which may yield faster execution. Still, there could be considerations of available storage. Two statements are in the following cell, and one is commented out. You may switch which mode you choose by changing the comment string.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7c5c9-179c-4016-b3d6-361953583f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call get_data('DEMO_Entity_Resolution_local');\" # Takes 2 minutes\n",
    "#%run -i ../run_procedure.py \"call get_data('DEMO_Entity_Resolution_cloud');\" # Takes 1 minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c39348-7120-4933-8428-52e692e248e4",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>2. Entity Datasets</b>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Let's look at the two entity datasets that we have and how the entities in these tables match with each other.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e6a18-7bb8-405f-8f0c-ac0cd0ad3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abt = DataFrame(in_schema('DEMO_Entity_Resolution', 'Item_About'))\n",
    "df_abt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c493be-d023-4236-8d3d-d3a49849d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buy = DataFrame(in_schema('DEMO_Entity_Resolution', 'Item_Buy'))\n",
    "df_buy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d71e13-ee40-4f26-a86f-0bb783b32f46",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>We also have a matching table for reference which let us know which id from the Item_About table corresponds with the Item_Buy table. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb759f6-ffe8-4353-b9e5-10c79e2a2c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_match = DataFrame(in_schema('DEMO_Entity_Resolution', 'Item_Abt_Buy_Match'))\n",
    "df_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be1f5d-a453-4b47-833b-0d5ad93e5696",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = DataFrame.from_query('''select a.idAbt, a.name as name_Abt, b.idBuy, b.name as name_Buy\n",
    "from DEMO_Entity_Resolution.Item_Abt_Buy_Match m\n",
    "inner join DEMO_Entity_Resolution.Item_About a\n",
    "on m.idAbt = a.idAbt\n",
    "inner join DEMO_Entity_Resolution.Item_Buy b\n",
    "on m.idBuy = b.idBuy''')\n",
    "df_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61376a56-ac0f-4ee2-9ca3-f5de9f1e70ad",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>From the above results we can see which ids in Item_About table are corresponding to Item_Buy table. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb08f18-f8ea-4197-863a-8fa9bcd3063b",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>3. Create Embeddings for the Datasets</b>\n",
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;'><b>3.1 Load HuggingFace Model</b>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>To generate embeddings, we need an ONNX model capable of transforming text into vector representations. We use a pretrained model from [Teradata's Hugging Face repository](https://huggingface.co/Teradata/bge-small-en-v1.5), such as <b>bge-small-en-v1.5</b>. The model and its tokenizer are downloaded and stored in Vantage tables as BLOBs using the save_byom function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bafb90-a4c0-4945-b6bf-2c2c1472935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5c3d3-48c9-4dcd-abbb-70588bf1510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name = \"bge-small-en-v1.5\"\n",
    "number_dimensions_output = 384\n",
    "model_file_name = \"model.onnx\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f6261-80e2-4b0c-9363-ecbc0fe9d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Model from Teradata HuggingFace Page\n",
    "\n",
    "hf_hub_download(repo_id=f\"Teradata/{model_name}\", filename=f\"onnx/{model_file_name}\", local_dir=\"./\")\n",
    "hf_hub_download(repo_id=f\"Teradata/{model_name}\", filename=f\"tokenizer.json\", local_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96f707-23c3-4774-abf6-b4bc4cb51091",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db_drop_table(\"embeddings_models\")\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    db_drop_table(\"embeddings_tokenizers\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8c569-ae20-4304-b625-4b760329ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models into Vantage\n",
    "# a) Embedding model\n",
    "save_byom(model_id = model_name, # must be unique in the models table\n",
    "               model_file = f\"onnx/{model_file_name}\",\n",
    "               table_name = 'embeddings_models' )\n",
    "# b) Tokenizer\n",
    "save_byom(model_id = model_name, # must be unique in the models table\n",
    "              model_file = 'tokenizer.json',\n",
    "              table_name = 'embeddings_tokenizers') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2581ab8-efc7-45e3-b821-7bc40426ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = DataFrame('embeddings_models')\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23856e27-f66e-4f8d-83ae-38dce26f2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token = DataFrame('embeddings_tokenizers')\n",
    "df_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e1ac6-fdd6-4e30-a3ff-12073a0a2b68",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<b style = 'font-size:18px;font-family:Arial;'>3.2 Generate Embeddings with ONNXEmbeddings</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33739ba-61c3-44ca-8766-af4defbdf1ad",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "Now it's time to generate the embeddings using <b>ONNXEmbeddings</b>.<br>We run the ONNXEmbeddings function to generate embeddings for a small subset of records. The model is <b>loaded into the cache memory on each node</b>, and Teradata's <b>Massively Parallel Processing (MPP)</b> architecture ensures that embeddings are computed in parallel using <b>ONNX Runtime</b> on each node.  <br>Having said that, generating embeddings for the entire training set can be time-consuming, especially when working on a system with limited resources. In the <b>ClearScape Analytics experience</b>, only a <b>4 AMP system</b> with constrained RAM and CPU power is available. To ensure smooth execution, we test embedding generation on a small sample and use <b>pre-calculated embeddings</b> for the remainder of demo. In a real-life scenario you would tyipically encounter multiple hundred AMPs with much more compute power!<br>Also have a look at the most important input parameters of this <b>ONNXEmbeddings</b> function.\n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "<li><b>InputTable</b>: The source table containing the text to be embedded. </li>\n",
    "<li><b>ModelTable</b>: The table storing the ONNX model.                    </li>\n",
    "<li><b>TokenizerTable</b>: The table storing the tokenizer JSON file.       </li>\n",
    "<li><b>Accumulate</b>: Specifies additional columns to retain in the output </li>  \n",
    "<li><b>OutputFormat</b>: Specifies the data format of the output embeddings (<b>FLOAT32(384)</b>, matching the model's output dimension).</li>\n",
    "</ul>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "Since embedding generation is computationally expensive, we only process <b>10 records for testing</b> and rely on precomputed embeddings for further analysis.  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364c35a-845c-4fec-a147-f9da6fcfbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure.byom_install_location = \"mldb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff678ca5-f66d-4022-84b4-a4cf9e34970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_sample10 = DataFrame.from_query(\"SELECT t.idAbt, t.name as txt FROM DEMO_Entity_Resolution.Item_About t sample 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bd0c9-d1c4-4497-8dee-fe78455285ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = DataFrame.from_query(f\"select * from embeddings_models where model_id = '{model_name}'\")\n",
    "my_tokenizer = DataFrame.from_query(f\"select model as tokenizer from embeddings_tokenizers where model_id = '{model_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b7058f-43ea-4f98-bb9c-fb397bbc6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_embeddings_sample = ONNXEmbeddings(\n",
    "    newdata = DF_sample10,\n",
    "    modeldata = my_model, \n",
    "    tokenizerdata = my_tokenizer, \n",
    "    accumulate = [\"idAbt\"],\n",
    "    model_output_tensor = \"sentence_embedding\",\n",
    "    output_format = f'FLOAT32({number_dimensions_output})',\n",
    "    enable_memory_check = False\n",
    ").result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed7cad-172f-4f45-859a-08b05441262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_embeddings_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c3ebf-732c-4d0e-845a-dd45426a5a79",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'> Here we can see how the embeddings are generated for the name text. We have generated embeddings for both Item_About and Item_Buy table.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4326a0-64e9-418f-a153-0f98d2fdff73",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>4. Feature Engineering</b>\n",
    "<p style = 'font-size:16px;font-family:Arial;'> We will create additional features. First we will create features using TopN Match rows (positive examples) and Non-Match rows (negative) by embedding distance Bi-Encoder (VectorDistance) results.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd9b18a-853f-4a64-8869-489a75ed099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "create volatile table entity_match_results_temp\n",
    "as (\n",
    "SELECT \n",
    "    dt.target_id as idAbt,\n",
    "    dt.reference_id as idBuy,\n",
    "    x.idBuy ground_truth_buy_id,\n",
    "    case when x.idBuy = dt.reference_id then 1 else 0 end as match,\n",
    "    e_tgt.name as target_txt,\n",
    "    e_ref.name as reference_txt,\n",
    "    sum(case when dt.distancetype = 'cosine' then 1.0 - dt.distance else 0 end) as emb_cosine,\n",
    "    sum(case when dt.distancetype = 'euclidean' then dt.distance else 0 end) as emb_euclidean,\n",
    "    sum(case when dt.distancetype = 'manhattan' then dt.distance else 0 end) as emb_manhattan\n",
    "FROM\n",
    "    TD_VECTORDISTANCE (\n",
    "        ON DEMO_Entity_Resolution.Item_About_Embeddings  AS TargetTable\n",
    "        ON DEMO_Entity_Resolution.Item_Buy_embeddings AS ReferenceTable DIMENSION\n",
    "        USING\n",
    "            TargetIDColumn('idAbt')\n",
    "            TargetFeatureColumns('[a_emb_0:a_emb_383]')\n",
    "            RefIDColumn('idBuy')\n",
    "            RefFeatureColumns('[b_emb_0:b_emb_383]')\n",
    "            DistanceMeasure('cosine', 'euclidean','manhattan')\n",
    "            topk(100) -- max allowed\n",
    "    ) AS dt\n",
    "JOIN DEMO_Entity_Resolution.Item_About e_tgt on e_tgt.idAbt = dt.target_id\n",
    "JOIN DEMO_Entity_Resolution.Item_Buy e_ref on e_ref.idBuy = dt.reference_id\n",
    "JOIN DEMO_Entity_Resolution.Item_Abt_Buy_Match x on x.idAbt = e_tgt.idAbt\n",
    "GROUP BY 1,2,3,4,5,6\n",
    ") with data on commit preserve rows\n",
    "\"\"\"\n",
    "\n",
    "execute_sql(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5c6b0-82ad-4f4d-a8c7-46ab4d3b78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = DataFrame(\"entity_match_results_temp\")\n",
    "tdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382ed37-f7c7-4685-9a7d-40ed70e42949",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Now we will create ngrams which we will subsequently use in creating freatures from StringSimilarity.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8252130-4a06-4c44-b1af-52c68756f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db_drop_table(\"entity_seq\")\n",
    "except:\n",
    "    True\n",
    "\n",
    "for i in range(2,8):\n",
    "    for x in ['About','Buy']:\n",
    "        try:\n",
    "            db_drop_table(\"{}_QGrams{}\".format(x,i))\n",
    "        except:\n",
    "            True\n",
    "        \n",
    "        try:\n",
    "            db_drop_table(\"{}_QGrams{}\".format(x,i))\n",
    "        except:\n",
    "            True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189417d3-4855-43da-bc6f-6b194d22c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_sql(\"\"\"\n",
    "    CREATE VOLATILE TABLE entity_seq AS(\n",
    "        SELECT (calendar_date - DATE '1900-01-01' + 1) AS n\n",
    "        FROM sys_calendar.calendar\n",
    "        WHERE calendar_date BETWEEN DATE '1900-01-01' AND DATE '1900-01-01' + 199\n",
    "    ) WITH DATA PRIMARY INDEX(n) ON COMMIT PRESERVE ROWS;\n",
    "\"\"\")\n",
    "\n",
    "for i in range(2,8):\n",
    "    for x in ['About']:\n",
    "        execute_sql(\"\"\"\n",
    "            CREATE VOLATILE TABLE {}_QGrams{} AS (\n",
    "                SELECT id as id, TRIM(TRAILING FROM XMLAGG(TRIM(QGrams) || ' ' ORDER BY n) (VARCHAR(10000))) AS QGrams\n",
    "                    FROM \n",
    "                        ( SELECT idAbt as id, n, SUBSTRING(name FROM n FOR {}) AS QGrams\n",
    "                            FROM DEMO_Entity_Resolution.item_{}, entity_seq\n",
    "                            WHERE n <= CHAR_LENGTH(name) - {} + 1\n",
    "                        ) as x\n",
    "                GROUP BY id\n",
    "            ) WITH DATA PRIMARY INDEX (id) ON COMMIT PRESERVE ROWS\n",
    "        \"\"\".format(x,i,i,x,i))\n",
    "\n",
    "for i in range(2,8):\n",
    "    for x in ['Buy']:\n",
    "        execute_sql(\"\"\"\n",
    "            CREATE VOLATILE TABLE {}_QGrams{} AS (\n",
    "                SELECT id, TRIM(TRAILING FROM XMLAGG(TRIM(QGrams) || ' ' ORDER BY n) (VARCHAR(10000))) AS QGrams\n",
    "                    FROM \n",
    "                        ( SELECT idBuy as id , n, SUBSTRING(name FROM n FOR {}) AS QGrams\n",
    "                            FROM DEMO_Entity_Resolution.item_{}, entity_seq\n",
    "                            WHERE n <= CHAR_LENGTH(name) - {} + 1\n",
    "                        ) as x\n",
    "                GROUP BY id\n",
    "            ) WITH DATA PRIMARY INDEX (id) ON COMMIT PRESERVE ROWS\n",
    "        \"\"\".format(x,i,i,x,i))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ce72d-edaf-467d-b66b-5f6dacbba550",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'> Lets take a look how these ngrams look.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb72693-ec75-465d-9c85-fb1cbd8cdef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf2= DataFrame(\"About_Qgrams2\")\n",
    "tdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd92e587-bf79-4113-b445-166678b8e026",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'> Now we will calculate the distance between the ngrams e.g Jaro Winkler etc to create additional features.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6486e047-cdd3-4f30-b112-35fd3d5ca159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query takes about 5min to execute\n",
    "execute_sql(\"\"\"\n",
    "create table entity_match_results\n",
    "as\n",
    "(\n",
    "    SELECT * FROM StringSimilarity (\n",
    "      ON (select\n",
    "              e.idAbt,\n",
    "              e.idBuy,\n",
    "              e.match,\n",
    "              e.emb_cosine,\n",
    "              e.emb_euclidean,\n",
    "              e.emb_manhattan,\n",
    "              cast(e.target_txt as varchar(200)) as target_txt,\n",
    "              cast(e.reference_txt as varchar(200)) as reference_txt,\n",
    "              cast(a2.QGrams as varchar(200)) as a2_txt, cast(b2.QGrams as varchar(200)) as b2_txt,\n",
    "              cast(a3.QGrams as varchar(200)) as a3_txt, cast(b3.QGrams as varchar(200)) as b3_txt,\n",
    "              cast(a4.QGrams as varchar(200)) as a4_txt, cast(b4.QGrams as varchar(200)) as b4_txt,\n",
    "              cast(a5.QGrams as varchar(200)) as a5_txt, cast(b5.QGrams as varchar(200)) as b5_txt,\n",
    "              cast(a6.QGrams as varchar(200)) as a6_txt, cast(b6.QGrams as varchar(200)) as b6_txt,\n",
    "              cast(a7.QGrams as varchar(200)) as a7_txt, cast(b7.QGrams as varchar(200)) as b7_txt\n",
    "          from \n",
    "             entity_match_results_temp e,\n",
    "             About_Qgrams2 a2, Buy_Qgrams2 b2,\n",
    "             About_Qgrams3 a3, Buy_Qgrams3 b3,\n",
    "             About_Qgrams4 a4, Buy_Qgrams4 b4,\n",
    "             About_Qgrams5 a5, Buy_Qgrams5 b5,\n",
    "             About_Qgrams6 a6, Buy_Qgrams6 b6,\n",
    "             About_Qgrams5 a7, Buy_Qgrams5 b7\n",
    "          where \n",
    "             e.idAbt = a2.id and e.idBuy = b2.id and\n",
    "             e.idAbt = a3.id and e.idBuy = b3.id and\n",
    "             e.idAbt = a4.id and e.idBuy = b4.id and\n",
    "             e.idAbt = a5.id and e.idBuy = b5.id and\n",
    "             e.idAbt = a6.id and e.idBuy = b6.id and\n",
    "             e.idAbt = a7.id and e.idBuy = b7.id\n",
    "            ) PARTITION BY ANY\n",
    "      USING\n",
    "      ComparisonColumnPairs ('jaro (target_txt, reference_txt) AS jaro',\n",
    "                             'jaro_winkler (target_txt, reference_txt) as jaro_winkler',\n",
    "                             'n_gram (target_txt, reference_txt, 1) AS ngram1',\n",
    "                             'n_gram (target_txt, reference_txt, 2) AS ngram2',\n",
    "                             'n_gram (target_txt, reference_txt, 3) AS ngram3',\n",
    "                             'n_gram (target_txt, reference_txt, 4) AS ngram4',\n",
    "                             'LD (target_txt, reference_txt) AS ld',\n",
    "                             'LDWS (target_txt, reference_txt) AS ldws',\n",
    "                             'OSA (target_txt, reference_txt) AS osa',\n",
    "                             'DL (target_txt, reference_txt) AS dl',\n",
    "                             'hamming (target_txt, reference_txt) AS hamming',\n",
    "                             'LCS (target_txt, reference_txt) AS lcs',\n",
    "                             'jaccard (target_txt, reference_txt) AS jaccard',\n",
    "                             'cosine (target_txt, reference_txt) AS term_cosine',\n",
    "                             'n_gram (a2_txt, b2_txt,1) as qgrams2_sim',\n",
    "                             'n_gram (a3_txt, b3_txt,1) as qgrams3_sim',\n",
    "                             'n_gram (a4_txt, b4_txt,1) as qgrams4_sim',\n",
    "                             'n_gram (a5_txt, b5_txt,1) as qgrams5_sim',\n",
    "                             'n_gram (a6_txt, b6_txt,1) as qgrams6_sim',\n",
    "                             'n_gram (a7_txt, b7_txt,1) as qgrams7_sim',\n",
    "                             'soundexcode (target_txt, reference_txt) AS soundexcode'\n",
    "      )\n",
    "      CaseSensitive ('false')\n",
    "      Accumulate ('idAbt', 'idBuy','emb_cosine','emb_euclidean','emb_manhattan','match','target_txt', 'reference_txt')\n",
    "    ) AS dt \n",
    ") with data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d06fbc-5571-432e-9768-52f70260f4bd",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Now we combine all the eambeddings and additional features we have calculated to create the final entity dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfe446-4073-417b-b08c-408a0fe88d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "create multiset table Entities_Final\n",
    "as\n",
    "(\n",
    "  select \n",
    "       em.match,\n",
    "       em.emb_cosine,\n",
    "       em.emb_euclidean,\n",
    "       em.emb_manhattan,\n",
    "       em.jaro,\n",
    "       em.jaro_winkler,\n",
    "       em.ngram1,\n",
    "       em.ngram2,\n",
    "       em.ngram3,\n",
    "       em.ngram4,\n",
    "       em.ld,\n",
    "       em.ldws,\n",
    "       em.osa,\n",
    "       em.dl,\n",
    "       em.hamming,\n",
    "       em.lcs,\n",
    "       em.jaccard,\n",
    "       em.term_cosine,\n",
    "       em.qgrams2_sim,\n",
    "       em.qgrams3_sim,\n",
    "       em.qgrams4_sim,\n",
    "       em.qgrams5_sim,\n",
    "       em.qgrams6_sim,\n",
    "       em.qgrams7_sim,\n",
    "       em.soundexcode,\n",
    "       abt.*,\n",
    "       buy.*\n",
    "  from \n",
    "       entity_match_results em,\n",
    "       DEMO_Entity_Resolution.Item_About_Embeddings abt,\n",
    "       DEMO_Entity_Resolution.Item_Buy_Embeddings buy\n",
    "  where\n",
    "       em.idAbt = abt.idAbt and\n",
    "       em.idBuy = buy.idBuy \n",
    ") with data\n",
    "\"\"\"\n",
    "\n",
    "execute_sql(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b022f-09f0-4c5a-9365-5511f9f68a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame('Entities_Final')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e055959-7b29-4b53-b3ec-88289b3ac18a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><b>Create train and test data</b><p style = 'font-size:16px;font-family:Arial'>Now we have transformed our data and it is fit to be used in machine learning models, let us split the whole dataset into train and test sets for model training and scoring. We will use <b>TrainTestSplit</b> function for this task.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce696a0f-976a-4739-ad6a-2ca97b53e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTestSplit_out = TrainTestSplit(\n",
    "                                    data = data,\n",
    "                                    id_column = \"idAbt\",\n",
    "                                    train_size = 0.80,\n",
    "                                    test_size = 0.20,\n",
    "                                    seed = 21,\n",
    "                                    stratify_column = \"match\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15baebb3-9fd6-47bb-a04e-65b0bfd7b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 2 virtual dataframes\n",
    "df_train = TrainTestSplit_out.result[TrainTestSplit_out.result['TD_IsTrainRow'] == 1].drop(['TD_IsTrainRow'], axis = 1)\n",
    "df_test = TrainTestSplit_out.result[TrainTestSplit_out.result['TD_IsTrainRow'] == 0].drop(['TD_IsTrainRow'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4efbc8b-893b-42e6-ab2f-e304c1563a70",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Save the training and test datasets</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46302375-a32c-4abc-93bf-729b4ff89e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(df_train,\n",
    "             table_name=\"Entities_Train_Final\",\n",
    "             if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ceaf45-5f58-4d2a-bf85-e8a3bc2a51d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(df_test,\n",
    "             table_name=\"Entities_Test_Final\",\n",
    "             if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e428302c-db2e-45c5-9732-a7c639ad63b6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Let us check the positive and negative matches in each of the train and test sets.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f7270-3e0e-4b4f-a398-3cd3577d35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Match = 1\",df_train[df_train['match'] == 1].shape)\n",
    "print(\"Match = 0\",df_train[df_train['match'] == 0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d15f8-3ac2-42c8-9441-98e6bdb8ac10",
   "metadata": {},
   "source": [
    "Match = 1 (891, 797)\n",
    "Match = 0 (93781, 797)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d828a0-b9f2-4105-b5e5-14f15835cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Match = 1\",df_test[df_test['match'] == 1].shape)\n",
    "print(\"Match = 0\",df_test[df_test['match'] == 0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed5056-6cff-42e4-a7a6-cabb503dc48f",
   "metadata": {},
   "source": [
    "Match = 1 (214, 797)\n",
    "Match = 0 (23231, 797)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7acebba-fea9-4b0d-afdb-bed7b24d920b",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial;'>5. Create and load H20 Classification Model</b>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e09213-8d7a-414a-a6af-01eb363dfebf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i><b>Note:</b>The H20 model creation is explained in a separate notebook named <b>\"Entity_Resolution_Classification_Model_Training.ipynb\"</b>.<br> Due to the size of this JupyterLab environment and the amount of data we are processing, the creation of this model will not complete and may result in this JupyterLab becoming non-responsive. This notebook will use a pre-trained model.</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abfeed8-a409-406b-bafa-437494893338",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.path.abspath(os.getcwd())\n",
    "model_path = os.path.join(current_path, \"artifacts/XGBoost_1_AutoML_1_pretrained.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe10646-df40-49eb-b243-b08022271bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f9571-93f5-4c54-a7e1-fb4c1ee2eff3",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>If you have created your own model, please update the model_path accordingly.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc1b2f-3fcc-49f1-9abc-a7f5b4a1bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db_drop_table(\"h2o_models\")\n",
    "except:\n",
    "    True\n",
    "\n",
    "save_byom(model_id=\"automl_model\", model_file=model_path, table_name=\"h2o_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aee300-2354-4425-b061-126310d211c6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'><b>BYOM Vantage Scoring on Test Data</b><br>\n",
    "    Let us take a look at our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe175bd-54f1-429f-8f2c-40fbcb5a5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db_drop_table(\"entity_scoring_table\")\n",
    "except:\n",
    "    True\n",
    "  \n",
    "\n",
    "execute_sql(\"\"\"\n",
    "create table entity_scoring_table\n",
    "as (\n",
    "  select ROW_NUMBER() OVER (ORDER BY x.idAbt, x.idBuy) AS row_id,\n",
    "         x.*\n",
    "  from       \n",
    "       Entities_Test_Final x\n",
    ") with data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee9c36c-9e77-4c57-8e15-d03680c9e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scoring_input = DataFrame(\"entity_scoring_table\")\n",
    "df_scoring_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e4cbed-441d-44b4-9016-f0d384c7d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scoring_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ebf819-e450-4c74-8643-5644b8abd993",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldata = retrieve_byom(\"automl_model\", table_name=\"h2o_models\")\n",
    "\n",
    "configure.byom_install_location = \"MLDB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719161b-5701-4e3b-8267-992d9b79af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking only 1000 records to save time, running on whole test dataset takes approx 30min\n",
    "df_scoring =  df_scoring_input[df_scoring_input['row_id'] <= 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795eea37-a4aa-4bc3-9091-c83be5f8557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = H2OPredict(newdata=df_scoring,\n",
    "                    newdata_partition_column='row_id',\n",
    "                    newdata_order_column='row_id',\n",
    "                    modeldata=modeldata,\n",
    "                    modeldata_order_column='model_id',\n",
    "                    model_output_fields=['classProbabilities'],\n",
    "                    #model_output_fields=['prob_0','prob_1'],\n",
    "                    accumulate=['row_id','idAbt','idBuy'],\n",
    "                    overwrite_cached_models='*',\n",
    "                    enable_options=['contributions','stageProbabilities'],\n",
    "                    model_type='OpenSource'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776efca5-de5e-4b6b-9f6f-19ec4555b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = result.result\n",
    "df_predict.to_sql(table_name='entity_classification_predictions_temp', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c6a98-1522-469c-801a-2b5b7ec49ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db_drop_table(\"entity_classification_predictions\")\n",
    "except:\n",
    "    True\n",
    "\n",
    "execute_sql(\"\"\"\n",
    "create table entity_classification_predictions\n",
    "as(\n",
    "WITH ranked_rows AS(\n",
    "   select \n",
    "      row_id,\n",
    "      idAbt,\n",
    "      idBuy,\n",
    "      CAST(NEW JSON(classprobabilities).JSONExtractValue('$.0') as FLOAT) AS p0,\n",
    "      CAST(NEW JSON(classprobabilities).JSONExtractValue('$.1') as FLOAT) AS p1,\n",
    "      ROW_NUMBER() OVER (PARTITION BY idAbt ORDER BY CAST(NEW JSON(classprobabilities).JSONExtractValue('$.1') as FLOAT) desc) as rnk\n",
    "   from\n",
    "      entity_classification_predictions_temp\n",
    ") \n",
    "SELECT * FROM ranked_rows where rnk = 1\n",
    ") with data\n",
    "\"\"\"); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb57d83d-5bcc-4d54-9b24-3eb8aada6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db_drop_table(\"entity_classification_similarity\")\n",
    "except:\n",
    "    True\n",
    "  \n",
    "\n",
    "execute_sql(\"\"\"\n",
    "create table entity_classification_similarity\n",
    "as (\n",
    "  select \n",
    "         x.idAbt,\n",
    "         x.idBuy,\n",
    "         y.p1, \n",
    "         -- Cut off Threshold 0.05058710706080187\n",
    "         CASE WHEN y.p1 >= 0.05058710706080187 THEN 1 else 0 END as prediction,\n",
    "         x.match,\n",
    "         x.target_txt,\n",
    "         x.reference_txt\n",
    "  from\n",
    "       entity_match_results x \n",
    "       inner join\n",
    "       entity_classification_predictions y \n",
    "           on (x.idAbt = y.idAbt and x.idBuy = y.idBuy)\n",
    ") with data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c44ed73-e481-4213-a736-ce263e25918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = DataFrame(\"entity_classification_similarity\")\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ac38b-bfce-40f8-bcf0-3172498458f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "final_df = final_df.to_pandas()\n",
    "final_df['match'] = final_df['match'].astype('str')\n",
    "final_df['prediction'] = final_df['prediction'].astype('str')\n",
    "\n",
    "actual = final_df['match']\n",
    "predicted = final_df['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489fa6d-d4f7-43c8-b250-8a626c1a5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5f685-f56d-445a-8260-82c5a47746f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm, index=['Actual Negative', 'Actual Positive'], columns=['Predicted Negative', 'Predicted Positive'])\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c93454-fb94-4983-b07e-d3abd74f0bbb",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>Conclusion</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Entity Resolution powered by <b>Teradata ClearScape Analytics</b> enables organizations to unify and standardize their data with exceptional accuracy, scalability, and efficiency. By combining advanced string similarity techniques, semantic vector embeddings, and in-database machine learning, it effectively reduces duplicates, enhances data quality, and improves trust across multiple systems. This integrated approach builds a strong foundation for analytics, AI, and data governance initiatives, while accelerating insights and reducing manual reconciliation efforts.<br><b>Key Takeaways</b><ul  style = 'font-size:16px;font-family:Arial;'>  \n",
    "    <li><b>TD_VectorDistance</b> bi-encoder functions (cosine, Euclidean, and Manhattan similarity) enable fast selection of non-match examples for training.</li>\n",
    "    <li>Maintaining a <b>ground truth “Match Table” </b>s essential; even LLMs can assist in generating this labeled data.</li>\n",
    "    <li><b>Embeddings generated via BYOM in Vantage </b>combined with ClearScape StringSimilarity functions, provide high-speed, scalable matching.</li>\n",
    "    <li>A <b>cross-encoder binary classification model  </b>helps evaluate performance through measurable metrics on holdout sets.</li>\n",
    "    <li>Models can be <b> deployed and inferenced directly in Vantage</b>or exposed through <b>ModelOps endpoints</b> for real-time scoring.</li>\n",
    "    <li>While this example demonstrates <b>H2O AutoML </b> it can easily be adapted for<b> Scikit-learn, Vantage AutoML, or ClearScape Vantage GLM training </b>— allowing flexibility in production environments.</li>\n",
    "    <li>Achieved a <b>5–15% accuracy improvement </b>over legacy methods.</li>\n",
    "    <li><b>Multilingual vector embeddings </b>enable matching across languages, enhancing global applicability.</li>\n",
    "    <li>Using a<b>Classical AI/ML model head </b>ensures high-speed inference, explainability, and flexibility to experiment with newer embedding models like ModernBERT for continuous improvement.</li>\n",
    "</ul>\n",
    "<p  style = 'font-size:16px;font-family:Arial;'> Together these capabilities position Teradata ClearScape Analytics as a comprehensive, production-ready platform for accurate, explainable, and scalable Entity Resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14bff0-2928-4cd9-a6a0-23c06bcf0f32",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial;'>6. Cleanup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f408b9-6d0e-4071-9569-8ff2608b7393",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;'><b>Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>Cleanup work tables to prevent errors next time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b31e3-9249-4460-bc59-8454db21cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['embeddings_models','embeddings_tokenizers','entity_seq','entity_match_results','Entities_Final','Entities_Train_Final','Entities_Test_Final','h2o_models','entity_scoring_table','entity_classification_predictions_temp','entity_classification_predictions','entity_classification_similarity']\n",
    "\n",
    "# Loop through the list of tables and execute the drop table command for each table\n",
    "for table in tables:\n",
    "    try:\n",
    "        db_drop_table(table_name=table)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeaaef8-4e83-4ffd-97fb-95b64b7206e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,8):\n",
    "    for x in ['About', 'Buy']:\n",
    "        try:\n",
    "            db_drop_table(table_name=f\"{x}_QGrams{i}\")\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8790f4-51a9-4eb9-b6a3-41fdf5d5e594",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;'> <b>Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bc259-30c2-43f0-8fb1-b37bd0abeb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_Entity_Resolution');\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b905efe-8416-41d3-a595-dfbd40ca2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a62860-daa5-4d4d-9389-af74c39146dd",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
