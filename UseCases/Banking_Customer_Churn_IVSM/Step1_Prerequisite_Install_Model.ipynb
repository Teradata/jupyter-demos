{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26632eab-8349-4ce6-82f2-3e34a35887c2",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Install HuggingFace model using BYOM\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77679c3b-4db5-427e-b78d-640d05275800",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Introduction</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "Hugging Face is a French-American company based in New York City that develops computation tools for building applications using machine learning. They are known for their <b>Transformers Library</b> which provides open-source implementations of transformer models for text, image, video, audio tasks including time-series. These models include well-known architectures like BERT and GPT. The library is compatible with PyTorch, TensorFlow, and JAX deep learning libraries. <br>\n",
    "    Deep Learning Models in HuggingFace are pre-trained by users/open source outfits/companies on various types of data – NLP, Audio, Images, Videos etc. Most popular tool of choice by users is PyTorch (open source python library) which helps create a Deep Learning model from scratch or take an existing model, retrain/fine-tune (Transfer Learning) on new set of data to be published in HF. Models can be inference with CPUs and GPUs with slight performance improvement for smaller models.<br>\n",
    "</p>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Why Vantage?</b></p>  \n",
    "<p style = 'font-size:16px;font-family:Arial'>As many Hugging Face models are available in <b>ONNX Runtime</b>, we can load them using the <b>BYOM</b> feature of Vantage and run them in Vantage. Because of <b>Graph Optimizations</b> on ONNX Runtime, there are proven benchmarks that show that inference with <b>ONNX Runtime will be 20% faster than a native PyTorch model on a CPU</b>. </p>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Vantage Parallelism</b> on top of boosted ONNX Runtime inference can turn a Vantage system as effective as inference on GPUs. If we have a <b>Vantage box with 72 AMPs</b>, assuming the table is perfectly distributed, it will <b>closely match the performance of a dedicated GPU and data never moves across the network saving time and I/O operations</b>. As parallelism increases with number of AMPs, the model inference will complete faster in Teradata Vantage with the same amount of text data vs a GPU. We can of course quantize the model (change float8 weights to int8/int4) for inference on CPU to go even faster with some tradeoff with accuracy. However, If Model size goes up GPU advantage will widen – example LLM like LLama3 and costs will be disproportionate with GPU but for smaller models we can get comparable performance. \n",
    "</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Overall flow:</b></p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c95843-85c5-402e-911e-d1f02698ee10",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/pat1.png\" alt=\"Design pattern 1\" width=1200 height=900/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0dba0-93e7-4b5a-be19-820c0c46e4ce",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>1. Configuring the environment</b>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.1 Install the required libraries</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a55daf-1401-4253-93b2-36ba13756146",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install optimum sentence_transformers==4.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaecd47b-ca12-44ea-bfc2-021a6cd48ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061373e-60b6-488e-989f-ba421dcb025a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Note: </b><i>Please restart the kernel after executing these two lines. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b>\n",
    "</i>\n",
    "    <br>You can remove or comment the <b>%%capture</b> is you want to observe what <i>!pip install</i> is doing. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb451e8f-55c0-49dd-90b3-35453d4a9e56",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.2 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "704ddfb5-2c6b-4df7-93f7-1135461dc442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Data manipluation and Visualization libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "# Teradata libraries\n",
    "from teradataml import (\n",
    "    create_context, \n",
    "    delete_byom, \n",
    "    display,\n",
    "    execute_sql,\n",
    "    save_byom,\n",
    "    configure,\n",
    "    ONNXEmbeddings,\n",
    "    copy_to_sql,\n",
    "    remove_context,\n",
    "    in_schema,\n",
    "    KMeans,\n",
    "    KMeansPredict,\n",
    "    VectorDistance,\n",
    "    DataFrame,\n",
    "    DATE,\n",
    "    db_drop_table,\n",
    "    db_drop_view,\n",
    ")\n",
    "display.max_rows = 5\n",
    "from sqlalchemy.sql import literal_column as col\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6de29e63-cb44-4864-8a15-653c0838a64d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tdml.configure.val_install_location = \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3fd1fb1-77b4-45e2-8c85-bba4f7b33cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Logon successful\n",
      "Connected as: teradatasql://demo_user:xxxxx@host.docker.internal/dbc\n",
      "Engine(teradatasql://demo_user:***@host.docker.internal)\n"
     ]
    }
   ],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403dfd9-1c21-4b51-8569-1d6d4337b76b",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>2. Load HuggingFace Model</b>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>To generate embeddings, we need an ONNX model capable of transforming text into vector representations. We use a pretrained model from [Teradata's Hugging Face repository](https://huggingface.co/Teradata/gte-base-en-v1.5), such as gte-base-en-v1.5. The model and its tokenizer are downloaded and stored in Vantage tables as BLOBs using the save_byom function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee05a8ab-d39b-452c-99c1-894cdfe74df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89204167-31ab-46cc-a0fc-f8eda8412e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name = \"bge-base-en-v1.5\"\n",
    "number_dimensions_output = 768\n",
    "model_file_name = \"model.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "476e82d5-35d9-402b-85c7-3ead749a24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_hub_download(repo_id = \"Teradata/bge-base-en-v1.5\", filename=\"onnx/model.onnx\", local_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6060ba07-8083-4289-9cba-a113993f7959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokenizer.json'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Download Model from Teradata HuggingFace Page\n",
    "\n",
    "hf_hub_download(repo_id=f\"Teradata/{model_name}\", filename=f\"onnx/{model_file_name}\", local_dir=\"./\")\n",
    "hf_hub_download(repo_id=f\"Teradata/{model_name}\", filename=f\"tokenizer.json\", local_dir=\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06c484-25a6-4fcb-a0e3-a6c0a4d87be0",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>2.1 Save the Model</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In above steps, we have checked that the model is working fine in ONNX format. Now we will save the model file.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4576671d-53d1-4c72-88fb-a40025918f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db_drop_table(\"embeddings_models\")\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    db_drop_table(\"embeddings_tokenizers\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76087f8e-bef9-4d98-bd60-85e5d264361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the model table 'embeddings_models' as it does not exist.\n",
      "Model is saved.\n",
      "Created the model table 'embeddings_tokenizers' as it does not exist.\n",
      "Model is saved.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load Models into Vantage\n",
    "# a) Embedding model\n",
    "save_byom(model_id = model_name, # must be unique in the models table\n",
    "               model_file = f\"onnx/{model_file_name}\",\n",
    "               table_name = 'embeddings_models' )\n",
    "# b) Tokenizer\n",
    "save_byom(model_id = model_name, # must be unique in the models table\n",
    "              model_file = 'tokenizer.json',\n",
    "              table_name = 'embeddings_tokenizers') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37752326-637f-4c6a-b6b0-0ee0bafd9653",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Recheck the installed model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75a0e5c5-443f-48d1-9820-6c69bb166651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ef84ecb43147cfa4becf6bb6cab464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "\ttable { border:ridge 5px}\n",
       "\ttable td {border:inset 1px;}\n",
       "\ttable tr#HeaderRow {background-color:grey; color:white;}\n",
       "\t</style>\n",
       "<html><table style=\"min-width:1000px;\">\n",
       "\t<tr id=\"HeaderRow\">\n",
       "<th>model_id</th><th>model</th>\n",
       "</tr>\n",
       "\t<tr>\n",
       "\t\t<td>bge-base-en-v1.5</td>\n",
       "\t\t<td>b'80812077079746F72...'</td>\n",
       "\t</tr>\n",
       "</table></html>"
      ],
      "text/plain": [
       "                                    model\n",
       "model_id                                 \n",
       "bge-base-en-v1.5  b'80812077079746F72...'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = DataFrame('embeddings_models')\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f751314f-9bf9-4b7a-820c-fd72a0c10eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae20f4ad020d4a4f950329861c05d057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "\ttable { border:ridge 5px}\n",
       "\ttable td {border:inset 1px;}\n",
       "\ttable tr#HeaderRow {background-color:grey; color:white;}\n",
       "\t</style>\n",
       "<html><table style=\"min-width:1000px;\">\n",
       "\t<tr id=\"HeaderRow\">\n",
       "<th>model_id</th><th>model</th>\n",
       "</tr>\n",
       "\t<tr>\n",
       "\t\t<td>bge-base-en-v1.5</td>\n",
       "\t\t<td>b'7B0A20202276657273...'</td>\n",
       "\t</tr>\n",
       "</table></html>"
      ],
      "text/plain": [
       "                                     model\n",
       "model_id                                  \n",
       "bge-base-en-v1.5  b'7B0A20202276657273...'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token = DataFrame('embeddings_tokenizers')\n",
    "df_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8169c-8827-44da-8d4e-39963cd2f91d",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>3. Cleanup</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will remove the context.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36bfae31-60de-4ff9-8a64-8e52c44efd14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dfbb00-2743-437d-8c0e-4c0d8506214c",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid \">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
