{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Financial Fraud Detection using Traditional approach \n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Traditionl Approach</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "    ClearScape Analytics provides powerful, flexible end-to-end data connectivity, feature engineering, model training, evaluation, and operational functions that can be deployed at scale as enterprise data assets; treating the products of ML and AI as first-class analytic processes in the enterprise. With ClearScape Analytics, data scientists can use their preferred language, tools and platform to develop models to identify this fraud. Even in large scale operations, users have the guarantee that Vantage can scale to their needs and reduce fraud.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Below are the steps involved in traditional approach:</p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li><b>Prepare data: </b>ClearScape Analytics offers highly optimized in-database functions for data preparation, minimizing data movement and enabling the enterprise feature store.</li>\n",
    "    <li><b>Train models: </b>ClearScape Analytics provides vertical and horizontal scaling capabilities that make it possible to efficiently train any number of models — from a few to a few million.</li>\n",
    "    <li><b>Deploy models: </b>ClearScape Analytics integrates model scoring with business data, both in real time and \u000b",
    "batch scoring, for effective operationalization and automated monitoring of AI models.</li>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'>In the traditional approach mentioned, we will follow the below steps:</p>    \n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Data Collection</li>\n",
    "    <li>Data Exploration</li>\n",
    "    <li>Enterprise feature store</li>\n",
    "    <li>Data Preparation using widgets</li>\n",
    "    <li>Model Training(2-3 different models)</li>\n",
    "    <li>Model Evaluation using ROC and Confusion Matrix</li>\n",
    "    <li>Best performing model</li>\n",
    "    <li>Model Scoring using best model</li>\n",
    "    <li>Operationalize Model using ModelOps</li>\n",
    "</ul>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Why Vantage?</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>To maximize the business value of advanced analytic techniques including Machine Learning and Artificial Intelligence, it is estimated that organizations must scale their model development and deployment pipelines to 100s or 1000s of times greater amounts of data, models, or both.</p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>1. Configuring the Environment</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade teradataml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Enterprise Feature Store is new feature added in teradataml 20.0.0.3 so we are upgrading the installed teradataml version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Note: </b><i>Please restart the kernel after executing the above command to bring the upgraded library into memory. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import getpass\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Teradata Libraries\n",
    "from teradataml import *\n",
    "\n",
    "# Configuration\n",
    "spacing_large = \" \"*95\n",
    "spacing_small = \" \"*12\n",
    "display.max_rows = 5\n",
    "configure.val_install_location = 'val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>2. Connect to Vantage</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run -i ../../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username = 'demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql(\"SET query_band='DEMO=EE_Financial_Fraud_Detection_Traditional_Approach.ipynb;' UPDATE FOR SESSION;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We begin running steps with Shift + Enter keys. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo on cloud storage. We have the option of either running the demo using foreign tables to access the data without using any storage on our environment or downloading the data to local storage, which may yield somewhat faster execution. However, we need to consider available storage. There are two statements in the following cell, and one is commented out. We may switch which mode we choose by changing the comment string.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ../run_procedure.py \"call get_data('DEMO_GLM_Fraud_cloud');\"        # Takes 1 minute\n",
    "%run -i ../../run_procedure.py \"call get_data('DEMO_GLM_Fraud_local');\"        # Takes 2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Optional step – We should execute the below step only if we want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../run_procedure.py \"call space_report();\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>3. Data Exploration</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We loaded the data from <a href = 'https://www.kaggle.com/code/georgepothur/4-financial-fraud-detection-xgboost/data'>https://www.kaggle.com/code/georgepothur/4-financial-fraud-detection-xgboost/data</a> into Vantage in a table named \"transaction_data\". We checked the data size and printed sample rows: 63k rows and 12 columns.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'><b><i>*Please scroll down to the end of the notebook for detailed column descriptions of the dataset.</i></b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txn_data = DataFrame(in_schema('DEMO_GLM_Fraud', 'transaction_data'))\n",
    "# txn_data = DataFrame(in_schema('demo_user', 'transaction_data'))\n",
    "print(txn_data.shape)\n",
    "txn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In this simulated scenario, deceptive agents engage in transactions with the objective of taking control of customers' accounts, transferring funds to another account, and ultimately cashing out for profit.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.1 How many fraudulent transactions do we have in our dataset?</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 92 fraud transactions i.e. 0.14% of fraud transactions in the dataset.\n",
    "print(\"No of fraud transactions: %d\\nPercentage of fraud transactions: %.2f%%\"%(\n",
    "    txn_data.loc[txn_data.isFraud == 1].shape[0],\n",
    "    txn_data.loc[txn_data.isFraud == 1].shape[0]/txn_data.shape[0]*100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.2 How many transactions do we have group by transaction type?</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for fraud transactions and group by 'type'\n",
    "transactions_by_type = txn_data.groupby('type').count().get(['type','count_txn_id'])\n",
    "\n",
    "\n",
    "# Sort by 'count_step' column in descending order\n",
    "transactions_by_type = transactions_by_type.sort('count_txn_id', ascending = False)\n",
    "\n",
    "transactions_by_type = transactions_by_type.assign(\n",
    "    type_int = case([\n",
    "        (transactions_by_type.type == 'CASH_IN', 0),\n",
    "        (transactions_by_type.type == 'CASH_OUT', 1),\n",
    "        (transactions_by_type.type == 'DEBIT', 2),\n",
    "        (transactions_by_type.type == 'PAYMENT ', 3),\n",
    "        (transactions_by_type.type == 'TRANSFER', 4),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_by_type.plot(\n",
    "    x = transactions_by_type.type_int,\n",
    "    y = transactions_by_type.count_txn_id,\n",
    "    kind = 'bar',\n",
    "    legend = ['Count by Type'],\n",
    "    ylabel = 'Count of Transactions',\n",
    "    xlabel = spacing_small.join(sorted(list(transactions_by_type[['type']].get_values().flatten()))),\n",
    "    title = \"Number of Transactions per Transaction Type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.3 How many fraudulent transactions do we have group by transaction type?</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter data for fraud transactions and group by 'type'\n",
    "fraud_transactions_by_type = txn_data.loc[txn_data.isFraud == 1].groupby('type').count().get(['type','count_txn_id'])\n",
    "\n",
    "# Sort by 'count_step' column in descending order\n",
    "fraud_transactions_by_type = fraud_transactions_by_type.sort('count_txn_id', ascending = False)\n",
    "\n",
    "fraud_transactions_by_type = fraud_transactions_by_type.assign(\n",
    "    total_fraud = txn_data.loc[txn_data.isFraud == 1].shape[0],\n",
    "    type_int = case([(fraud_transactions_by_type.type == 'TRANSFER', 0)], else_ = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fraud_transactions_by_type.plot(\n",
    "    x = fraud_transactions_by_type.type_int,\n",
    "    y = [fraud_transactions_by_type.total_fraud, fraud_transactions_by_type.count_txn_id],\n",
    "    kind = 'bar',\n",
    "    figsize = (800, 500),\n",
    "    legend = ['Total Fraud', 'Count by Type'],\n",
    "    ylabel = 'Count of Fraud Transactions',\n",
    "    xlabel = 'TRANSFER' + spacing_large + 'CASH_OUT',\n",
    "    title = \"Number of Fraud Transactions by Transaction Type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>From the above result, we can see that out of the 92 fraud transactions, 47 are from transaction type \"TRANSFER\" and 45 are from \"CASH_OUT\".</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.4 What percentage of fraudulent transactions do we have where transaction amount is equal to old balance in the origin account?</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This might be the case where the fraudster emptied the account of the victim.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of cleanout fraud transactions: %d\\nPercentage of cleanout fraud transactions: %.2f%%\"%(\n",
    "    txn_data.loc[txn_data['amount'] == txn_data.oldbalanceOrig].loc[txn_data['isFraud'] == 1].shape[0],\n",
    "    txn_data.loc[txn_data['amount'] == txn_data.oldbalanceOrig].loc[txn_data['isFraud'] == 1].shape[0] / txn_data.loc[txn_data.isFraud == 1].shape[0]*100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>From the above result, we can see that out of 92 Fraud transactions, the amount involved in 90 fraud transactions was equal to the total balance in the account. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Below are some insights about the dataset:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>We have 92 fraud transactions, which account for 0.14% of the dataset.</li>\n",
    "    <li>Out of these 92 fraud transactions, 47 are of type TRANSFER, and 45 are of type CASH_OUT.</li>\n",
    "    <li>Approximately 97.83% of our fraud transactions have a transaction amount equal to oldbalanceOrig, indicating account cleanout.</li>\n",
    "    <li>About 71.74% of our fraud transactions have the recipient's old balance as zero.</li>\n",
    "    <li>The isFlaggedFraud indicator is correct only two times among our 92 fraud transactions.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.5 Univariate statistics</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The describe function computes the count, mean, std, min, percentiles, and max for numeric columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.6 Checking for Null Values</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The ColumnSummary() function can be used to take a quick look at the columns, their datatypes, and summary of NULLs/non-NULLs for a given table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsum = ColumnSummary(\n",
    "    data  = txn_data,\n",
    "    target_columns = [':']\n",
    ")\n",
    "colsum.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>4. Feature Engineering</b>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Teradata Enterprise Feature Store (EFS) Functions are designed to handle feature management within the Vantage environment. While inspired by the syntax of Feast, Teradata EFS Functions stands out, offering efficiency and robustness in data management and feature handling tailored specifically for the use of Teradata Vantage. Teradata EFS Functions use Teradata Dataframes for Feature management, to the contrary of the pandas dataframe of Feast. With Teradata Dataframes we avoid extracting the data to create or use Features from the Enterprise Feature Store (EFS). The EFS Functions are crafted to empower Data Science teams for effective and streamlined feature management. This notebook will walk you through the capabilities of EFS Functions, demonstrating how it integrates seamlessly with your data models and processes.</p>\n",
    "\n",
    "<hr style=\"height:1px;border:none;\">\n",
    "<b style = 'font-size:18px;font-family:Arial'>4.1 Setup a Feature Store Repository</b>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>The Enterprise Feature Store (EFS) SDK is designed with a totally object-oriented approach, focusing on intuitive interaction with feature stores. Central to this design are several core objects: Feature, Entity, DataSource, FeatureGroup. Together, these objects facilitate the efficient management and utilization of features within your data ecosystem, leveraging Teradata Vantage for metadata storage.</p>\n",
    "<p style='font-size:16px;font-family:Arial'>A feature store repository serves as the foundational environment for storing and managing your data features. The owner of the FeatureStore can grant/revoke read only, write only or read and write authorization to other user(s)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureStore is not setup for repo LabRepoOne. Let's setup.\n",
    "fin_fs = FeatureStore('FinFraud')\n",
    "fin_fs.setup(perm_size='10e8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify by listing the repo's.\n",
    "FeatureStore.list_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.2 Create and Register Entity </b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity for DataFrame 'patient_profile_df'\n",
    "entity=Entity(name='TrxnId', columns=txn_data.txn_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Entity.\n",
    "fin_fs.apply(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at existing Entities after registering the Entity.\n",
    "fin_fs.list_entities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.3 Create and Register FeatureGroup </b></p>\n",
    "<li style = 'font-size:16px;font-family:Arial'>FeatureGroup can be created using Teradata DataFrame.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>FeatureGroup can be created using SQL Query. </li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>FeatureGroup can be created using objects of Feature, Entity, DataSource.  </li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><b>Creating a FeatureGroup from Teradata DataFrame\n",
    "</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_fg = FeatureGroup.from_DataFrame(\n",
    "    name='TransDF', \n",
    "    entity_columns='txn_id', \n",
    "    df=txn_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at Properties.\n",
    "fin_fg.features, fin_fg.entity, fin_fg.data_source, fin_fg.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_fs.apply(fin_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_fs.list_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<b style = 'font-size:18px;font-family:Arial'>4.4 Use Enterprise Feature Store with teradataml analytic functions for data preparation.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Since FeatureStore stores DataSource also, you can retrive Teradata DataFrame from FeatureStore. <br> `FeatureStore.get_dataset()` get's Teradata DataFrame from FeatureGroup.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DataSet for FeatureGroup PatientProfile. \n",
    "txn_data_df=fin_fs.get_dataset('TransDF')\n",
    "txn_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>5. Data Preparation using widgets</b>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'><b>We'll perform the following steps:</b></p>\n",
    "<ul style='font-size:16px;font-family:Arial'>\n",
    "    <li>We will one-hot encode the categorical \"type\" column.</li>\n",
    "    <li>We will perform feature scaling using ScaleFit and ScaleTransform on numerical columns.</li>\n",
    "    <li>We will split the data into training and testing datasets (80:20 split).</li>\n",
    "</ul>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>We perform feature scaling during data pre-processing to handle highly varying magnitudes, values, or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values higher and consider smaller values as lower ones, regardless of the unit of the values.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.1 Drop redundant columns</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We don't need nameDest, nameOrigin, and isFlaggedFraud for model training as they do not impact the outcome. We have txn_id to uniquely identify each transaction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txn_data_df = txn_data_df.drop(['nameDest', 'nameOrig', 'isFlaggedFraud'], axis = 1)\n",
    "txn_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will copy this dataset to a Vantage table to be used in widgets for Data Preparation</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "copy_to_sql(txn_data_df, table_name='transaction_data_new', if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.2 Checking for Outliers using widgets</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The OutlierFilterFit() function calculates the lower percentile, upper percentile, count of rows and median for all the \"target_columns\" provided by the user. These metrics for each column help the function OutlierTransform() detect outliers in data.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here we are using teradataml syntax for the function. The same can be achived using the following SQL as well.</p>\n",
    "\n",
    "<code>SELECT * FROM TD_OutlierFilterFit(\n",
    "    ON \"DEMO_GLM_Fraud\".\"transaction_data\" AS InputTable\n",
    "    OUT TABLE OutputTable(\"DEMO_USER\".\"Outlier_output\")\n",
    "    USING\n",
    "    TargetColumns('amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig')\n",
    ") as dt;</code>\n",
    "\n",
    "<p style = 'font-size:14px;font-family:Arial'><b><i>*Please note that both the versions run in-database and there is no data transfer involved.</i></b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from teradatamlwidgets.analytic_functions.Ui import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>As we are showcasing widgets here, we will have to follow some specific steps to execute this function.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'><b>After we execute the below cell it will showcase the widgets output where you will have to enter the below details </b></p>\n",
    "    <li style='font-size:16px;font-family:Arial'>Select a functions: Please select <b>OutlierFilterFit</b></li>\n",
    "    \n",
    "<p style='font-size:16px;font-family:Arial'>After select the function we will see the widget for entering the Inputs we need to provide for the function. Below are the inputs to be provided</p> \n",
    "    <li style='font-size:16px;font-family:Arial'><code>InputTable:</code> Select <b>transaction_data_new</b></li>\n",
    "    <li style='font-size:16px;font-family:Arial'><code>TargetColumns:</code> Please select below columns 1 in each box <b>'amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig'</b></li>\n",
    "</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>After doing this click on the <b>Execute</b> button and check the output</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"transaction_data\",\"transaction_data_new\"]\n",
    "outputs = [\"outlierfit_table\"]\n",
    "ui = Ui(\n",
    "    outputs = outputs,\n",
    "    inputs = inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>After OutlierFilterFit we will execute the OutlierFilterTransform function.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'><b>After we execute the below cell it will showcase the widgets output where you will have to enter the below details </b></p>\n",
    "    <li style='font-size:16px;font-family:Arial'>Select a functions: Please select <b>OutlierFilterTransform</b></li>\n",
    "    \n",
    "<p style='font-size:16px;font-family:Arial'>After select the function we will see the widget for entering the Inputs we need to provide for the function. Below are the inputs to be provided</p> \n",
    "    <li style='font-size:16px;font-family:Arial'><code>InputTable:</code> Select <b>transaction_data_new</b></li>\n",
    "    <li style='font-size:16px;font-family:Arial'><code>FitTable:</code> Please select <b> outlierfit_table</b></li>\n",
    "</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>After doing this click on the <b>Execute</b> button and check the output</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['transaction_data_new','outlierfit_table']\n",
    "outputs = ['outlier_trans_table']\n",
    "ui = Ui(\n",
    "    outputs = outputs,\n",
    "    inputs = inputs,\n",
    "    function=\"OutlierFilterTransform\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = DataFrame('outlier_trans_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rows before removing outliers: {txn_data_df.shape[0]}\\n\\\n",
    "Rows after removing outliers: {res.shape[0]}\\n\\\n",
    "Total outliers: {txn_data_df.shape[0] - res.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers = td_minus([txn_data_df, res])\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.3 One-hot encoding</b></p>\n",
    "<p style='font-size:16px;font-family:Arial'>\n",
    "Here, we are one-hot encoding the \"type\" column. We find one-hot encoding necessary in many cases to represent categorical variables as binary values, enable numerical processing, ensure feature independence, handle non-numeric data, and improve the performance and interpretability of our machine learning models.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txn_type_encoder = OneHotEncoder(\n",
    "    values = [\"CASH_IN\", \"CASH_OUT\", \"DEBIT\", \"PAYMENT\", \"TRANSFER\"],\n",
    "    columns = \"type\"\n",
    ")\n",
    "\n",
    "retain = Retain(\n",
    "    columns = ['step', 'amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig', 'isFraud']\n",
    ")\n",
    "\n",
    "obj = valib.Transform(\n",
    "    data = txn_data,\n",
    "    one_hot_encode = txn_type_encoder,\n",
    "    retain = retain,\n",
    "    index_columns = 'txn_id'\n",
    ")\n",
    "txn_trans = obj.result\n",
    "txn_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_trans = txn_trans.assign(isFraud=txn_trans.isFraud.cast(type_=INTEGER()))\n",
    "txn_trans                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above output shows that we have transformed the data into a transfromed dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(txn_trans, table_name = 'clean_data', if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>6. Create training and testing datasets in Vantage</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We'll create two datasets for training and testing in the ratio of 80:20.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrainTestSplit_out = TrainTestSplit(\n",
    "    data = txn_trans,\n",
    "    id_column = \"txn_id\",\n",
    "    train_size = 0.80,\n",
    "    test_size = 0.20,\n",
    "    seed = 25\n",
    ")\n",
    "\n",
    "df_train = TrainTestSplit_out.result[TrainTestSplit_out.result['TD_IsTrainRow'] == 1].drop(['TD_IsTrainRow'], axis = 1)\n",
    "df_test = TrainTestSplit_out.result[TrainTestSplit_out.result['TD_IsTrainRow'] == 0].drop(['TD_IsTrainRow'], axis = 1)\n",
    "\n",
    "print(\"Training Set = \" + str(df_train.shape[0]) + \". Testing Set = \" + str(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "copy_to_sql(df_train, table_name = 'clean_data_train', if_exists = 'replace')\n",
    "copy_to_sql(df_test, table_name = 'clean_data_test', if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above output shows that we have transformed the data into a scaled dataset. Scaling our data makes it easy for our model to learn and understand the problem.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>6. In-Database model training</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>ClearScape Analytics provides vertical and horizontal scaling capabilities that make it possible to efficiently train any number of models — from a few to a few million. With a variety of model tarining functions we can build and train high quality ML models, which leverages the analytic datasets. These set of ML components and capabilities help to reduce effort, lower costs, and get ML models into production as quickly as possible</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we will be using In-DB XGBoost and DecisionForest functions to train our models.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<b style = 'font-size:18px;font-family:Arial'>6.1 XGBoost model training</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The XGBoost() function, also known as eXtreme Gradient Boosting, is an implementation of the gradient boosted decision tree algorithm designed for speed and performance. It has recently been dominating applied machine learning.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In gradient boosting, each iteration fits a model to the residuals (errors) of the previous iteration to correct the errors made by existing models. The predicted residual is multiplied by this learning rate and then added to the previous prediction. Models are added sequentially until no further improvements can be made. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here we are using teradataml syntax for the function. The same can be achived using the following SQL as well.</p>\n",
    "\n",
    "<code>SELECT * FROM TD_XGBoost(\n",
    "\tON \"DEMO_USER\".\"clean_data_train\" AS \"input\"\n",
    "\tPARTITION BY ANY\n",
    "\tUSING InputColumns('amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig','CASH_IN_type','CASH_OUT_type','DEBIT_type','PAYMENT_type','TRANSFER_type')\n",
    "\tResponseColumn('isFraud')\n",
    "\tMaxDepth(7)\n",
    "\tSeed(42)\n",
    "\tModelType('Classification')\n",
    "\tRegularizationLambda(120.0)\n",
    "\tShrinkageFactor(0.1)\n",
    ") as sqlmr</code>\n",
    "\n",
    "<p style = 'font-size:14px;font-family:Arial'><b><i>*Please note that both the versions run in-database and there is no data transfer involved.</i></b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = df_train.columns\n",
    "cols.remove('txn_id')\n",
    "cols.remove('step')\n",
    "cols.remove('isFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XGBoost_out = XGBoost(\n",
    "    data=df_train,\n",
    "    input_columns=cols,\n",
    "    response_column = 'isFraud',\n",
    "    lambda1 = 120.0,\n",
    "    model_type='Classification',\n",
    "    seed=42,\n",
    "    shrinkage_factor=0.1,\n",
    "    max_depth=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XGBoost_out.output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The function output is a trained XGBoost model, and we can input it to the XGBoostPredict() function for prediction.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>6.2 Decision Forest model Training</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The <a href = 'https://docs.teradata.com/search/all?query=TD_DecisionForest&content-lang=en-US'>DecisionForest</a> model function is an ensemble algorithm used for classification and regression predictive modeling problems. It is an extension of bootstrap aggregation (bagging) of decision trees. Typically, constructing a decision tree involves evaluating the value for each input feature in the data to select a split point. The function reduces the features to a random subset (that can be considered at each split point); the algorithm can force each decision tree in the forest to be very different to improve prediction accuracy. </p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This function takes the training data as input, as well as the following function parameters</p>\n",
    "    <ul style = 'font-size:16px;font-family:Arial'>\n",
    "        <li style = 'font-size:16px;font-family:Arial'>InputColumns; list or range of columns used as features (we used an ordinal reference of columns 2:217)</li>\n",
    "        <li style = 'font-size:16px;font-family:Arial'>ResponseColumn; the dependent or target value (we used “class”, the first column)</li>\n",
    "        <li style = 'font-size:16px;font-family:Arial'>TreeType; either CLASSIFICATION or REGRESSION</li>\n",
    "    <li style = 'font-size:16px;font-family:Arial'>Other hyperparameter values detailed in the documentation</li>\n",
    "        </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_new = DataFrame('clean_data_train')\n",
    "df_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionForest_out = DecisionForest(data = df_train_new, \n",
    "                            input_columns = cols, \n",
    "                            response_column = 'isFraud', \n",
    "                            max_depth = 16, \n",
    "                            num_trees = 8, \n",
    "                            min_node_size = 1, \n",
    "                            mtry = 3, \n",
    "                            mtry_seed = 3, \n",
    "                            seed = 3, \n",
    "                            tree_type = 'CLASSIFICATION')\n",
    "# Print the result DataFrame.\n",
    "DecisionForest_out.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>7. In-Database model scoring</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>ClearScape Analytics integrates model scoring with business data, both in real time and batch scoring, for effective operationalization and automated monitoring of AI models.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>By deploying these models to conduct live data scoring, Vantage delivers the crucial insights needed to drive business outcomes</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<b style = 'font-size:18px;font-family:Arial'>7.1 XGBoost model scoring</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The XGBoostPredict() function runs the predictive algorithm based on the model generated by XGBoost(). The XGBoost() function, also known as eXtreme Gradient Boosting, performs classification or regression analysis on datasets.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XGBoostPredict_out = XGBoostPredict(\n",
    "    newdata=df_test,\n",
    "    object=XGBoost_out.result,\n",
    "    model_type='Classification',\n",
    "    id_column='txn_id',\n",
    "    object_order_column=['task_index', 'tree_num',\n",
    "                       'iter', 'tree_order'],\n",
    "    accumulate='isFraud',\n",
    "    output_prob=True,\n",
    "    output_responses=['0', '1']\n",
    ").result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XGBoostPredict_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The output above shows our prob_1, i.e., the transaction is fraud, and prob_0, i.e., the transaction is not a fraud. We use these probabilities in our prediction column to assign a class label.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<b style = 'font-size:18px;font-family:Arial'>7.2 Decision Forest model scoring</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The DecisionForestPredict() function uses the model generated by the DecisionForest() function to generate predictions on a response variable for a test set of data. The model can be stored in either a teradataml DataFrame or a DecisionForest object.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_new = DataFrame('clean_data_test')\n",
    "df_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_forest_predict_out = TDDecisionForestPredict(object = DecisionForest_out.result,\n",
    "                                                        newdata = df_test_new,\n",
    "                                                        id_column = \"txn_id\",\n",
    "                                                        detailed = False,\n",
    "                                                        output_prob = True,\n",
    "                                                        output_responses = ['0','1'],\n",
    "                                                        accumulate = 'isFraud')\n",
    "df_predict = decision_forest_predict_out.result\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>8. In-Database model evaluation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>ClearScape Analytics model evaluation report highlights the model performance in the form of certain metrics and compare models based on the metric values.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The ClassificationEvaluator() function evaluate and emits various metrics of classification model based on its predictions on the data. Apart from accuracy, the secondary output data returns micro, macro, and weighted-averaged metrics of precision, recall, and F1-score values.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_char = df = df_predict.assign(isFraud = df_predict.isFraud.cast(type_=VARCHAR(2))\n",
    "                                        ,prediction_ch = df_predict.prediction.cast(type_=VARCHAR(2)))\n",
    "df_predict_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df = df_test.join(XGBoostPredict_out, on='txn_id', lsuffix='test', rsuffix='pred')\n",
    "combined_df[combined_df['Prediction']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = XGBoostPredict_out.assign(Prediction = XGBoostPredict_out.Prediction.cast(type_ = BYTEINT))\n",
    "out = out.assign(Prediction = out.Prediction.cast(type_ = VARCHAR(2)))\n",
    "out = out.assign(isFraud = out.isFraud.cast(type_ = VARCHAR(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ClassificationEvaluator_obj = ClassificationEvaluator(\n",
    "    data = out,\n",
    "    observation_column = 'isFraud',\n",
    "    prediction_column = 'Prediction',\n",
    "    labels = ['0', '1']\n",
    ")\n",
    "xgb_eval = ClassificationEvaluator_obj.output_data\n",
    "xgb_eval.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ClassificationEvaluator_obj = ClassificationEvaluator(\n",
    "    data = df_predict_char,\n",
    "    observation_column = 'isFraud',\n",
    "    prediction_column = 'prediction_ch',\n",
    "    labels = ['0', '1']\n",
    ")\n",
    "df_eval = ClassificationEvaluator_obj.output_data\n",
    "df_eval.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>9. Visualize the results (ROC curve and AUC)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We create the ROC curve, which is a graph between TPR (True Positive Rate) and FPR (False Positive Rate). We use the area under the ROC curve as a metric to evaluate how well our model can distinguish between positive and negative classes. A higher AUC indicates better performance in distinguishing between the positive and negative categories. We generally consider an AUC above 0.75 as decent.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from teradataml import ROC\n",
    "\n",
    "xgb_roc_out = ROC(\n",
    "    probability_column = '\"Prob_1\"',\n",
    "    observation_column = \"isFraud\",\n",
    "    positive_class = \"1\",\n",
    "    data = XGBoostPredict_out,\n",
    "    num_thresholds=300\n",
    ")\n",
    "\n",
    "xgb_roc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from teradataml import ROC\n",
    "\n",
    "df_roc_out = ROC(\n",
    "    probability_column = '\"prob_1\"',\n",
    "    observation_column = \"isFraud\",\n",
    "    positive_class = \"1\",\n",
    "    data = df_predict,\n",
    "    num_thresholds=300\n",
    ")\n",
    "\n",
    "df_roc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assigning new index column\n",
    "xgb_roc_out.result = xgb_roc_out.result.assign(row = 1)\n",
    "# Changing the index label.\n",
    "xgb_roc_out.result._index_label = [\"row\"]\n",
    "xgb_auc = xgb_roc_out.result.get_values()[0][0]\n",
    "\n",
    "df_roc_out.result = df_roc_out.result.assign(row = 1)\n",
    "# Changing the index label.\n",
    "df_roc_out.result._index_label = [\"row\"]\n",
    "df_auc = df_roc_out.result.get_values()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from teradataml import subplots\n",
    "fig, axis = subplots(1, 2)\n",
    "fig.height, fig.width = 600,1200\n",
    "plot = xgb_roc_out.output_data.plot(x=xgb_roc_out.output_data.fpr,\n",
    "    y=[xgb_roc_out.output_data.tpr, xgb_roc_out.output_data.fpr],\n",
    "    xlabel='False Positive Rate',\n",
    "    ylabel='True Positive Rate',\n",
    "    figure=fig,\n",
    "    ax=axis[0],\n",
    "    color='carolina blue',\n",
    "    legend=[f'XGBoost AUC = {round(xgb_auc, 4)}', 'AUC Baseline'],\n",
    "    legend_style='lower right',\n",
    "    grid_linestyle='--',\n",
    "    grid_linewidth=0.5,\n",
    "    linestyle = ['-', '--'])\n",
    "\n",
    "plot = df_roc_out.output_data.plot(\n",
    "    x=df_roc_out.output_data.fpr,\n",
    "    y=[df_roc_out.output_data.tpr, df_roc_out.output_data.fpr],\n",
    "    xlabel='False Positive Rate',\n",
    "    ylabel='True Positive Rate',\n",
    "    figure=fig,\n",
    "    ax=axis[1],\n",
    "    color='carolina blue',\n",
    "    legend=[f'DecisionForest AUC = {round(df_auc, 4)}', 'AUC Baseline'],\n",
    "    legend_style='lower right',\n",
    "    grid_linestyle='--',\n",
    "    grid_linewidth=0.5,\n",
    "    linestyle = ['-', '--']\n",
    ")\n",
    "\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Looking at the above ROC Curve, we can confidently say that our model has performed well on testing data. The AUC value is above 0.75 and resonates with our understanding that the model is performing well.</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Conclusion</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demonstration, we have illustrated a simplified - but complete - overview of how we can implement a typical machine learning workflow completely inside the database using Vantage. This allows us to leverage Vantage's operational scale, power, and stability.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>10. ModelOps for Financial Fraud Detection</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We used feature store to store features as well as its processing. We re-used it in model training. The features and processing can be re-used accross multiple machine learning models and use-case , helping to improve data science productivity</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Teradata's traditional approach using Clearscape Analytic functions play a crucial role in this context by automating the complex process of building and deploying machine learning models. Various Clearscape Analytic functions are used for optimal preparation and training of models, delivering high-quality machine learning models in minutes. With the capabilities of ClearScape Analytics ModelOps, Analytics-driven organizations can follow a mature methodology and automated capabilities to solve this gap and make efficient model operationalization at Scale in Vantage.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>ClearScape Analytics ModelOps manages the operationalization of advanced analytics in Teradata Vantage providing Deployment, Governance and Monitoring of your AI/ML models at scale. ModelOps provides an easy-to-use web-based user interface (UI), a command line interface (CLI) and Python/R Software Development Kit (SDK).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>As a part of this End-to_End demo for Financial Fraud Detection, we will implement the ModelOps cycle using Vantage In-Db Vantage functions. Click the below button which will showcase the steps required for the ModelOps cycle as a part of Operatinilizing the model.</p>\n",
    "\n",
    "<a href=\"FinFraud_EndtoEnd_ModelOps_GIT_Python_indb_XGB.ipynb\" style=\"display: inline-flex; align-items: center; justify-content: center; background-color: #007373; color: #FFFFFF; font-family: Arial, sans-serif; font-size: 16px; font-weight: bold; text-decoration: none; padding: 12px 24px; border: none; border-radius: 8px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); cursor: pointer; transition: all 0.3s ease;\">\n",
    "  LAUNCH Notebook for ModelOps\n",
    "  <img src=\"https://img.icons8.com/ios-filled/50/ffffff/external-link.png\" alt=\"External Link Icon\" style=\"margin-left: 8px; width: 20px; height: 20px;\">\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>11. Cleanup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Note: </b>The tables created in this demo will be used in the ModelOps notebook which can be invoked on click of the above button. Please uncomment the below lines of code in case you do not want to run the ModelOps notebook and want to delete the tables created for this demo.</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We need to clean up our work tables to prevent errors next time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = ['clean_data', 'clean_data_train', 'clean_data_test']\n",
    "\n",
    "# # Loop through the list of tables and execute the drop table command for each table\n",
    "# for table in tables:\n",
    "#     try:\n",
    "#         db_drop_table(table_name = table)\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin_fs.archive_feature_group(feature_group='TransDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin_fs.delete_feature_group(feature_group='TransDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin_fs.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'> <b>Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will use the following code to clean up tables and databases created for this demonstration.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ../run_procedure.py \"call remove_data('Demo_glm_fraud');\"        # Takes 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<b style = 'font-size:20px;font-family:Arial'>Required Materials</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Let’s look at the elements we have available for reference for this notebook:</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Filters:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li><b>Industry:</b> Finance</li>\n",
    "    <li><b>Functionality:</b> Machine Learning</li>\n",
    "    <li><b>Use Case:</b> Fraud Detection</li>\n",
    "</ul>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Related Resources:</b></p>\n",
    "\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li><a href='https://www.teradata.com/Blogs/Fraud-Busting-AI'>Fraud-Busting-AI</a></li>\n",
    "    <li><a href='https://www.teradata.com/Industries/Financial-Services'>Financial Services</a></li>\n",
    "    <li><a href='https://www.teradata.com/Resources/Datasheets/Move-from-Detection-to-Prevention-and-Outsmart-Fraudsters'>Move from Detection to Prevention and Outsmart Tech-Savvy Fraudsters</a></li>\n",
    "</ul>\n",
    "\n",
    "<b style = 'font-size:20px;font-family:Arial'>Dataset:</b>\n",
    "\n",
    "- `txn_id`: transaction id\n",
    "- `step`: maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (31 days simulation).\n",
    "- `type`: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER\n",
    "- `amount`: amount of the transaction in local currency\n",
    "- `nameOrig`: customer who started the transaction\n",
    "- `oldbalanceOrig`: customer's balance before the transaction\n",
    "- `newbalanceOrig`: customer's balance after the transaction\n",
    "- `nameDest`: customer who is the recipient of the transaction\n",
    "- `oldbalanceDest`: recipient's balance before the transaction\n",
    "- `newbalanceDest`: recipient's balance after the transaction\n",
    "- `isFraud`: identifies a fraudulent transaction (1) and non fraudulent (0)\n",
    "- `isFlaggedFraud`: flags illegal attempts to transfer more than 200,000 in a single transaction\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Links:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Uses a dataset and feature discovery methods outlined here: <a href = 'https://www.kaggle.com/georgepothur/4-financial-fraud-detection-xgboost/notebook'>https://www.kaggle.com/georgepothur/4-financial-fraud-detection-xgboost/notebook</a></li>\n",
    "    <li>Teradataml Python reference: <a href = 'https://docs.teradata.com/search/all?query=Python+Package+User+Guide&content-lang=en-US'>here</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid #91A0Ab\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
