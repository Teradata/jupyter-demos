{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Financial Fraud Detection using Traditional approach \n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Traditionl Approach</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "    ClearScape Analytics provides powerful, flexible end-to-end data connectivity, feature engineering, model training, evaluation, and operational functions that can be deployed at scale as enterprise data assets; treating the products of ML and AI as first-class analytic processes in the enterprise. With ClearScape Analytics, data scientists can use their preferred language, tools and platform to develop models to identify this fraud. Even in large scale operations, users have the guarantee that Vantage can scale to their needs and reduce fraud.</p>\n",
    "    \n",
    "<p style = 'font-size:20px;font-family:Arial'><b>Overview</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "    This notebook demonstrates an end-to-end Financial Fraud Detection workflow using Teradata ClearScape Analytics,\n",
    "enhanced with the Enterprise Feature Store (EFS) lineage management for <b>Week 1</b> and <b>Week 2</b> datasets.\n",
    "It showcases feature engineering, model training, scoring, and evaluation using Vantage In-DB capabilities.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Below are the steps involved in traditional approach:</p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li><b>Prepare data: </b>ClearScape Analytics offers highly optimized in-database functions for data preparation, minimizing data movement and enabling the enterprise feature store.</li>\n",
    "    <li><b>Train models: </b>ClearScape Analytics provides vertical and horizontal scaling capabilities that make it possible to efficiently train any number of models — from a few to a few million.</li>\n",
    "    <li><b>Deploy models: </b>ClearScape Analytics integrates model scoring with business data, both in real time and \u000b",
    "batch scoring, for effective operationalization and automated monitoring of AI models.</li>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'>In the traditional approach mentioned, we will follow the below steps:</p>    \n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Data Collection</li>\n",
    "    <li>Data Exploration</li>\n",
    "    <li>Enterprise feature store</li>\n",
    "    <li>Data Preparation using widgets</li>\n",
    "    <li>Model Training(2-3 different models)</li>\n",
    "    <li>Model Evaluation using ROC and Confusion Matrix</li>\n",
    "    <li>Best performing model</li>\n",
    "    <li>Model Scoring using best model</li>\n",
    "    <li>Operationalize Model using ModelOps</li>\n",
    "</ul>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Why Vantage?</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>To maximize the business value of advanced analytic techniques including Machine Learning and Artificial Intelligence, it is estimated that organizations must scale their model development and deployment pipelines to 100s or 1000s of times greater amounts of data, models, or both.</p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>1. Configuring the Environment</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install teradataml --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial;'><b>Note: </b><i>Please execute the above pip install to get the latest version of the required library. Be sure to restart the kernel after executing those lines to bring the installed libraries into memory. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import getpass\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Teradata Libraries\n",
    "from teradataml import *\n",
    "\n",
    "# Configuration\n",
    "spacing_large = \" \"*95\n",
    "spacing_small = \" \"*12\n",
    "display.max_rows = 5\n",
    "configure.val_install_location = 'val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>2. Connect to Vantage</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run -i ../../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username = 'demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql(\"SET query_band='DEMO=EE_week1andweek2.ipynb;' UPDATE FOR SESSION;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We begin running steps with Shift + Enter keys. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo on cloud storage. We have the option of either running the demo using foreign tables to access the data without using any storage on our environment or downloading the data to local storage, which may yield somewhat faster execution. However, we need to consider available storage. There are two statements in the following cell, and one is commented out. We may switch which mode we choose by changing the comment string.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ../run_procedure.py \"call get_data('DEMO_GLM_Fraud_cloud');\"        # Takes 1 minute\n",
    "%run -i ../../run_procedure.py \"call get_data('DEMO_GLM_Fraud_local');\"        # Takes 2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Optional step – We should execute the below step only if we want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../run_procedure.py \"call space_report();\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>3. Data Exploration</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We loaded the data from <a href = 'https://www.kaggle.com/code/georgepothur/4-financial-fraud-detection-xgboost/data'>https://www.kaggle.com/code/georgepothur/4-financial-fraud-detection-xgboost/data</a> into Vantage in a table named \"transaction_data\". We checked the data size and printed sample rows: 63k rows and 12 columns.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'><b><i>*Please scroll down to the end of the notebook for detailed column descriptions of the dataset.</i></b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txn_data = DataFrame(in_schema('DEMO_GLM_Fraud', 'transaction_data'))\n",
    "# txn_data = DataFrame(in_schema('demo_user', 'transaction_data'))\n",
    "print(txn_data.shape)\n",
    "txn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In this simulated scenario, deceptive agents engage in transactions with the objective of taking control of customers' accounts, transferring funds to another account, and ultimately cashing out for profit.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>3.1 How many fraudulent transactions do we have in our dataset?</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 92 fraud transactions i.e. 0.14% of fraud transactions in the dataset.\n",
    "print(\"No of fraud transactions: %d\\nPercentage of fraud transactions: %.2f%%\"%(\n",
    "    txn_data.loc[txn_data.isFraud == 1].shape[0],\n",
    "    txn_data.loc[txn_data.isFraud == 1].shape[0]/txn_data.shape[0]*100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.2 How many transactions do we have group by transaction type?</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for fraud transactions and group by 'type'\n",
    "transactions_by_type = txn_data.groupby('type').count().get(['type','count_txn_id'])\n",
    "\n",
    "\n",
    "# Sort by 'count_step' column in descending order\n",
    "transactions_by_type = transactions_by_type.sort('count_txn_id', ascending = False)\n",
    "\n",
    "transactions_by_type = transactions_by_type.assign(\n",
    "    type_int = case([\n",
    "        (transactions_by_type.type == 'CASH_IN', 0),\n",
    "        (transactions_by_type.type == 'CASH_OUT', 1),\n",
    "        (transactions_by_type.type == 'DEBIT', 2),\n",
    "        (transactions_by_type.type == 'PAYMENT ', 3),\n",
    "        (transactions_by_type.type == 'TRANSFER', 4),\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_by_type.plot(\n",
    "    x = transactions_by_type.type_int,\n",
    "    y = transactions_by_type.count_txn_id,\n",
    "    kind = 'bar',\n",
    "    legend = ['Count by Type'],\n",
    "    ylabel = 'Count of Transactions',\n",
    "    xlabel = spacing_small.join(sorted(list(transactions_by_type[['type']].get_values().flatten()))),\n",
    "    title = \"Number of Transactions per Transaction Type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.3 How many fraudulent transactions do we have group by transaction type?</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter data for fraud transactions and group by 'type'\n",
    "fraud_transactions_by_type = txn_data.loc[txn_data.isFraud == 1].groupby('type').count().get(['type','count_txn_id'])\n",
    "\n",
    "# Sort by 'count_step' column in descending order\n",
    "fraud_transactions_by_type = fraud_transactions_by_type.sort('count_txn_id', ascending = False)\n",
    "\n",
    "fraud_transactions_by_type = fraud_transactions_by_type.assign(\n",
    "    total_fraud = txn_data.loc[txn_data.isFraud == 1].shape[0],\n",
    "    type_int = case([(fraud_transactions_by_type.type == 'TRANSFER', 0)], else_ = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fraud_transactions_by_type.plot(\n",
    "    x = fraud_transactions_by_type.type_int,\n",
    "    y = [fraud_transactions_by_type.total_fraud, fraud_transactions_by_type.count_txn_id],\n",
    "    kind = 'bar',\n",
    "    figsize = (800, 500),\n",
    "    legend = ['Total Fraud', 'Count by Type'],\n",
    "    ylabel = 'Count of Fraud Transactions',\n",
    "    xlabel = 'TRANSFER' + spacing_large + 'CASH_OUT',\n",
    "    title = \"Number of Fraud Transactions by Transaction Type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>From the above result, we can see that out of the 92 fraud transactions, 47 are from transaction type \"TRANSFER\" and 45 are from \"CASH_OUT\".</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.4 What percentage of fraudulent transactions do we have where transaction amount is equal to old balance in the origin account?</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This might be the case where the fraudster emptied the account of the victim.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of cleanout fraud transactions: %d\\nPercentage of cleanout fraud transactions: %.2f%%\"%(\n",
    "    txn_data.loc[txn_data['amount'] == txn_data.oldbalanceOrig].loc[txn_data['isFraud'] == 1].shape[0],\n",
    "    txn_data.loc[txn_data['amount'] == txn_data.oldbalanceOrig].loc[txn_data['isFraud'] == 1].shape[0] / txn_data.loc[txn_data.isFraud == 1].shape[0]*100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>From the above result, we can see that out of 92 Fraud transactions, the amount involved in 90 fraud transactions was equal to the total balance in the account. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><b>Below are some insights about the dataset:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>We have 92 fraud transactions, which account for 0.14% of the dataset.</li>\n",
    "    <li>Out of these 92 fraud transactions, 47 are of type TRANSFER, and 45 are of type CASH_OUT.</li>\n",
    "    <li>Approximately 97.83% of our fraud transactions have a transaction amount equal to oldbalanceOrig, indicating account cleanout.</li>\n",
    "    <li>About 71.74% of our fraud transactions have the recipient's old balance as zero.</li>\n",
    "    <li>The isFlaggedFraud indicator is correct only two times among our 92 fraud transactions.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.5 Univariate statistics</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The describe function computes the count, mean, std, min, percentiles, and max for numeric columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.6 Checking for Null Values</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The ColumnSummary() function can be used to take a quick look at the columns, their datatypes, and summary of NULLs/non-NULLs for a given table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsum = ColumnSummary(\n",
    "    data  = txn_data,\n",
    "    target_columns = [':']\n",
    ")\n",
    "colsum.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b style='font-size:18px;font-family:Arial'>3.7 Week-wise Dataset Split</b>\n",
    "<p style='font-size:16px;font-family:Arial'>\n",
    "The dataset is divided into <b>Week 1</b> and <b>Week 2</b> for separate feature lineage tracking.\n",
    "This simulates time-based feature evolution in financial systems.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_result = txn_data.sample(frac=[0.3, 0.7], seed=42, id_column=\"txn_id\")\n",
    "\n",
    "week1_df = split_result.loc[split_result.sampleid == 1]\n",
    "week2_df = split_result.loc[split_result.sampleid == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>\n",
    "Week 1 and Week 2 datasets represent sequential transactional windows.\n",
    "Each set is used to derive different sets of engineered features within the EFS.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Week1:\", week1_df.shape)\n",
    "print(\"Week2:\", week2_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style = 'font-size:20px;font-family:Arial'>4. Feature Engineering</b>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Teradata Enterprise Feature Store (EFS) Functions are designed to handle feature management within the Vantage environment. While inspired by the syntax of Feast, Teradata EFS Functions stands out, offering efficiency and robustness in data management and feature handling tailored specifically for the use of Teradata Vantage. Teradata EFS Functions use Teradata Dataframes for Feature management, to the contrary of the pandas dataframe of Feast. With Teradata Dataframes we avoid extracting the data to create or use Features from the Enterprise Feature Store (EFS). The EFS Functions are crafted to empower Data Science teams for effective and streamlined feature management. This notebook will walk you through the capabilities of EFS Functions, demonstrating how it integrates seamlessly with your data models and processes.</p>\n",
    "\n",
    "<br>\n",
    "<b style = 'font-size:18px;font-family:Arial'>4.1 Setup a Feature Store Repository</b>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>The Enterprise Feature Store (EFS) SDK is designed with a totally object-oriented approach, focusing on intuitive interaction with feature stores. Central to this design are several core objects: Feature, Entity, DataSource, FeatureGroup. Together, these objects facilitate the efficient management and utilization of features within your data ecosystem, leveraging Teradata Vantage for metadata storage.</p>\n",
    "<p style='font-size:16px;font-family:Arial'>A feature store repository serves as the foundational environment for storing and managing your data features. The owner of the FeatureStore can grant/revoke read only, write only or read and write authorization to other user(s)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'> Create the Feature Store</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureStore(repo=\"financial_fraud\", data_domain=\"transaction_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'> Setup the Feature Store</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureStore(repo=\"financial_fraud\", data_domain=\"transaction_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List whether FeatureStore is setup or not.\n",
    "fs.list_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style='font-size:18px;font-family:Arial'><b>4.2 Week 1 Features: Foundational Transaction Behavior</b></p>\n",
    "<p style='font-size:16px;font-family:Arial'>\n",
    "Week 1 focuses on the <b>origin account’s behavior</b> — how much money is moving out and how the balances change over time.\n",
    "These features establish the baseline of a sender’s transaction patterns, helping to detect anomalies in withdrawal and transfer patterns.\n",
    "</p>\n",
    "\n",
    "<ul style='font-size:16px;font-family:Arial'>\n",
    "  <li><b>step</b>: Represents the transaction’s time sequence. It helps detect whether frauds occur at specific time intervals or bursts, which is common in coordinated attacks.</li>\n",
    "  <li><b>oldbalanceOrig</b>: The balance in the origin (sender) account before the transaction. A sudden or total depletion of this balance is a strong fraud indicator.</li>\n",
    "  <li><b>newbalanceOrig</b>: The balance in the origin account after the transaction. Comparing old and new balances helps quantify the transaction magnitude and detect “cleanout” patterns (when the entire account is emptied).</li>\n",
    "</ul>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>\n",
    "In summary, Week 1 features define <b>“how the sender behaves.”</b> These features are crucial for early anomaly detection, since many fraudulent actions start with abnormal outgoing transactions or depleted source accounts.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Features\n",
    "week1_df = week1_df.assign(\n",
    "    \n",
    "    step = week1_df.step,\n",
    "    newbalanceOrig = week1_df.newbalanceOrig,\n",
    "    oldbalanceOrig = week1_df.oldbalanceOrig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'> We are storing the transformation here. So, even if underlying data varies, the data transformation steps remain same.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view\n",
    "week1_view = week1_df.create_view(\"fraud_week1_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>Creating the FeatureProcess </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp1 = FeatureProcess(\n",
    "    repo=\"financial_fraud\",\n",
    "    data_domain=\"transaction_features\",\n",
    "    object=week1_view,\n",
    "    entity=\"txn_id\",\n",
    "    features=[\n",
    "        \"step\",\n",
    "        \"newbalanceOrig\",\n",
    "        \"oldbalanceOrig\"\n",
    "    ],\n",
    "    description=\"Week1 features capturing balance and transaction ratios\"\n",
    ")\n",
    "fp1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the feature process.\n",
    "fs.list_feature_processes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>We ingested three features <code>step</code>, <code>newbalanceOrig</code>, and <code>oldbalanceOrig </code> from a single feature process. This demonstrates how multiple related features can be managed and tracked together within the feature store, maintaining their lineage to the originating process.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.mind_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style='font-size:18px;font-family:Arial'><b>4.3 Week 2 Features: Downstream Transaction Linkage</b></p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>\n",
    "Week 2 expands the feature lineage to include <b>destination and relational features</b>, capturing how funds propagate through the system.\n",
    "Fraudulent activity often involves multiple accounts where money is transferred and quickly moved again to obscure its source.\n",
    "</p>\n",
    "\n",
    "<ul style='font-size:16px;font-family:Arial'>\n",
    "  <li><b>nameOrig</b>: Encodes the originating account ID. Used for entity mapping and relationship tracking across transactions.</li>\n",
    "  <li><b>nameDest</b>: The destination account ID. Monitoring repeated destinations helps detect mule accounts or hubs of fraudulent transfers.</li>\n",
    "  <li><b>oldbalanceDest</b>: The destination account’s balance before the transaction. If it’s zero, it could indicate a newly created or rarely used account used for fraudulent inflow.</li>\n",
    "  <li><b>newbalanceDest</b>: The destination account’s balance after the transaction. Large sudden increases can indicate funds consolidation or laundering behavior.</li>\n",
    "</ul>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>\n",
    "Together, these features characterize <b>“how the money flows through the network.”</b> This second lineage adds situational awareness, helping the model learn destination-based patterns such as zero-balance inflows, cascading transfers, and dormant account activations - all red flags for financial fraud.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week2_df = week2_df.assign(\n",
    "    \n",
    "    step = week2_df.step,\n",
    "    newbalanceOrig = week2_df.newbalanceOrig,\n",
    "    oldbalanceOrig = week2_df.oldbalanceOrig\n",
    ")\n",
    "week2_df = week2_df.assign(\n",
    "    nameOrig = week2_df.nameOrig,\n",
    "    nameDest =  week2_df.nameDest,\n",
    "    oldbalanceDest = week2_df.oldbalanceDest,\n",
    "    newbalanceDest = week2_df.newbalanceDest\n",
    ")\n",
    "\n",
    "week2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'> We are storing the transformation here. So, even if underlying data varies, the data transformation steps remain same.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view\n",
    "week2_view = week2_df.create_view(\"fraud_week2_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>Creating the FeatureProcess </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp2 = FeatureProcess(\n",
    "    repo=\"financial_fraud\",\n",
    "    data_domain=\"transaction_features\",\n",
    "    object=week2_view,\n",
    "    entity=\"txn_id\",\n",
    "    features=[\n",
    "        \"step\",\n",
    "        \"newbalanceOrig\",\n",
    "        \"oldbalanceOrig\",\n",
    "        \"nameOrig\",\n",
    "        \"nameDest\",\n",
    "        \"oldbalanceDest\",\n",
    "        \"newbalanceDest\"\n",
    "    ],\n",
    "    description=\"Week2 features detecting zero balance and full account cleanouts\"\n",
    ")\n",
    "fp2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the feature process.\n",
    "fs.list_feature_processes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.mind_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b style='font-size:18px;font-family:Arial'>4.4 Building a Curated Dataset</b>\n",
    "<p style='font-size:16px;font-family:Arial'>\n",
    "Both Week 1 and Week 2 feature processes are merged into a curated dataset\n",
    "that serves as the foundation for subsequent modeling.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_customer = fs.get_dataset_catalog()\n",
    "\n",
    "selected_profile_features = {\n",
    "        'step': fp2.process_id,\n",
    "        'newbalanceOrig': fp2.process_id,\n",
    "        'oldbalanceOrig': fp2.process_id,\n",
    "        'nameOrig': fp2.process_id,\n",
    "        'nameDest': fp2.process_id,\n",
    "        'oldbalanceDest': fp2.process_id,\n",
    "        'newbalanceDest': fp2.process_id\n",
    "    }\n",
    "\n",
    "tdf = dc_customer.build_dataset(entity='txn_id',\n",
    "                          selected_features=selected_profile_features,\n",
    "                          view_name=\"fraud_dataset\",\n",
    "                          description=\"Curated dataset for financial fraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.list_dataset_catalogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.mind_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b style='font-size:18px;font-family:Arial'>4.5 Explore the FeatureStore</b>\n",
    "<p style='font-size:16px;font-family:Arial'>\n",
    "The <b>Feature Catalog</b> acts as \n",
    "a centralized registry for all feature definitions within a given repository and data domain. \n",
    "It allows users to efficiently organize, search, and retrieve reusable features for various analytical and machine learning workflows.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FeatureCatalog(repo=\"financial_fraud\",\n",
    "                    data_domain=\"transaction_features\")\n",
    "fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>The <code>list_feature_versions()</code> function provides visibility into this versioning history, enabling precise tracking of feature evolution over time.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ver = fc.list_feature_versions()\n",
    "f_ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>\n",
    "After registering and versioning the features within the Enterprise Feature Store (<b>EFS</b>), \n",
    "each feature process automatically creates a corresponding physical table in the underlying repository schema. \n",
    "These tables follow a standardized naming convention such as \n",
    "<code>FS_T_&lt;UUID&gt;</code>, where the unique identifier corresponds to a specific <b>FeatureProcess</b>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = DataFrame(in_schema('financial_fraud', 'FS_T_0b9c6f83_0f98_78ec_e504_ec0a5017380b'))\n",
    "feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>The <code>features</code> property of the feature catalog lists all features currently registered in the catalog.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>The <code>data_domain</code> property shows the business domain associated with the feature catalog.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.data_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>\n",
    "The <code>list_features()</code> function provides a detailed view of all features ingested within the current repository, \n",
    "including their names, data domains, data types, and the originating <b>FeatureProcess</b> ID. \n",
    "This serves as a quick validation checkpoint to confirm the features available for modeling.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Features\n",
    "fc.list_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>\n",
    "The <code>list_feature_catalogs()</code> function, gives an overview of all curated feature collections (catalogs) \n",
    "that have been built using one or more feature processes. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List feature_catalogs\n",
    "fs.list_feature_catalogs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>\n",
    "After constructing the curated feature dataset from the Enterprise Feature Store (<b>EFS</b>), \n",
    "the next step is to merge these engineered features with selected raw transactional attributes from the original dataset. \n",
    "This ensures that the model benefits from both derived and native variables, enriching the learning context.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with features with the original table\n",
    "final_df = tdf.join(\n",
    "    txn_data.select(['txn_id','type','amount','isFraud','isFlaggedFraud']),\n",
    "    on='txn_id',\n",
    "    lprefix=\"f\",\n",
    "    rprefix=\"t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming txn_id after joining\n",
    "modeling_df = final_df.assign(\n",
    "    txn_id = final_df.f_txn_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>5. Data Preparation using widgets</b>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'><b>We'll perform the following steps:</b></p>\n",
    "<ul style='font-size:16px;font-family:Arial'>\n",
    "    <li>We will one-hot encode the categorical \"type\" column.</li>\n",
    "    <li>We will perform feature scaling using ScaleFit and ScaleTransform on numerical columns.</li>\n",
    "    <li>We will split the data into training and testing datasets (80:20 split).</li>\n",
    "</ul>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>We perform feature scaling during data pre-processing to handle highly varying magnitudes, values, or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values higher and consider smaller values as lower ones, regardless of the unit of the values.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.1 Drop redundant columns</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We don't need nameDest, nameOrigin, isFlaggedFraud, f_txn_id and t_txn_id for model training as they do not impact the outcome. We have txn_id to uniquely identify each transaction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns\n",
    "txn_data_df = modeling_df.drop(['nameDest', 'nameOrig', 'isFlaggedFraud', 'f_txn_id', 't_txn_id'], axis = 1)\n",
    "txn_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will copy this dataset to a Vantage table to be used in widgets for Data Preparation</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(txn_data_df, table_name='transaction_data_new', if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.2 Checking for Outliers using widgets</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The OutlierFilterFit() function calculates the lower percentile, upper percentile, count of rows and median for all the \"target_columns\" provided by the user. These metrics for each column help the function OutlierTransform() detect outliers in data.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here we are using teradataml syntax for the function. The same can be achived using the following SQL as well.</p>\n",
    "\n",
    "<code>SELECT * FROM TD_OutlierFilterFit(\n",
    "    ON \"DEMO_GLM_Fraud\".\"transaction_data\" AS InputTable\n",
    "    OUT TABLE OutputTable(\"DEMO_USER\".\"Outlier_output\")\n",
    "    USING\n",
    "    TargetColumns('amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig')\n",
    ") as dt;</code>\n",
    "\n",
    "<p style = 'font-size:14px;font-family:Arial'><b><i>*Please note that both the versions run in-database and there is no data transfer involved.</i></b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from teradatamlwidgets.analytic_functions.Ui import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>As we are showcasing widgets here, we will have to follow some specific steps to execute this function.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'><b>After we execute the below cell it will showcase the widgets output where you will have to enter the below details </b></p>\n",
    "    <li style='font-size:16px;font-family:Arial'>Select a functions: Please select <b>OutlierFilterFit</b></li>\n",
    "    \n",
    "<p style='font-size:16px;font-family:Arial'>After select the function we will see the widget for entering the Inputs we need to provide for the function. Below are the inputs to be provided</p> \n",
    "    <li style='font-size:16px;font-family:Arial'><code>InputTable:</code> Select <b>transaction_data_new</b></li>\n",
    "    <li style='font-size:16px;font-family:Arial'><code>TargetColumns:</code> Please select below columns 1 in each box <b>'amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig'</b></li>\n",
    "</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>After doing this click on the <b>Execute</b> button and check the output</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"transaction_data\",\"transaction_data_new\"]\n",
    "outputs = [\"outlierfit_table\"]\n",
    "ui = Ui(\n",
    "    outputs = outputs,\n",
    "    inputs = inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>After OutlierFilterFit we will execute the OutlierFilterTransform function.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'><b>After we execute the below cell it will showcase the widgets output where you will have to enter the below details </b></p>\n",
    "    <li style='font-size:16px;font-family:Arial'>Select a functions: Please select <b>OutlierFilterTransform</b></li>\n",
    "    \n",
    "<p style='font-size:16px;font-family:Arial'>After select the function we will see the widget for entering the Inputs we need to provide for the function. Below are the inputs to be provided</p> \n",
    "    <li style='font-size:16px;font-family:Arial'><code>InputTable:</code> Select <b>transaction_data_new</b></li>\n",
    "    <li style='font-size:16px;font-family:Arial'><code>FitTable:</code> Please select <b> outlierfit_table</b></li>\n",
    "</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>After doing this click on the <b>Execute</b> button and check the output</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['transaction_data_new','outlierfit_table']\n",
    "outputs = ['outlier_trans_table']\n",
    "ui = Ui(\n",
    "    outputs = outputs,\n",
    "    inputs = inputs,\n",
    "    function=\"OutlierFilterTransform\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = DataFrame('outlier_trans_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rows before removing outliers: {txn_data_df.shape[0]}\\n\\\n",
    "Rows after removing outliers: {res.shape[0]}\\n\\\n",
    "Total outliers: {txn_data_df.shape[0] - res.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers = td_minus([txn_data_df, res])\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.3 One-hot encoding</b></p>\n",
    "<p style='font-size:16px;font-family:Arial'>\n",
    "Here, we are one-hot encoding the \"type\" column. We find one-hot encoding necessary in many cases to represent categorical variables as binary values, enable numerical processing, ensure feature independence, handle non-numeric data, and improve the performance and interpretability of our machine learning models.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_type_encoder = OneHotEncoder(\n",
    "    values = [\"CASH_IN\", \"CASH_OUT\", \"DEBIT\", \"PAYMENT\", \"TRANSFER\"],\n",
    "    columns = \"type\"\n",
    ")\n",
    "\n",
    "retain = Retain(\n",
    "    columns = ['step', 'amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig', 'isFraud']\n",
    ")\n",
    "\n",
    "obj = valib.Transform(\n",
    "    data = txn_data,\n",
    "    one_hot_encode = txn_type_encoder,\n",
    "    retain = retain,\n",
    "    index_columns = 'txn_id'\n",
    ")\n",
    "txn_trans = obj.result\n",
    "txn_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_trans = txn_trans.assign(isFraud=txn_trans.isFraud.cast(type_=INTEGER()))\n",
    "txn_trans                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above output shows that we have transformed the data into a transfromed dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(txn_trans, table_name = 'clean_data', if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>6. Create training and testing datasets in Vantage</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We'll create two datasets for training and testing in the ratio of 80:20.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TrainTestSplit_out = TrainTestSplit(\n",
    "    data = txn_trans,\n",
    "    id_column = \"txn_id\",\n",
    "    train_size = 0.80,\n",
    "    test_size = 0.20,\n",
    "    seed = 25\n",
    ")\n",
    "\n",
    "df_train = TrainTestSplit_out.result[TrainTestSplit_out.result['TD_IsTrainRow'] == 1].drop(['TD_IsTrainRow'], axis = 1)\n",
    "df_test = TrainTestSplit_out.result[TrainTestSplit_out.result['TD_IsTrainRow'] == 0].drop(['TD_IsTrainRow'], axis = 1)\n",
    "\n",
    "print(\"Training Set = \" + str(df_train.shape[0]) + \". Testing Set = \" + str(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "copy_to_sql(df_train, table_name = 'clean_data_train', if_exists = 'replace')\n",
    "copy_to_sql(df_test, table_name = 'clean_data_test', if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The above output shows that we have transformed the data into a scaled dataset. Scaling our data makes it easy for our model to learn and understand the problem.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>7. In-Database model training</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>ClearScape Analytics provides vertical and horizontal scaling capabilities that make it possible to efficiently train any number of models — from a few to a few million. With a variety of model tarining functions we can build and train high quality ML models, which leverages the analytic datasets. These set of ML components and capabilities help to reduce effort, lower costs, and get ML models into production as quickly as possible</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we will be using In-DB XGBoost and DecisionForest functions to train our models.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b style = 'font-size:18px;font-family:Arial'>7.1 XGBoost model training</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The XGBoost() function, also known as eXtreme Gradient Boosting, is an implementation of the gradient boosted decision tree algorithm designed for speed and performance. It has recently been dominating applied machine learning.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In gradient boosting, each iteration fits a model to the residuals (errors) of the previous iteration to correct the errors made by existing models. The predicted residual is multiplied by this learning rate and then added to the previous prediction. Models are added sequentially until no further improvements can be made. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here we are using teradataml syntax for the function. The same can be achived using the following SQL as well.</p>\n",
    "\n",
    "<code>SELECT * FROM TD_XGBoost(\n",
    "\tON \"DEMO_USER\".\"clean_data_train\" AS \"input\"\n",
    "\tPARTITION BY ANY\n",
    "\tUSING InputColumns('amount','newbalanceOrig','oldbalanceDest','newbalanceDest','oldbalanceOrig','CASH_IN_type','CASH_OUT_type','DEBIT_type','PAYMENT_type','TRANSFER_type')\n",
    "\tResponseColumn('isFraud')\n",
    "\tMaxDepth(7)\n",
    "\tSeed(42)\n",
    "\tModelType('Classification')\n",
    "\tRegularizationLambda(120.0)\n",
    "\tShrinkageFactor(0.1)\n",
    ") as sqlmr</code>\n",
    "\n",
    "<p style = 'font-size:14px;font-family:Arial'><b><i>*Please note that both the versions run in-database and there is no data transfer involved.</i></b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = df_train.columns\n",
    "cols.remove('txn_id')\n",
    "cols.remove('step')\n",
    "cols.remove('isFraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XGBoost_out = XGBoost(\n",
    "    data=df_train,\n",
    "    input_columns=cols,\n",
    "    response_column = 'isFraud',\n",
    "    lambda1 = 120.0,\n",
    "    model_type='Classification',\n",
    "    seed=42,\n",
    "    shrinkage_factor=0.1,\n",
    "    max_depth=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XGBoost_out.output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The function output is a trained XGBoost model, and we can input it to the XGBoostPredict() function for prediction.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>7.2 Decision Forest model Training</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The <a href = 'https://docs.teradata.com/search/all?query=TD_DecisionForest&content-lang=en-US'>DecisionForest</a> model function is an ensemble algorithm used for classification and regression predictive modeling problems. It is an extension of bootstrap aggregation (bagging) of decision trees. Typically, constructing a decision tree involves evaluating the value for each input feature in the data to select a split point. The function reduces the features to a random subset (that can be considered at each split point); the algorithm can force each decision tree in the forest to be very different to improve prediction accuracy. </p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This function takes the training data as input, as well as the following function parameters</p>\n",
    "    <ul style = 'font-size:16px;font-family:Arial'>\n",
    "        <li style = 'font-size:16px;font-family:Arial'>InputColumns; list or range of columns used as features (we used an ordinal reference of columns 2:217)</li>\n",
    "        <li style = 'font-size:16px;font-family:Arial'>ResponseColumn; the dependent or target value (we used “class”, the first column)</li>\n",
    "        <li style = 'font-size:16px;font-family:Arial'>TreeType; either CLASSIFICATION or REGRESSION</li>\n",
    "    <li style = 'font-size:16px;font-family:Arial'>Other hyperparameter values detailed in the documentation</li>\n",
    "        </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_new = DataFrame('clean_data_train')\n",
    "df_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionForest_out = DecisionForest(data = df_train_new, \n",
    "                            input_columns = cols, \n",
    "                            response_column = 'isFraud', \n",
    "                            max_depth = 16, \n",
    "                            num_trees = 8, \n",
    "                            min_node_size = 1, \n",
    "                            mtry = 3, \n",
    "                            mtry_seed = 3, \n",
    "                            seed = 3, \n",
    "                            tree_type = 'CLASSIFICATION')\n",
    "# Print the result DataFrame.\n",
    "DecisionForest_out.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>8. In-Database model scoring</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>ClearScape Analytics integrates model scoring with business data, both in real time and batch scoring, for effective operationalization and automated monitoring of AI models.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>By deploying these models to conduct live data scoring, Vantage delivers the crucial insights needed to drive business outcomes</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b style = 'font-size:18px;font-family:Arial'>8.1 XGBoost model scoring</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The XGBoostPredict() function runs the predictive algorithm based on the model generated by XGBoost(). The XGBoost() function, also known as eXtreme Gradient Boosting, performs classification or regression analysis on datasets.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XGBoostPredict_out = XGBoostPredict(\n",
    "    newdata=df_test,\n",
    "    object=XGBoost_out.result,\n",
    "    model_type='Classification',\n",
    "    id_column='txn_id',\n",
    "    object_order_column=['task_index', 'tree_num',\n",
    "                       'iter', 'tree_order'],\n",
    "    accumulate='isFraud',\n",
    "    output_prob=True,\n",
    "    output_responses=['0', '1']\n",
    ").result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XGBoostPredict_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The output above shows our prob_1, i.e., the transaction is fraud, and prob_0, i.e., the transaction is not a fraud. We use these probabilities in our prediction column to assign a class label.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b style = 'font-size:18px;font-family:Arial'>8.2 Decision Forest model scoring</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The DecisionForestPredict() function uses the model generated by the DecisionForest() function to generate predictions on a response variable for a test set of data. The model can be stored in either a teradataml DataFrame or a DecisionForest object.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_new = DataFrame('clean_data_test')\n",
    "df_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_forest_predict_out = TDDecisionForestPredict(object = DecisionForest_out.result,\n",
    "                                                        newdata = df_test_new,\n",
    "                                                        id_column = \"txn_id\",\n",
    "                                                        detailed = False,\n",
    "                                                        output_prob = True,\n",
    "                                                        output_responses = ['0','1'],\n",
    "                                                        accumulate = 'isFraud')\n",
    "df_predict = decision_forest_predict_out.result\n",
    "df_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>9. In-Database model evaluation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>ClearScape Analytics model evaluation report highlights the model performance in the form of certain metrics and compare models based on the metric values.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The ClassificationEvaluator() function evaluate and emits various metrics of classification model based on its predictions on the data. Apart from accuracy, the secondary output data returns micro, macro, and weighted-averaged metrics of precision, recall, and F1-score values.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict_char = df = df_predict.assign(isFraud = df_predict.isFraud.cast(type_=VARCHAR(2))\n",
    "                                        ,prediction_ch = df_predict.prediction.cast(type_=VARCHAR(2)))\n",
    "df_predict_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df = df_test.join(XGBoostPredict_out, on='txn_id', lsuffix='test', rsuffix='pred')\n",
    "combined_df[combined_df['Prediction']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = XGBoostPredict_out.assign(Prediction = XGBoostPredict_out.Prediction.cast(type_ = BYTEINT))\n",
    "out = out.assign(Prediction = out.Prediction.cast(type_ = VARCHAR(2)))\n",
    "out = out.assign(isFraud = out.isFraud.cast(type_ = VARCHAR(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ClassificationEvaluator_obj = ClassificationEvaluator(\n",
    "    data = out,\n",
    "    observation_column = 'isFraud',\n",
    "    prediction_column = 'Prediction',\n",
    "    labels = ['0', '1']\n",
    ")\n",
    "xgb_eval = ClassificationEvaluator_obj.output_data\n",
    "xgb_eval.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ClassificationEvaluator_obj = ClassificationEvaluator(\n",
    "    data = df_predict_char,\n",
    "    observation_column = 'isFraud',\n",
    "    prediction_column = 'prediction_ch',\n",
    "    labels = ['0', '1']\n",
    ")\n",
    "df_eval = ClassificationEvaluator_obj.output_data\n",
    "df_eval.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>10. Visualize the results (ROC curve and AUC)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We create the ROC curve, which is a graph between TPR (True Positive Rate) and FPR (False Positive Rate). We use the area under the ROC curve as a metric to evaluate how well our model can distinguish between positive and negative classes. A higher AUC indicates better performance in distinguishing between the positive and negative categories. We generally consider an AUC above 0.75 as decent.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from teradataml import ROC\n",
    "\n",
    "xgb_roc_out = ROC(\n",
    "    probability_column = '\"Prob_1\"',\n",
    "    observation_column = \"isFraud\",\n",
    "    positive_class = \"1\",\n",
    "    data = XGBoostPredict_out,\n",
    "    num_thresholds=300\n",
    ")\n",
    "\n",
    "xgb_roc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from teradataml import ROC\n",
    "\n",
    "df_roc_out = ROC(\n",
    "    probability_column = '\"prob_1\"',\n",
    "    observation_column = \"isFraud\",\n",
    "    positive_class = \"1\",\n",
    "    data = df_predict,\n",
    "    num_thresholds=300\n",
    ")\n",
    "\n",
    "df_roc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assigning new index column\n",
    "xgb_roc_out.result = xgb_roc_out.result.assign(row = 1)\n",
    "# Changing the index label.\n",
    "xgb_roc_out.result._index_label = [\"row\"]\n",
    "xgb_auc = xgb_roc_out.result.get_values()[0][0]\n",
    "\n",
    "df_roc_out.result = df_roc_out.result.assign(row = 1)\n",
    "# Changing the index label.\n",
    "df_roc_out.result._index_label = [\"row\"]\n",
    "df_auc = df_roc_out.result.get_values()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from teradataml import subplots\n",
    "fig, axis = subplots(1, 2)\n",
    "fig.height, fig.width = 600,1200\n",
    "plot = xgb_roc_out.output_data.plot(x=xgb_roc_out.output_data.fpr,\n",
    "    y=[xgb_roc_out.output_data.tpr, xgb_roc_out.output_data.fpr],\n",
    "    xlabel='False Positive Rate',\n",
    "    ylabel='True Positive Rate',\n",
    "    figure=fig,\n",
    "    ax=axis[0],\n",
    "    color='carolina blue',\n",
    "    legend=[f'XGBoost AUC = {round(xgb_auc, 4)}', 'AUC Baseline'],\n",
    "    legend_style='lower right',\n",
    "    grid_linestyle='--',\n",
    "    grid_linewidth=0.5,\n",
    "    linestyle = ['-', '--'])\n",
    "\n",
    "plot = df_roc_out.output_data.plot(\n",
    "    x=df_roc_out.output_data.fpr,\n",
    "    y=[df_roc_out.output_data.tpr, df_roc_out.output_data.fpr],\n",
    "    xlabel='False Positive Rate',\n",
    "    ylabel='True Positive Rate',\n",
    "    figure=fig,\n",
    "    ax=axis[1],\n",
    "    color='carolina blue',\n",
    "    legend=[f'DecisionForest AUC = {round(df_auc, 4)}', 'AUC Baseline'],\n",
    "    legend_style='lower right',\n",
    "    grid_linestyle='--',\n",
    "    grid_linewidth=0.5,\n",
    "    linestyle = ['-', '--']\n",
    ")\n",
    "\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Looking at the above ROC Curve, we can confidently say that our model has performed well on testing data. The AUC value is above 0.75 and resonates with our understanding that the model is performing well.</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Conclusion</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demonstration, we have illustrated a simplified - but complete - overview of how we can implement a typical machine learning workflow completely inside the database using Vantage. This allows us to leverage Vantage's operational scale, power, and stability.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>11. ModelOps for Financial Fraud Detection</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We used feature store to store features as well as its processing. We re-used it in model training. The features and processing can be re-used accross multiple machine learning models and use-case , helping to improve data science productivity</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Teradata's traditional approach using Clearscape Analytic functions play a crucial role in this context by automating the complex process of building and deploying machine learning models. Various Clearscape Analytic functions are used for optimal preparation and training of models, delivering high-quality machine learning models in minutes. With the capabilities of ClearScape Analytics ModelOps, Analytics-driven organizations can follow a mature methodology and automated capabilities to solve this gap and make efficient model operationalization at Scale in Vantage.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>ClearScape Analytics ModelOps manages the operationalization of advanced analytics in Teradata Vantage providing Deployment, Governance and Monitoring of your AI/ML models at scale. ModelOps provides an easy-to-use web-based user interface (UI), a command line interface (CLI) and Python/R Software Development Kit (SDK).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>As a part of this End-to_End demo for Financial Fraud Detection, we will implement the ModelOps cycle using Vantage In-Db Vantage functions. Click the below button which will showcase the steps required for the ModelOps cycle as a part of Operatinilizing the model.</p>\n",
    "\n",
    "<a href=\"FinFraud_EndtoEnd_ModelOps_GIT_Python_indb_XGB.ipynb\" style=\"display: inline-flex; align-items: center; justify-content: center; background-color: #007373; color: #FFFFFF; font-family: Arial, sans-serif; font-size: 16px; font-weight: bold; text-decoration: none; padding: 12px 24px; border: none; border-radius: 8px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); cursor: pointer; transition: all 0.3s ease;\">\n",
    "  LAUNCH Notebook for ModelOps\n",
    "  <img src=\"https://img.icons8.com/ios-filled/50/ffffff/external-link.png\" alt=\"External Link Icon\" style=\"margin-left: 8px; width: 20px; height: 20px;\">\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>12. Cleanup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Note: </b>The tables created in this demo will be used in the ModelOps notebook which can be invoked on click of the above button. Please uncomment the below lines of code in case you do not want to run the ModelOps notebook and want to delete the tables created for this demo.</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We need to clean up our work tables to prevent errors next time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = [\"fraud_week1_view\", \"fraud_week2_view\"]\n",
    "for view in views:\n",
    "    try:\n",
    "        db_drop_view(view_name=view)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['clean_data', 'clean_data_train', 'clean_data_test']\n",
    "\n",
    "# Loop through the list of tables and execute the drop table command for each table\n",
    "for table in tables:\n",
    "    try:\n",
    "        db_drop_table(table_name = table)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'> <b>Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will use the following code to clean up tables and databases created for this demonstration.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../../run_procedure.py \"call remove_data('Demo_glm_fraud');\"        # Takes 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<b style = 'font-size:20px;font-family:Arial'>Required Materials</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Let’s look at the elements we have available for reference for this notebook:</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Filters:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li><b>Industry:</b> Finance</li>\n",
    "    <li><b>Functionality:</b> Machine Learning</li>\n",
    "    <li><b>Use Case:</b> Fraud Detection</li>\n",
    "</ul>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Related Resources:</b></p>\n",
    "\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li><a href='https://www.teradata.com/Blogs/Fraud-Busting-AI'>Fraud-Busting-AI</a></li>\n",
    "    <li><a href='https://www.teradata.com/Industries/Financial-Services'>Financial Services</a></li>\n",
    "    <li><a href='https://www.teradata.com/Resources/Datasheets/Move-from-Detection-to-Prevention-and-Outsmart-Fraudsters'>Move from Detection to Prevention and Outsmart Tech-Savvy Fraudsters</a></li>\n",
    "</ul>\n",
    "\n",
    "<b style = 'font-size:20px;font-family:Arial'>Dataset:</b>\n",
    "\n",
    "- `txn_id`: transaction id\n",
    "- `step`: maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (31 days simulation).\n",
    "- `type`: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER\n",
    "- `amount`: amount of the transaction in local currency\n",
    "- `nameOrig`: customer who started the transaction\n",
    "- `oldbalanceOrig`: customer's balance before the transaction\n",
    "- `newbalanceOrig`: customer's balance after the transaction\n",
    "- `nameDest`: customer who is the recipient of the transaction\n",
    "- `oldbalanceDest`: recipient's balance before the transaction\n",
    "- `newbalanceDest`: recipient's balance after the transaction\n",
    "- `isFraud`: identifies a fraudulent transaction (1) and non fraudulent (0)\n",
    "- `isFlaggedFraud`: flags illegal attempts to transfer more than 200,000 in a single transaction\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Links:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Uses a dataset and feature discovery methods outlined here: <a href = 'https://www.kaggle.com/georgepothur/4-financial-fraud-detection-xgboost/notebook'>https://www.kaggle.com/georgepothur/4-financial-fraud-detection-xgboost/notebook</a></li>\n",
    "    <li>Teradataml Python reference: <a href = 'https://docs.teradata.com/search/all?query=Python+Package+User+Guide&content-lang=en-US'>here</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid #91A0Ab\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2025. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
