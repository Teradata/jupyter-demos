{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc58cc6-6764-4db3-ad38-1f56ba0fa631",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Augmented Call Center: Revolutionizing Customer Support with Advanced AI Technologies\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377a726-527f-4ae8-9af9-eb5b2019e896",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Introduction:</b></p>\n",
    "\n",
    "\n",
    " <p style = 'font-size:16px;font-family:Arial'><strong>Augmented Call Center: Revolutionizing Customer Support with Advanced AI Technologies</strong></p>\n",
    "    <p style = 'font-size:16px;font-family:Arial'>Welcome to the future of customer support with our Augmented Call Center, where cutting-edge technologies converge to deliver unparalleled service experiences. Our solution leverages the power of <b>LangGraph, Large Language Models (LLM), Retrieval-Augmented Generation (RAG), and Propensity</b> Models to transform traditional call centers into intelligent, efficient, and customer-centric hubs.</p>\n",
    "    <p style = 'font-size:16px;font-family:Arial'><strong>Key Features:</strong></p>\n",
    "    <ol style = 'font-size:16px;font-family:Arial'>\n",
    "        <li><strong>LangGraph Integration:</strong>\n",
    "            <p style = 'font-size:16px;font-family:Arial'>LangGraph enables seamless multilingual support, breaking down language barriers and ensuring clear communication with customers worldwide. This technology automatically translates and understands multiple languages, providing accurate and context-aware responses.</p>\n",
    "        </li>\n",
    "        <li><strong>Large Language Models (LLM):</strong>\n",
    "            <p style = 'font-size:16px;font-family:Arial'>Our call center is powered by state-of-the-art LLMs, which understand and generate human-like text. These models can handle complex queries, provide detailed information, and engage in natural, conversational interactions, enhancing the overall customer experience.</p>\n",
    "        </li>\n",
    "        <li><strong>Retrieval-Augmented Generation (RAG):</strong>\n",
    "            <p style = 'font-size:16px;font-family:Arial'>RAG combines the strengths of retrieval-based and generation-based models. It retrieves relevant information from vast databases and generates precise, contextually appropriate responses. This ensures that customers receive accurate and up-to-date information quickly.</p>\n",
    "        </li>\n",
    "        <li><strong>Propensity Models:</strong>\n",
    "            <p style = 'font-size:16px;font-family:Arial'>Propensity models analyze customer behavior and predict future actions. By understanding customer preferences and tendencies, our call center can proactively address needs, offer personalized recommendations, and improve customer satisfaction and retention.</p>\n",
    "        </li>\n",
    "    </ol>\n",
    "    <p style = 'font-size:16px;font-family:Arial'><strong>Benefits:</strong></p>\n",
    "    <ol style = 'font-size:16px;font-family:Arial'>\n",
    "        <li>Enhanced Efficiency: Automate routine tasks and reduce call handling times, allowing agents to focus on more complex issues.</li>\n",
    "        <li>Improved Accuracy: Provide precise and relevant information, minimizing errors and enhancing trust.</li>\n",
    "        <li>Personalized Service: Tailor interactions based on customer data and preferences, creating a more personalized and engaging experience.</li>\n",
    "        <li>Scalability: Easily scale operations to handle increased call volumes without compromising service quality.</li>\n",
    "        <li>Global Reach: Support customers in multiple languages, expanding your reach and improving accessibility.</li>\n",
    "    </ol>\n",
    "    <p style = 'font-size:16px;font-family:Arial'>Experience the next generation of customer support with our Augmented Call Center. Harness the power of advanced AI technologies to deliver exceptional service, drive customer loyalty, and achieve operational excellence.</p>\n",
    "\n",
    "\n",
    "<center><img src=\"images/augmented_call_center.png\" alt=\"call center\"  width=800 height=800/></center>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95205678-b4ec-4435-ba1b-56c9ac59e9eb",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b>Why Vantage?</b></p>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Chatbot, powered by Machine Learning and Artificial Intelligence, offer numerous advantages in the context of improving customer experience, particularly in the banking and financial institutions sector. Traditional methods of visiting a bank or financial institution in person and providing extensive information, which is then manually reviewed by an officer, are often time-consuming, error-prone, and rely on outdated manual processes.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Vantage provides these same proven capabilities to search user details, proving personalized offers, helping to search through policy documents, create proposal,etc, integrated as native ClearScape Analytic functions. This allows organizations to drastically reduce human workforce by chatbot, while allowing for much more user friendly and easy interactions.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1817e185-1f71-4b38-a5ce-410cd16ad7e0",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><b>Steps in the analysis:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Configuring the environment</li>\n",
    "    <li>Connection to Vantage and OpenAI</li>\n",
    "    <li>Confirmation for propensity model installation</li>\n",
    "    <li>Define the Agent State</li>\n",
    "    <li>Define the Agent Graph</li>\n",
    "    <li>Launch the Chatbot</li>\n",
    "    <li>Cleanup</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6849575-cc34-4c36-84cf-d6071eed679a",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>1. Configuring the environment</b>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i><b>Note:</b>The installation of the required libraries will take approximately <b>4 to 5 minutes</b> for the first-time installation. However, if the libraries are already installed, the execution will complete within 5 seconds.</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152a10b-57cf-46c5-a51d-36be1a616980",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df71e9be-0d9e-4507-b57e-632c7ce28cb5",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "    <i>The above statements will install the required libraries to run this demo.</i></p>\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i><b>Note:</b> Please restart the kernel after executing the pip install to bring the installed libraries into memory. The simplest way to restart the Kernel is by typing zero zero: <b>0 0</b></i></p></div>\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i><b>Note:</b> To ensure that the Chatbot interface reflects the latest changes, please reload the page by clicking the 'Reload' button or pressing F5 on your keyboard for <b>first-time only</b> This will update the notebook with the latest modifications, and you'll be able to interact with the Chatbot using the new libraries.</i></p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb5e00-e746-446c-8654-28cdd5b87077",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.1 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cdfc2e-3341-4192-99c9-31aaa6694401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teradata lib\n",
    "from teradataml import *\n",
    "\n",
    "# LLM\n",
    "from langchain import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Pydentic\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict, Annotated, Any, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, Literal, TypedDict\n",
    "from typing import Literal, Optional, Any\n",
    "\n",
    "# common\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# util\n",
    "from deploy_propensity_model import deploy_propensity_model, setup_test_data\n",
    "\n",
    "configure.byom_install_location = \"mldb\"\n",
    "configure.val_install_location = \"val\"\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "display.max_rows = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f3b10-08db-44e9-af92-3384686d390a",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>2. Connection to Vantage and OpenAI</b>\n",
    "\n",
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>2.1 Connect to Vantage</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>You will be prompted to provide the password. Enter your password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc18b2-3a44-4634-b77d-cb9303a42a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e4cbb-d7e0-439b-8ddd-7cb6b1060334",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>2.2 Get the OpenAI API key</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In order to utilize this demo, you will need an OpenAI API key. If you do not have one, please refer to the instructions provided in this guide to obtain your OpenAI API key: </p>\n",
    "\n",
    "[Openai_setup_api_key_guide](..//Openai_setup_api_key/Openai_setup_api_key.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b69d1-c427-41db-9109-e17d859ad625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(key: str):\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57021ffc-3d7e-4e56-b4c4-53748850ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "# setup LLM\n",
    "llm = ChatOpenAI(temperature=0, streaming=True, model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc5915-8715-4f52-8207-797af5dcf928",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>3. Confirmation for propensity model installation</b>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Before starting let us confirm that the required propensity model is installed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea25de-944f-429d-8d0a-5e9c627ce19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_model_exists = False\n",
    "try:\n",
    "    is_model_exists = (\n",
    "        DataFrame.from_query(\"select count(*) as model_cnt from mm_glm\").get_values()[\n",
    "            0\n",
    "        ][0]\n",
    "        > 0\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ad850-65d5-4bfc-8769-3a484afe60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_model_exists:\n",
    "    deploy_propensity_model()\n",
    "    result = setup_test_data()\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"propensity model is already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be9664",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.2 Create common functions for logs</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Let's create some common functions</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71070c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "logs_history = []\n",
    "\n",
    "\n",
    "def logs_console(msg: str):\n",
    "    \"\"\"Print formatted log message.\"\"\"\n",
    "    print(f\"\\n‚ÑπÔ∏è LOGS : {msg}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "def thought(thought: str):\n",
    "    \"\"\"Record agent's thinking process.\"\"\"\n",
    "    print(f\"\\nü§î Thought: {thought}\")\n",
    "    logs_history.append(f\"\\nü§î Thought: {thought}\")\n",
    "\n",
    "\n",
    "def action_obs(action: str, result: any, mrkdwn: str = \"\"):\n",
    "    \"\"\"Record agent's actions and results.\"\"\"\n",
    "    print(f\"üéØ Action: {action}\")\n",
    "    logs_history.append(f\"üéØ Action: {action}\")\n",
    "\n",
    "    if mrkdwn:\n",
    "        display(Markdown(mrkdwn))\n",
    "    print(f\"üìù Observation: {result}\")\n",
    "    logs_history.append(f\"üìù Observation: {result}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06abe4e5",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>4. Define the Agent State</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demo we are using <b>LangGraph</b> to build a multi-agent system. In LangGraph, the state refers to the data that the agent carries and updates as it performs tasks. This state is managed using a StateGraph, which is a graph parameterized by a state object. Each node in the StateGraph represents a specific operation or decision point, and it can update the state by either setting new values or adding to the existing ones</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad957852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict, total=False):\n",
    "    messages: Optional[Annotated[list[AnyMessage], add_messages]] = []\n",
    "    user_query: Optional[str] = \"\"\n",
    "    lookup_response: Optional[str] = \"\"\n",
    "    user_info: Optional[str] = \"\"\n",
    "    proposal: Optional[str] = \"\"\n",
    "    propensity: Optional[float] = 0.0\n",
    "    start_node: Optional[str] = \"\"\n",
    "    is_coverage: Optional[bool] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61996aaa-f9ab-402f-93ce-8e0a00fdff68",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.1 Agent 1: Policy Lookup</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The Policy Lookup is responsible for search the policy wordings from policy documents. First we will read the policy document and then store it into vector database.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d48844-dac8-4330-bc34-d7f11a84f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\n",
    "    \"./data/SmartTraveller_International.pdf\",\n",
    ")\n",
    "\n",
    "docs_list = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Add to vectorDB\n",
    "vector_store = FAISS.from_documents(doc_splits, embeddings)\n",
    "\n",
    "# Save the index for reuse\n",
    "vector_store.save_local(\"./embeddings/policy_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6762c1-81bf-4d96-b674-ad0b3419a4fd",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Load the vector database.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bbd80c-e787-4a19-89fe-a0799d274bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"./embeddings/policy_index\", embeddings, allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def lookup_policy(query: str) -> str:\n",
    "    \"\"\"Consult the company insurance policies to check whether certain options are permitted.\"\"\"\n",
    "    docs = vectorstore.similarity_search(query, k=2)\n",
    "    return {\"messages\": \"\\n\\n\".join([doc.page_content for doc in docs])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035aab4-d4a6-48d7-ae3c-c4a888dcb4b3",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.2 Agent 2: User Insurance policy Details</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This agent will fetch the user details from Customer360 table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b5b87-51cf-4c5d-a926-4ae7da2f97df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_user_insurance_information(state: AgentState) -> AgentState:\n",
    "    \"\"\"Fetch all the insurance policies for the user along with corresponding personal information and policy information.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary which contains the user's personal and policy details, Claim details, etc. to the user.\n",
    "    \"\"\"\n",
    "    logs_console(\"fetch_user_insurance_information called\")\n",
    "\n",
    "    customer_id = \"C1001\"\n",
    "    qry = \"select * from customer360  WHERE CustomerID = ? \"\n",
    "\n",
    "    # with eng.raw_connection() as connection:\n",
    "    conn = eng.raw_connection()\n",
    "    cursor = conn.cursor()\n",
    "    rows = cursor.execute(qry, (customer_id,)).fetchall()\n",
    "    column_names = [column[0] for column in cursor.description]\n",
    "    results = [dict(zip(column_names, row)) for row in rows]\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    action_obs(\"fetch_user_insurance_information\", results[0])\n",
    "    state[\"user_info\"] = results[0]\n",
    "    state[\"is_coverage\"] = True\n",
    "    logs_console(f\"state at user-info: {state}\")\n",
    "    logs_console(\"fetch_user_insurance_information completed...\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82553d3a-9e06-4ba1-a5f3-a1a5ae0b043f",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.3 Agent 3: Predict the user's Propensity to buy Dental treatment</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Propensity agent is responsible for Predict the user's Propensity to buy Dental treatment as addon. Propensity model will consider age, past dental history, claims etc and predict the propensity.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae1bd7-5845-4619-9be7-a3f8197360a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_user_propensity(state: AgentState) -> AgentState:\n",
    "    \"\"\"Fetch propensity to buy a dental treatment based on customer's personal information and previous health and dental related information.\n",
    "    Returns:\n",
    "        A dictionary which contains the customer's Customer ID and prediction.\n",
    "    \"\"\"\n",
    "    logs_console(\"fetch_user_propensity called\")\n",
    "    thought(\n",
    "        \"I have to pass customer's details to ClearScape Analytics hosted Model to get the propensity of buying the Dental Treatment\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        qry = \"\"\"\n",
    "        SELECT * FROM \"mldb\".PMMLPredict(\n",
    "            ON \"df_test_user\" AS InputTable\n",
    "            PARTITION BY ANY \n",
    "            ON (select model_id,model from \"DEMO_USER\".\"mm_glm\") AS ModelTable\n",
    "            DIMENSION\n",
    "            USING\n",
    "            Accumulate('Customer_ID')\n",
    "            OverwriteCachedModel('*')\n",
    "        ) as sqlmr\n",
    "        \"\"\"\n",
    "\n",
    "        conn = eng.raw_connection()\n",
    "        cursor = conn.cursor()\n",
    "        rows = cursor.execute(qry).fetchall()\n",
    "        column_names = [column[0] for column in cursor.description]\n",
    "        results = [dict(zip(column_names, row)) for row in rows]\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        action_obs(\"fetch_user_propensity\", results[0][\"prediction\"])\n",
    "        state[\"propensity\"] = float(results[0][\"prediction\"])\n",
    "        logs_console(\"fetch_user_propensity completed returning propensity...\")\n",
    "\n",
    "        return state\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fetch_user_propensity: {e}\")\n",
    "        return {\"error\": e}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceafa64-22eb-44e1-9a85-061a45f9d830",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.4 Agent 4: Generate the Insurance Proposal</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This agent is responsible for creation of the proposal using customer360 data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f743de-6e53-4711-b190-50f69018ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insurance_proposal(state: AgentState) -> AgentState:\n",
    "    \"\"\"Fetch all the insurance policies for the user along with corresponding personal information and policy information.\n",
    "    Returns:\n",
    "        Generate a Dental Treatment Proposal for given user.\n",
    "    \"\"\"\n",
    "    logs_console(\"generate_insurance_proposal called\")\n",
    "\n",
    "    thought(\n",
    "        \"I have create a proposal for the Dental Treatment as addon to existing Insurance\"\n",
    "    )\n",
    "\n",
    "    customer_id = \"C1001\"\n",
    "\n",
    "    qry = \"select * from customer360  WHERE CustomerID = ? \"\n",
    "\n",
    "    # with eng.raw_connection() as connection:\n",
    "    conn = eng.raw_connection()\n",
    "    cursor = conn.cursor()\n",
    "    rows = cursor.execute(qry, (customer_id,)).fetchall()\n",
    "    column_names = [column[0] for column in cursor.description]\n",
    "    results = [dict(zip(column_names, row)) for row in rows]\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    action_obs(\"generate_insurance_proposal - Get user details\", results[0])\n",
    "\n",
    "    # setup LLM\n",
    "    logs_console(\"Now, generating the Proposal\")\n",
    "    primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful customer support assistant for ABC Overseas Travel Insurance. \"\n",
    "                \"Use the provided customer information and write a dental treatment proposal, as addon to existing insurance policies,\"\n",
    "                \"Write a travel insurance proposal for Dental Treatment as addon using given features. Write it with proper markdown and style.\"\n",
    "                \"Add the reasoning for Propensity to buy Dental Treatment basis on propensity and user_info provided below:\\n\\n\"\n",
    "                \"Example: Bases on customer's age, location, and previous dental history, we have calculated the propensity to buy Dental Treatment as 0.8.\"\n",
    "                \"\\n\\nCurrent user:\\n<User>\\n{user_info}\\n</User>\"\n",
    "                \"\\n\\nPropensity to buy Dental Treatment: {propensity}\"\n",
    "                \"\\nCurrent time: {time}.\",\n",
    "            ),\n",
    "            (\"placeholder\", \"{messages}\"),\n",
    "        ]\n",
    "    ).partial(time=datetime.now)\n",
    "\n",
    "    prompt1 = primary_assistant_prompt.format_messages(\n",
    "        user_info=results[0], propensity=state[\"propensity\"]\n",
    "    )\n",
    "    response = llm.invoke(prompt1)\n",
    "    action_obs(\n",
    "        \"generate_insurance_proposal - generate the proposal\",\n",
    "        None,\n",
    "        response.content[:100],\n",
    "    )\n",
    "    state[\"proposal\"] = response.content\n",
    "    logs_console(\"generate_insurance_proposal completed\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f378a3",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.5 Agent 5: Intent identification</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Intent identification can accurately determine whether a user's query is related to insurance or not.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5def57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insurance_identify_intent(state) -> Literal[\"retrieve\", \"agent\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the user is asking for insurance policy or claim related question\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the intent is related to policy or not\n",
    "    \"\"\"\n",
    "\n",
    "    logs_console(\"insurance_identify_intent calls\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        insurance_binary_score: str = Field(description=\"intent score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = llm.with_structured_output(grade, method=\"function_calling\")\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a specialized travel insurance intent recognition system.\n",
    "        Analyze the following user query and identify the most relevant travel insurance intent.\n",
    "        \n",
    "        Here is the user question: {question} \\n\n",
    "        \n",
    "        Available intents:\n",
    "        - policy_information: Questions about policy terms, documentation, or general policy details\n",
    "        - claim_process: Questions about filing claims, claim status, or claim requirements\n",
    "        - coverage_details: Questions about what is covered or not covered by travel insurance, coverage period, etc.\n",
    "        - eligibility: Questions about who can purchase or qualify for travel insurance\n",
    "        - pricing: Questions about costs, premiums, or payment options, refunds, etc.\n",
    "        - medical_coverage: Questions about medical coverage, medical expenses, etc.\n",
    "        - trip_cancellation: Questions about trip cancellation, refunds, etc.\n",
    "        - documentation: Questions about required documents, proof of purchase, etc.\n",
    "        - emergency_assistance: Questions about emergency services, medical evacuation, etc.\n",
    "        - cancellation_policy: Questions about trip cancellation, refunds, etc.\n",
    "        - trip delay: Questions about trip delays, missed connections, etc.\n",
    "        - personal liability: Questions about personal liability coverage, legal expenses, etc.\n",
    "        - multiple injuries: Questions about coverage for multiple injuries, accidents, etc.\n",
    "        - dental_treatment: Questions about dental treatment coverage, dental emergencies, etc.\n",
    "        - general_inquiry: General questions about travel insurance not fitting other categories\n",
    "        - customer_service: Questions about contacting customer service, support options, email address, phone numbers, etc.\n",
    "        - other: Queries not related to travel insurance\n",
    "        \n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the question is relevant to travel insurance or not.\"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    question = state[\"messages\"][-1].content\n",
    "    logs_console(f\"question : {question}\")\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question})\n",
    "\n",
    "    logs_console(f\"insurance intent scored_result: {scored_result}\")\n",
    "\n",
    "    score = scored_result.insurance_binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: intent found to insurance ---\")\n",
    "        return \"retrieve\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: intent not found to insurance ---\")\n",
    "        print(score)\n",
    "        return \"agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6818316a",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.6 Agent 6: Coverage Intent identification</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Intent identification has the capability to determine whether a user's query is related to insurance coverage or not. For example, it can identify queries about health insurance, like Coverage for personal belongings, valuables, or luggage.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009cf0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_coverage_intent(\n",
    "    state: AgentState,\n",
    ") -> Literal[\"fetch_user_info\", \"stop_end\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the user is asking for insurance coverage or claim related question\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "    Returns:\n",
    "        str: A decision for whether the intent is related to coverage or not\n",
    "    \"\"\"\n",
    "\n",
    "    logs_console(\"identify_coverage_intent calls\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        coverage_binary_score: str = Field(\n",
    "            description=\"coverage intent score 'yes' or 'no'\"\n",
    "        )\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = llm.with_structured_output(grade, method=\"function_calling\")\n",
    "\n",
    "    # Prompt\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a specialized travel insurance coverage intent classifier.\n",
    "\n",
    "        Task: Analyze the user query and determine if it relates to travel insurance coverage details.\n",
    "\n",
    "        User query: {question}\n",
    "\n",
    "        Coverage Intent Definition:\n",
    "        \"coverage_details\" refers to questions about what is included or excluded in a travel insurance policy, including:\n",
    "        - Scope of medical coverage (illness, injury, emergency care)\n",
    "        - Coverage for personal belongings, valuables, or luggage\n",
    "        - Coverage limitations based on trip type (domestic vs international)\n",
    "        - Age restrictions or health condition limitations\n",
    "        - Activity exclusions (extreme sports, pre-existing conditions)\n",
    "        - Coverage amounts and reimbursement limits\n",
    "        - Documentation requirements for claims\n",
    "\n",
    "        Excluded from Coverage Intent:\n",
    "        - Geographic coverage areas or travel destinations (\"What is the area of travel included?\")\n",
    "        - Customer support contact information (\"How do I reach customer service?\")\n",
    "        - Fraud-related inquiries (\"What happens in case of fraud related to claims?\")\n",
    "        - Billing or payment questions (\"When is my premium due?\")\n",
    "        - Policy purchase process (\"How do I buy travel insurance?\")\n",
    "        - Claims submission procedures (\"How do I file a claim?\")\n",
    "        - Policy cancellation inquiries (\"Can I cancel my policy?\")\n",
    "        - Refund requests (\"How do I get a refund?\")\n",
    "\n",
    "        Output Format:\n",
    "        1. Intent Classification: Respond with \"Yes\" if the query relates to coverage details, or \"No\" if it does not.\n",
    "\n",
    "        Example Classifications:\n",
    "        - \"Does travel insurance cover scuba diving accidents?\" ‚Üí Yes\n",
    "        - \"Am I covered if I get COVID-19 during my trip?\" ‚Üí Yes\n",
    "        - \"Does my policy cover rental car damage?\" ‚Üí Yes\n",
    "        - \"How do I file a claim for my delayed flight?\" ‚Üí No\n",
    "        - \"What's the best travel insurance for seniors?\" ‚Üí No\n",
    "        - \"Can I add my spouse to my travel insurance?\" ‚Üí No\"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "    question = state[\"messages\"][-1].content\n",
    "    logs_console(f\"question : {question}\")\n",
    "    scored_result = chain.invoke({\"question\": question})\n",
    "    logs_console(f\"coverage intent scored_result: {scored_result}\")\n",
    "    score = scored_result.coverage_binary_score\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: intent found to coverage ---\")\n",
    "        return \"fetch_user_info\"\n",
    "    else:\n",
    "        print(\"---DECISION: intent not found to coverage ---\")\n",
    "        print(score)\n",
    "        return \"stop_end\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97844630-26ab-4479-8d0b-56295578097a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Based on the user's query, retrieve relevant chunks from the vector database and prepare a comprehensive answer.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_insurance_policy(state: AgentState) -> any:\n",
    "    \"\"\"Retrieve the insurance policy based on the user query and provide the policy information.\"\"\"\n",
    "    logs_console(\"retrieve_insurance_policy called\")\n",
    "    assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are a helpful customer support assistant for ABC Overseas Travel Insurance.\n",
    "                    Use the following pieces of retrieved context to answer the question.\n",
    "                    Briefly explain, provide a syntax in professional tone.\n",
    "                    If you don't know the answer, just say that you don't know. \n",
    "                    Use the provided context to generate the response for the user's queries.\n",
    "\n",
    "                    *Most Critical* : At the end of the response, give suggestion the user for addon treatment like, Dental treatment. \n",
    "                    For example, here is the response: <answer>, 'However, it does not include dental treatment, \n",
    "                    Based on your loyalty, and your contract, we would recommend extended warranty package which includes dental insurance.'\n",
    "                    combine answer and suggestion in one sentence.\n",
    "                    query: {query}\n",
    "                    context: {context}\n",
    "\n",
    "                    Response strictly in valid JSON format:\n",
    "                    \"answer\": \"your answer and suggestion\"\n",
    "                    \"source\": file name\n",
    "\n",
    "                    Instructions:\n",
    "                    - Do not include ```json in the response.\n",
    "                    - Give the answer in a professional tone and brief.\n",
    "                \"\"\",\n",
    "            ),\n",
    "            (\"placeholder\", \"{messages}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    query = state[\"messages\"][-1].content\n",
    "    logs_console(f\"query: {query}\")\n",
    "    docs = lookup_policy(query)\n",
    "    prompt = assistant_prompt.format_messages(query=query, context=docs[\"messages\"])\n",
    "    response = llm.invoke(prompt)\n",
    "    print(f\"\\n\\n response: {response.content}\")\n",
    "    response_dict = json.loads(response.content)\n",
    "    logs_console(f\"response: {response_dict}\")\n",
    "    state[\"lookup_response\"] = response_dict[\"answer\"]\n",
    "    state[\"user_info\"] = \"\"\n",
    "    state[\"proposal\"] = \"\"\n",
    "    state[\"propensity\"] = 0.0\n",
    "\n",
    "    logs_console(f\"lookup resp: {response}\")\n",
    "    logs_console(f\"generate state: {state}\")\n",
    "    logs_console(\"generate completed...\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf17676c-68e8-4a5a-9a6d-56aca345d974",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.7 Agent 7: Small LLM Calls</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>This agent is capable of engaging in small talk, such as greeting the user. It can also handle queries outside the scope of insurance. For example, if a user asks about AI agents or Teradata's enterprise vector store, the agent can provide relevant information..</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff0561-0fbf-4aa7-a543-7674862ceb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes\n",
    "def agent(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    logs_console(\"MAIN agent called...\")\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def welcome(state: AgentState):\n",
    "    logs_console(\"welcome called\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f3b2d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>5. Define the Agent Graph</b>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial;color:#00233C'>An Agent Graph is a conceptual framework used to design and manage AI agents, particularly those that need to handle complex tasks and maintain state over time. Here are some key aspects: </p>\n",
    "\n",
    "<ol style='font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <li><strong>Graph Structure</strong>: The agent's operations are represented as nodes in a graph. Each node corresponds to a specific action or decision point, allowing the agent to navigate through various tasks systematically.</li>\n",
    "    <li><strong>State Management</strong>: The agent maintains a state that evolves as it interacts with users or performs tasks. This state is crucial for remembering past interactions, making informed decisions, and ensuring continuity.</li>\n",
    "    <li><strong>Control Flows</strong>: Agent Graphs support diverse control flows, including single-agent, multi-agent, hierarchical, and sequential setups. This flexibility allows for robust handling of realistic, complex scenarios.</li>\n",
    "    <li><strong>Moderation and Quality Loops</strong>: To ensure reliability, Agent Graphs can incorporate moderation and quality loops. These mechanisms prevent agents from veering off course and allow for human-in-the-loop interventions.</li>\n",
    "    <li><strong>Streaming and Interaction</strong>: Agent Graphs can provide real-time feedback to users by streaming intermediate steps and reasoning processes. This enhances transparency and user engagement.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1200e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop_condition(state: AgentState) -> Literal[\"assistant\", \"stop_end\"]:\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logs_console(\"prop_condition called\")\n",
    "\n",
    "        prop = state[\"propensity\"]\n",
    "        logs_console(f\"propensity : {prop}\")\n",
    "        state[\"current_node\"] = \"prop_condition\"\n",
    "\n",
    "        if float(prop) > 0.70:\n",
    "            logs_console(\"propensity > 0.70\")\n",
    "            logs_console(\"setting up next agent = assistant\")\n",
    "            return \"assistant\"\n",
    "        else:\n",
    "            logs_console(\"propensity < 0.70\")\n",
    "            logs_console(\"setting up next agent = stop_end\")\n",
    "            return \"stop_end\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prop_condition: {e}\")\n",
    "        return {\"error\": e}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd15e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop_condition_checker(state: AgentState):\n",
    "    logs_console(\"prop_condition_checker called\")\n",
    "    logs_console(f\"prop_condition_checker state: {state}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_end(state: AgentState):\n",
    "    logs_console(\"stop_end called\")\n",
    "    logs_console(f\"stop_end state: {state}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eabeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage_intent_checker(state: AgentState):\n",
    "    logs_console(\"coverage_intent_checker called\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4c7a3-5435-4fbe-9c1f-156240675ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_workflow(memory: MemorySaver, start_node=\"welcome\"):\n",
    "    \"\"\"Create and configure the agent workflow graph.\"\"\"\n",
    "\n",
    "    workflow1 = StateGraph(AgentState)\n",
    "\n",
    "    # Define a new graph\n",
    "    workflow1.add_node(\"agent\", agent)  # agent\n",
    "    workflow1.add_node(\"welcome\", welcome)  # agent\n",
    "    workflow1.add_node(\"retrieve\", retrieve_insurance_policy)  # retrieval\n",
    "    workflow1.add_node(\"fetch_user_info\", fetch_user_insurance_information)\n",
    "    workflow1.add_node(\"coverage_intent_checker\", coverage_intent_checker)\n",
    "    workflow1.add_node(\"fetch_user_propensity\", fetch_user_propensity)\n",
    "    workflow1.add_node(\"prop_condition_checker\", prop_condition_checker)\n",
    "\n",
    "    workflow1.add_node(\"assistant\", generate_insurance_proposal)\n",
    "    workflow1.add_node(\"stop_end\", stop_end)\n",
    "\n",
    "    # EDGES\n",
    "    workflow1.add_edge(START, \"welcome\")\n",
    "    workflow1.add_conditional_edges(\n",
    "        \"welcome\",\n",
    "        insurance_identify_intent,\n",
    "    )\n",
    "\n",
    "    workflow1.add_edge(\"agent\", END)\n",
    "    workflow1.add_edge(\"retrieve\", \"coverage_intent_checker\")\n",
    "    workflow1.add_conditional_edges(\n",
    "        \"coverage_intent_checker\",\n",
    "        identify_coverage_intent,\n",
    "    )\n",
    "    workflow1.add_edge(\"fetch_user_info\", \"fetch_user_propensity\")\n",
    "    workflow1.add_edge(\"fetch_user_propensity\", \"prop_condition_checker\")\n",
    "    workflow1.add_conditional_edges(\n",
    "        \"prop_condition_checker\",\n",
    "        prop_condition,\n",
    "    )\n",
    "    workflow1.add_edge(\"assistant\", END)\n",
    "    workflow1.add_edge(\"stop_end\", END)\n",
    "\n",
    "    # Compile\n",
    "    return workflow1.compile(\n",
    "        checkpointer=memory, interrupt_before=[\"fetch_user_propensity\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884b473a-228f-4306-afdd-28b707f6c025",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<b style = 'font-size:18px;font-family:Arial'>5.1 Visualize the Agent graph</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To view graph, visualization libraries like networkx or graphviz in Python to create custom visualizations of our agent graphs. These libraries allow us to programmatically generate and display the graph structure based on the data from LangGraph.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa340b2-d713-4ba3-a932-918e731a1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "app = create_agent_workflow(memory=MemorySaver())\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    display(Markdown(\"\"\"<img src=\"images/agentgraph.png\" alt=\"agent graph\" width=427 height=871/>\"\"\"))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2c73b-88a8-4765-bf6e-dc74333e46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with the backup file so we can restart from the original place in each section\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Function to run the workflow\n",
    "def process_user_message(\n",
    "    user_message: str, state=None, start_node: str = \"welcome\"\n",
    ") -> dict[str, Any]:\n",
    "    try:\n",
    "        # add memory\n",
    "        memory = MemorySaver()\n",
    "\n",
    "        if state is None:\n",
    "            state = AgentState(\n",
    "                messages=[HumanMessage(content=user_message)],\n",
    "                start_node=start_node,\n",
    "                user_query=\"\",\n",
    "                lookup_response=\"\",\n",
    "                user_info=\"\",\n",
    "                proposal=\"\",\n",
    "                propensity=0.0,\n",
    "                is_coverage=False,\n",
    "            )\n",
    "\n",
    "        # workflow 2 starts from here\n",
    "        if start_node != \"welcome\":\n",
    "            logs_console(\"workflow 2 started...\")\n",
    "            # ---------------------\n",
    "            workflow2 = StateGraph(AgentState)\n",
    "            workflow2.add_node(\"fetch_user_propensity\", fetch_user_propensity)\n",
    "            workflow2.add_node(\"assistant\", generate_insurance_proposal)\n",
    "            workflow2.add_node(\"prop_condition_checker\", prop_condition_checker)\n",
    "            workflow2.add_node(\"stop_end\", stop_end)\n",
    "\n",
    "            workflow2.add_edge(START, start_node)\n",
    "            workflow2.add_edge(\"fetch_user_propensity\", \"prop_condition_checker\")\n",
    "            workflow2.add_conditional_edges(\n",
    "                \"prop_condition_checker\",\n",
    "                # Assess agent decision\n",
    "                prop_condition,\n",
    "            )\n",
    "\n",
    "            workflow2.add_edge(\"assistant\", END)\n",
    "            workflow2.add_edge(\"stop_end\", END)\n",
    "\n",
    "            # add memory\n",
    "            memory = MemorySaver()\n",
    "\n",
    "            # Compile\n",
    "            app2 = workflow2.compile(checkpointer=memory)\n",
    "            logs_console(\"workflow 2 graph created...\")\n",
    "\n",
    "            # try:\n",
    "            #     display(Image(app2.get_graph(xray=True).draw_mermaid_png()))\n",
    "            # except Exception as e:\n",
    "            #     print(\"\\n\\nERROR: \", e)\n",
    "            #     # This requires some extra dependencies and is optional\n",
    "            #     pass\n",
    "\n",
    "            # Run the workflow\n",
    "            result2 = app2.invoke(state, config=config, debug=False)\n",
    "            return result2, state\n",
    "        # workflow 2 ends here\n",
    "\n",
    "        logs_console(\"workflow 1 started...\")\n",
    "\n",
    "        app1 = create_agent_workflow(memory)\n",
    "        result1 = app1.invoke(state, config=config, debug=False)\n",
    "        # Return result to the user\n",
    "        return result1, state\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_user_message: {e}\")\n",
    "        return {\"error\": e}, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d36a16",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>6. Launch the Chatbot</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demo we are using ChatOpenAI model as LLM. This advanced technology allows us to store and recall conversations, enabling our chatbot to provide more personalized and informed responses.</p>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i><b>Note:</b>Chatbot is accessing multiple components, including databases and LLMs. This may cause a brief delay in responses. Your patience is appreciated.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54284f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "def chat_callback(contents, user, instance):\n",
    "    global logs_history\n",
    "    logs_history = []\n",
    "\n",
    "    # Get current timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Log the user message\n",
    "    log_message = f\"[{timestamp}] User: {contents}\"\n",
    "    current_logs = logs.value\n",
    "    logs.value = (\n",
    "        current_logs + log_message + \"\\n\" if current_logs else log_message + \"\\n\"\n",
    "    )\n",
    "\n",
    "    #  call agent workflow\n",
    "    result1, state = process_user_message(contents)\n",
    "    if result1.get(\"lookup_response\"):\n",
    "        action_obs(\"process called..\", \"--- LOOKUP RESPONSE FOUND----\")\n",
    "        lookup_response1 = result1[\"lookup_response\"]\n",
    "        print(f\"\\n**Insurance Assistant**: {lookup_response1}\")\n",
    "        response_preview1 = lookup_response1\n",
    "\n",
    "        # Update history with the response\n",
    "        ans1 = (\n",
    "            response_preview1 + \"\\n\\n‚è≥ Generating the proposal, please wait ‚è≥\\n\"\n",
    "            if result1.get(\"is_coverage\")\n",
    "            else response_preview1\n",
    "        )\n",
    "\n",
    "        # return answer1\n",
    "        instance.send(ans1, user=\"Assistant\", respond=False)\n",
    "\n",
    "        # Log the response\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_message = f\"[{timestamp}] Bot: {ans1}\"\n",
    "        logs.value = logs.value + \"\\n\".join(logs_history) + \"\\n\"\n",
    "\n",
    "        if result1.get(\"is_coverage\"):\n",
    "            print(\"--- FOUND COVERAGE ----\")\n",
    "            # Test resuming from fetch_user_propensity\n",
    "            print(\"\\n\\n Resuming from fetch_user_propensity: \\n\\n\")\n",
    "            result2 = process_user_message(\n",
    "                contents, start_node=\"fetch_user_propensity\", state=state\n",
    "            )\n",
    "\n",
    "            response_preview2 = result2[0].get(\"proposal\")\n",
    "            ans2 = f\"\\n\\n *Dental Proposal*: {response_preview2}\"\n",
    "            instance.send(ans2, user=\"Assistant\", respond=False)\n",
    "            logs.value = logs.value + \"\\n\".join(logs_history) + \"\\n\"\n",
    "\n",
    "    else:\n",
    "        print(\"--- ELSE----\")\n",
    "        response_preview = result1[\"messages\"][-1].content\n",
    "        # response = f\"{response}\\n\\n {response_preview}\"\n",
    "        instance.send(response_preview, user=\"Assistant\", respond=False)\n",
    "\n",
    "        # Log the response\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        log_message = f\"[{timestamp}] Bot: {response_preview}\"\n",
    "        logs.value = logs.value + log_message + \"\\n\"\n",
    "\n",
    "# Create chat interface\n",
    "chat_interface = pn.chat.ChatInterface(\n",
    "    callback=chat_callback,\n",
    "    show_rerun=False,\n",
    "    show_clear=True,\n",
    "    show_button_name=True,\n",
    "    sizing_mode=\"stretch_width\",\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "\n",
    "# Create a logs text area\n",
    "logs = pn.widgets.TextAreaInput(\n",
    "    value=\"\",\n",
    "    placeholder=\"Chat logs will appear here...\",\n",
    "    height=500,\n",
    "    sizing_mode=\"stretch_width\",\n",
    "    disabled=True,\n",
    "    name=\"\",\n",
    "    styles={\"background\": \"white\", \"font-color\": \"black\"},\n",
    ")\n",
    "\n",
    "# Create a clear logs button\n",
    "clear_logs_button = pn.widgets.Button(name=\"\", button_type=\"default\", width=100)\n",
    "\n",
    "def clear_logs(event):\n",
    "    logs.value = \"\"\n",
    "\n",
    "clear_logs_button.on_click(clear_logs)\n",
    "\n",
    "# Create the layout\n",
    "logs_column = pn.Column(\n",
    "    pn.pane.Markdown(\"### Chat Logs\"),\n",
    "    logs,\n",
    "    clear_logs_button,\n",
    "    sizing_mode=\"stretch_both\",\n",
    ")\n",
    "\n",
    "app = pn.Row(\n",
    "    pn.Column(\n",
    "        pn.pane.Markdown(\"# Augmented Call Center üì≤ using Agentic AI\"),\n",
    "        chat_interface,\n",
    "        sizing_mode=\"stretch_both\",\n",
    "    ),\n",
    "    logs_column,\n",
    "    sizing_mode=\"stretch_both\",\n",
    ")\n",
    "\n",
    "app.servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01929f-56ea-4c44-82af-130c90846445",
   "metadata": {},
   "source": [
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Here are some example questions:</b></p>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "  <li><strong>Insurance queries:</strong> Ask us questions about claim, coverage, areas of travel, etc. For example:\n",
    "    <ul style = 'font-size:16px;font-family:Arial'>\n",
    "            <li><i>What travel area is included?</i></li>\n",
    "            <li><i>What happens in case of fraud related to claims?</i></li>\n",
    "            <li><i>Is there a limit on benefits for multiple injuries?</i></li>\n",
    "            <li><i>What should I do if I experience a medical emergency while traveling?</i> </li>\n",
    "        <li><i>I am traveling to Malaysia. Does my insurance cover medical expense?</i> </li>\n",
    "        <li><i>Are dental treatments covered?</i> </li>\n",
    "        <li><i>Is the loss of personal belongings covered?</i> </li>\n",
    "        </ul></li>\n",
    "</ol>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style='font-size:16px;font-family:Arial'>If the chatbot didn't work when we pressed ENTER, on our first time using this demo on our environment, did we use F5 to reload the site? See instructions at the top of the notebook. If we asked a question and got no response after a few minutes, it is possible that we will need to type 0 0 to restart the kernel and re-run the demo. Questions outside the model seem to confuse the chatbot.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20085c-c648-488f-bdf8-faa4497af09d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>7 Cleanup</b>\n",
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>7.1 Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Cleanup work tables to prevent errors next time.</p>\n",
    "\n",
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'> <b>7.2 Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afbf9b2-580e-48c5-81c0-bb15df2e7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [\n",
    "    \"customer360\",\n",
    "    \"df_test_user\",\n",
    "    \"mm_glm\",\n",
    "]:\n",
    "\n",
    "    try:\n",
    "        db_drop_table(t)\n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708412f2-0797-4306-8c53-0645b572dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd32783-987b-4ecb-a4ee-23cafe78e397",
   "metadata": {},
   "source": [
    "<b id=\"datasetInfo1\" style = 'font-size:20px;font-family:Arial'>Dataset:</b>\n",
    "\n",
    "**Customer360 Table**\n",
    "- `CustomerID`: Unique identifier for each customer.\n",
    "- `Name`: Full name of the customer.\n",
    "- `Age`: Age of the customer.\n",
    "- `Income`: Annual income of the customer.\n",
    "- `Occupation`: Job or profession of the customer.\n",
    "- `MaritalStatus`: Marital status of the customer (e.g., single, married).\n",
    "- `Children`: Number of children the customer has.\n",
    "- `Location`: Geographic location of the customer.\n",
    "- `PolicyID`: Unique identifier for each insurance policy.\n",
    "- `PolicyType`: Type of insurance policy (e.g., health, auto).\n",
    "- `CoverageAmount`: Amount of coverage provided by the policy.\n",
    "- `PremiumAmount`: Amount paid for the insurance premium.\n",
    "- `RenewalDate`: Date when the policy is due for renewal.\n",
    "- `ClaimID`: Unique identifier for each insurance claim.\n",
    "- `ClaimType`: Type of insurance claim (e.g., accident, theft).\n",
    "- `ClaimAmount`: Amount claimed by the customer.\n",
    "- `ClaimStatus`: Current status of the claim (e.g., pending, approved).\n",
    "- `InteractionID`: Unique identifier for each customer interaction.\n",
    "- `InteractionType`: Type of interaction (e.g., call, email).\n",
    "- `InteractionDate`: Date of the interaction.\n",
    "- `InteractionNotes`: Notes or details about the interaction.\n",
    "- `Preferences`: Customer preferences (e.g., communication method).\n",
    "- `BehavioralPatterns`: Patterns in customer behavior.\n",
    "- `RiskLevel`: Assessed risk level of the customer.\n",
    "- `RiskFactors`: Factors contributing to the customer's risk level.\n",
    "- `FraudulentActivitiesDetected`: Indicator of any detected fraudulent activities.\n",
    "- `UnusualPatterns`: Any unusual patterns in customer behavior.\n",
    "- `SatisfactionScore`: Customer's satisfaction score.\n",
    "- `FeedbackNotes`: Notes or comments from customer feedback.\n",
    "- `PotentialChurnRisk`: Likelihood of the customer leaving.\n",
    "- `LifetimeValuePrediction`: Predicted lifetime value of the customer.\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Dataset source:</b> Synthetically generated </p>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Links:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Teradataml Python reference: <a href = 'https://docs.teradata.com/search/all?query=Python+Package+User+Guide&content-lang=en-US'>here</a></li>\n",
    "    <li>LangGraph reference: <a href='https://langchain-ai.github.io/LangGraph/tutorials/introduction/'>here</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d8cf6-887a-4157-a936-b2f53caaf991",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid #91A0Ab\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics‚Ñ¢</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright ¬© Teradata Corporation - 2023. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
