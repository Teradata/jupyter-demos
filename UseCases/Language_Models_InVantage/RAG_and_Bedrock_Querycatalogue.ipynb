{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04f16ff-2384-4fb1-b97b-ec2284162244",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       RAG solution with Vantage catalogue and AWS Bedrock integration\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Introduction</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demo we will explore how to do extract data from metadata tables of Teradata using embedding and vector db style indexing in Vantage and then query LLM with context/prompts to get the details.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><p style = 'font-size:16px;font-family:Arial'>We use ONNXEmbeddings function for creating embeddings using the Hugging Face PyTorch models stored inDb using the BYOM functionality.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Teradata has Integration with LLMs with Amazon BedRock etc., and also emerging Open Analytics Framework in the Cloud Lake, where you can host a Language Model etc.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Known challenges of LLMs include:</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial'>Presenting false information when it does not have the answer.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Presenting out-of-date or generic information when the user expects a specific, current response.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Creating a response from non-authoritative sources.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.</li>\n",
    "</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demo, we will work on creating a catalogue using the tables in the database and use LLM to answer the prompts regarding the catalogue.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec6bdf-7ebd-4012-b4ca-cc374a3cb14f",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>1. Configuring the environment</b>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.1 Install the required libraries</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18932935-6f82-48ee-a199-0067dbd72b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain_community pypdf\n",
    "!pip install boto3 awscli\n",
    "!pip install pyopenssl --upgrade --force-reinstall\n",
    "!pip install -U pandas==2.1.3\n",
    "!pip install langchain\n",
    "!pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a73fe1-4b19-40c2-b5f3-64a1efd9976e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Note: </b><i>Please restart the kernel after executing these two lines. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48096e2d-1fb2-400e-8ea2-172315e9246a",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.2 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24df4b-a5c7-48c0-a9ee-7bded9bf823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from teradataml import *\n",
    "import getpass\n",
    "\n",
    "configure.byom_install_location = \"mldb\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display.max_rows=5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7b265-66ac-4456-bc63-79b71f90e853",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>1.3. Connect to Vantage</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae119d6f-fff0-43fc-a521-43456ef9c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host='host.docker.internal', username='demo_user', password=password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf08fe4-23b9-4351-a053-2b7bbc443727",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql(\"SET query_band='DEMO=Language_Model_RAG_Catalogue_Python.ipynb;' UPDATE FOR SESSION;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764a11a-52c0-4397-b790-64126d32e502",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>2. Confirmation for Model</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Before starting let us confirm that the required functions are installed.</p>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65456281-dbed-4617-a32f-11f859c30a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bge-small-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b62a9-2e10-45a2-a03d-9d8fa00cc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "df_check= DataFrame.from_query(f'''select (select 1 as cnt from embeddings_models where model_id = '{model_name}') +\n",
    "(select 1 as cnt from embeddings_tokenizers where model_id =  '{model_name}') as cnt''')\n",
    "if df_check.get_values()[0][0] == 2:\n",
    "    print('Model is installed, please continue.')\n",
    "else:\n",
    "    print('Model is not installed, please go to Instalization notebook before proceeding further')\n",
    "    display(Markdown(\"[Initialization Notebook](./Initialization_and_Model_Load.ipynb)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f664cf2-7a8d-4738-b6d7-385a0a0e98f7",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>3. Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo on cloud storage. Since we are using embeddings stored in Vantage for this demo we are only using the local storage for the demo. We will only use the option of creating table locally.</p>   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1e7d2-83bd-4ade-9818-9e912323d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call get_data('DEMO_SLMRAG_Catalogue_local');\"\n",
    " # Takes about 2 minutes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98989a-25d7-4157-80fc-e1068ca1a101",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Optional step – We should execute the below step only if we want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ca612-1a92-455a-a678-e89e049f6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42b64d-db35-43c8-b0bc-1fc625aaeaa5",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>4. Create td catalogue using the tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here we will create a catalogue from the tables which are present in the database.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In order to create catalogue, we will first create tables realted to different use cases. We are creating these tables here for showcasing the usecase. In actual production system there will be existing schemas and tables on which we can create the catalogue.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cb1a9-315c-4ef0-bffa-52ebca20ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call get_data('DEMO_BankChurn_cloud');\"\n",
    "%run -i ../run_procedure.py \"call get_data('DEMO_UAF_cloud');\"\n",
    "%run -i ../run_procedure.py \"call get_data('DEMO_TelcoNetwork_cloud');\"\n",
    "%run -i ../run_procedure.py \"call get_data('DEMO_5G_cloud');\"\n",
    "%run -i ../run_procedure.py \"call get_data('DEMO_SalesForecasting_cloud');\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353b433-07f8-44b0-8ead-1e1845fcf55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"CREATE TABLE td_catalog_for_rag\n",
    "    as\n",
    "    (\n",
    "        SELECT \n",
    "            sum(1) over( rows unbounded preceding ) as id,\n",
    "            schema as txt\n",
    "        FROM(\n",
    "            SELECT\n",
    "                'Database: ' || DATABASENAME ||', Table: ' || TABLENAME || ', Columns: ' || TRIM(TRAILING ',' \n",
    "                 FROM (XMLAGG(TRIM(ColumnName) || ', ' ORDER BY ColumnId)(VARCHAR(10000)))) AS Schema\n",
    "            FROM dbc.columnsV\n",
    "            WHERE TableName NOT LIKE 'ml__%' \n",
    "            and DataBasename like any ('DBC','Demo%','mldb')\n",
    "            GROUP BY DATABASENAME, TABLENAME \n",
    "        ) as x\n",
    "    ) with data\n",
    ";\"\"\"\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "    print('Table Created')\n",
    "except:\n",
    "    db_drop_table('td_catalog_for_rag')\n",
    "    execute_sql(qry)\n",
    "    print('Table Created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e09c3-9793-4797-b9f7-4832007c8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame('td_catalog_for_rag')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c674018-74d2-4784-b7ba-191a91152630",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>5. Generate embeddings on the TD catalogue</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will create embeddings for the catalogue.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5399175-a890-4585-a4a5-12a91d2a717b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "Now it's time to generate the embeddings using <b>ONNXEmbeddings</b>.<br>We run the ONNXEmbeddings function to generate embeddings for a small subset of records. The model is <b>loaded into the cache memory on each node</b>, and Teradata's <b>Massively Parallel Processing (MPP)</b> architecture ensures that embeddings are computed in parallel using <b>ONNX Runtime</b> on each node.  <br>Having said that, generating embeddings for the entire training set can be time-consuming, especially when working on a system with limited resources. In the <b>ClearScape Analytics experience</b>, only a <b>4 AMP system</b> with constrained RAM and CPU power is available. To ensure smooth execution, we test embedding generation on a small sample and use <b>pre-calculated embeddings</b> for the remainder of demo. In a real-life scenario you would tyipically encounter multiple hundred AMPs with much more compute power!<br>Also have a look at the most important input parameters of this <b>ONNXEmbeddings</b> function.\n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "<li><b>InputTable</b>: The source table containing the text to be embedded. </li>\n",
    "<li><b>ModelTable</b>: The table storing the ONNX model.                    </li>\n",
    "<li><b>TokenizerTable</b>: The table storing the tokenizer JSON file.       </li>\n",
    "<li><b>Accumulate</b>: Specifies additional columns to retain in the output </li>  \n",
    "<li><b>OutputFormat</b>: Specifies the data format of the output embeddings (<b>FLOAT32(354)</b>, matching the model's output dimension).</li>\n",
    "</ul>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "Since embedding generation is computationally expensive, we only process <b>100 records for testing</b> and rely on precomputed embeddings for further analysis.  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa294b9b-66e3-4919-962a-a97ceedc85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = DataFrame.from_query(f\"select * from embeddings_models where model_id = '{model_name}'\")\n",
    "my_tokenizer = DataFrame.from_query(f\"select model as tokenizer from embeddings_tokenizers where model_id = '{model_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f2055-cfcb-4d70-8576-757fdde9fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.iloc[:100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07fddd4-0a48-48bc-94ee-6bf76ac2b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_dimensions_output = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ea35e-ec82-41a7-9db8-5c7802790be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_embeddings_sample = ONNXEmbeddings(\n",
    "    newdata = df_sample,\n",
    "    modeldata = my_model, \n",
    "    tokenizerdata = my_tokenizer, \n",
    "    accumulate = [\"id\", \"txt\"],\n",
    "    model_output_tensor = \"sentence_embedding\",\n",
    "    output_format = f'FLOAT32({number_dimensions_output})',\n",
    "    enable_memory_check = False\n",
    ").result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078743e9-bdef-4c68-85d9-cdc0da50bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_embeddings_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec155a-9b7c-41f4-9d23-2aeab81b2c43",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'> Here we can see how the embeddings are generated for the catalogue data. For further analysis we will use the precomputed embeddings.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ad166-1aa3-4b2b-90db-b1fa3a825e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emb = DataFrame(in_schema(\"DEMO_SLMRAG_Catalogue\",\"Catalogue_Embedding_Data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb302a-7795-4b8f-840a-730bdfabb7c0",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>6. Insert Prompts into a Table</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31bb37-ea9b-4858-a6f4-600413bec442",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will create the required table and than we will insert different values for the prompts.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5908a2-9da4-4310-8507-075c36af4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''CREATE MULTISET TABLE rag_topics_of_interest(\n",
    "      txt VARCHAR(1024) CHARACTER SET UNICODE NOT CASESPECIFIC,\n",
    "      id INT) NO PRIMARY INDEX''' ;\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "except:\n",
    "    db_drop_table('rag_topics_of_interest')\n",
    "    execute_sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9447f-0080-4bb9-adff-200b5aa02523",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will create prompts for different questions that can be answered from the document. Below are some sample questions that can be asked.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74565e-0342-4850-82d0-2580f6b7a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"I have to demo Teradata features to a Telco network for use case they have. What database or tables \\\n",
    " can I use to prepare for my demo so the presentation is relevant ? Can you write some Teradata SQL queries around these \\\n",
    " tables. Make sure you prefix the relevant database name in front of the tables in the queries\",\n",
    "           \"I have to demo Teradata features to a Bank customer churn for some use case they have. What database or tables \\\n",
    " can I use to prepare for my demo so the presentation is relevant ? Can you write some Teradata SQL queries around these \\\n",
    " tables. Make sure you prefix the relevant database name in front of the tables in the queries\",\n",
    "           \"I have to demo Teradata features for Sales Forecasting for some use case they have. What database or tables \\\n",
    " can I use to prepare for my demo so the presentation is relevant ? Can you write some Teradata SQL queries around these \\\n",
    " tables. Make sure you prefix the relevant database name in front of the tables in the queries\",\n",
    "           \"What logging tables are available in Teradata to check AWT usage ?\",\n",
    "           \"What logging tables are available in Teradata to check CPU usage ?\",\n",
    "           \"What metadata tables are available in Teradata to DBQL Details ?\",\n",
    "           \"What BYOM functions are available in Teradata?\"]\n",
    "\n",
    "for idx, prompt in enumerate(prompts, start=1):\n",
    "    execute_sql(f'''INSERT into rag_topics_of_interest values ('{prompt}', {idx});''')\n",
    "    # print(f'''INSERT into rag_topics_of_interest values ('{prompt}', {idx});''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e112f-640a-41e8-96ef-7aff734be076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame('rag_topics_of_interest')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7349ce1-bba5-44fb-b4f8-a64d07ade6c7",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>7. Generate Embeddings from the Prompts</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will create embeddings for the prompts which we have inserted into the table above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327208b-2e33-4894-b5f7-500c0dc7668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_rag_embeddings = ONNXEmbeddings(\n",
    "    newdata = DataFrame('rag_topics_of_interest'),\n",
    "    modeldata = my_model, \n",
    "    tokenizerdata = my_tokenizer, \n",
    "    accumulate = [\"id\", \"txt\"],\n",
    "    model_output_tensor = \"sentence_embedding\",\n",
    "    output_format = f'FLOAT32({number_dimensions_output})',\n",
    "    enable_memory_check = False\n",
    ").result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c0a11-c2da-42ff-a5e8-4e452c57d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(DF_rag_embeddings,table_name='rag_topics_embeddings_store', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a94a1f-6eb7-4836-bcd9-2657a6c4bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame('rag_topics_embeddings_store')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bafc59-63d9-418d-a256-8acb0bab089b",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>8. Find top 10 matching chunks</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will find the top 10 chunks that match the queries using the TD_VectorDistance. The TD_VectorDistance function accepts a table of target vectors and a table of reference vectors and returns a table that contains the distance between target-reference pairs. The function computes the distance between the target pair and the reference pair from the same table. We must have the same column order in the TargetFeatureColumns argument and the RefFeatureColumns argument. The function ignores the feature values during distance computation if the value is either NULL, NAN, or INF.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc39b67e-9b20-4328-a740-b165fb32893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry=\"\"\"create multiset table rag_semantic_search_results\n",
    "as (\n",
    "SELECT \n",
    "    dt.target_id,\n",
    "    dt.reference_id,\n",
    "    e_tgt.txt as target_txt,\n",
    "    e_ref.txt as reference_txt,\n",
    "    (1.0 - dt.distance) as similarity \n",
    "FROM\n",
    "    TD_VECTORDISTANCE (\n",
    "        ON rag_topics_embeddings_store AS TargetTable\n",
    "        ON DEMO_SLMRAG_Catalogue.Catalogue_Embedding_Data AS ReferenceTable DIMENSION\n",
    "        USING\n",
    "            TargetIDColumn('id')\n",
    "            TargetFeatureColumns('[emb_0:emb_383]')\n",
    "            RefIDColumn('id')\n",
    "            RefFeatureColumns('[emb_0:emb_383]')\n",
    "            DistanceMeasure('cosine')\n",
    "            topk(10)\n",
    "    ) AS dt\n",
    "JOIN rag_topics_embeddings_store e_tgt on e_tgt.id = dt.target_id\n",
    "JOIN DEMO_SLMRAG_Catalogue.Catalogue_Embedding_Data e_ref on e_ref.id = dt.reference_id\n",
    ") with data;\"\"\"\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "    print('Table Created')\n",
    "except:\n",
    "    db_drop_table('rag_semantic_search_results')\n",
    "    execute_sql(qry)\n",
    "    print('Table Created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d93ea9-e0db-46b8-b837-5e4ec958173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame('rag_semantic_search_results').to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62fa75b-b96c-4fb4-b212-b93a56e933f6",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<a id=\"rule\"></a>\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>9. Create Context and Prompt for LLM</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will create context and prepare instructions and prompt to the LLM.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb71199-66ca-40ca-9fbf-9a388bbdf82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"I have to demo Teradata features to a Telco network for use case they have. What database or tables \\\n",
    " can I use to prepare for my demo so the presentation is relevant ? Can you write some Teradata SQL queries around these \\\n",
    " tables. Make sure you prefix the relevant database name in front of the tables in the queries\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455776b7-194d-403c-af57-eca93cb26b98",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Below are some options available.</p>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"I have to demo Teradata features to a Bank customer churn for some use case they have. What database or tables \\\n",
    " can I use to prepare for my demo so the presentation is relevant ? Can you write some Teradata SQL queries around these \\\n",
    " tables. Make sure you prefix the relevant database name in front of the tables in the queries\"]</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"I have to demo Teradata features for Sales Forecasting for some use case they have. What database or tables \\\n",
    " can I use to prepare for my demo so the presentation is relevant ? Can you write some Teradata SQL queries around these \\\n",
    " tables. Make sure you prefix the relevant database name in front of the tables in the queries\"]</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"What logging tables are available in Teradata to check AWT usage ?\"]</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"What tables are available in Teradata to check CPU usage ?\"]</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"What metadata tables are available in Teradata to DBQL Details ?\"]</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"What BYOM functions are available in Teradata?\"]</li>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43947f-1ac5-4208-bea9-7eac2159c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = str.join('\\n',df['reference_txt'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40dbf2-0fa4-4024-85cc-9051d8d933e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_query = \"Answer the question based only on the following context: \" + context + \\\n",
    "\"Answer the question based on the above context: \" + prompt[0] + \\\n",
    "\"\"\" \n",
    "Provide a detailed answer.\n",
    "Don’t justify your answers.\n",
    "Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
    "Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8f8fc-be28-4b01-9300-33c31656d842",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>10. Configuring AWS CLI and Initialize Bedrock Model</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following cell will prompt us for the following information:</p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "<li><b>aws_access_key_id</b>: Enter your AWS access key ID</li>\n",
    "<li><b>aws_secret_access_key</b>: Enter your AWS secret access key</li>\n",
    "<li><b>region name</b>: Enter the AWS region you want to configure (e.g., us-east-1)</li>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421501f5-cbb5-403d-8232-6e5e3fa53e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Prompt for credentials securely\n",
    "aws_access_key_id = getpass.getpass(\"Enter AWS Access Key ID: \")\n",
    "aws_secret_access_key = getpass.getpass(\"Enter AWS Secret Access Key: \")\n",
    "region_name = getpass.getpass(\"Enter AWS Region (e.g., us-east-1): \")\n",
    "aws_session_token = getpass.getpass(\"Enter AWS Session Token (if any, else leave blank): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af130eb2-c370-4a94-b2f2-aaa935d2e6a2",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial'>Initialize the Bedrock Model</b>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "<li>The code below initializes a Boto3 client for the “bedrock-runtime” service.</li>\n",
    "<li>We provide the model to be used.</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3221d-bd0c-4890-936a-56882ef109e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bedrock Runtime client with user-provided credentials\n",
    "client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=region_name,\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_session_token\n",
    ")\n",
    "\n",
    "\n",
    "# Set the model ID\n",
    "model_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "\n",
    "print(\"Bedrock client successfully created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a21d8f-edaa-44c1-ae88-9b521387f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Boto3 client for the \"bedrock-runtime\" service in the us-east-1 region\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\", region_name='us-east-1')\n",
    "\n",
    "def get_llm():\n",
    "    # Create a Bedrock model with specific configuration options\n",
    "    return Bedrock(\n",
    "        model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
    "        client=bedrock,\n",
    "        model_kwargs={\n",
    "            'temperature': 0.2,\n",
    "            'max_tokens' : 200\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Get the Bedrock model\n",
    "\n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25cf75-b714-4943-90ab-7e7e8e6237a9",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>11. Pass the question and get Answer from the catalogue</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following cell will pass the question to the llm model and get the answer using the embeddings created from the catalogue and the prompts.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996470b9-e3e5-422c-b20e-0358f2cc359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Prepare the message for the model ----\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": llm_query}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16852e-e970-4004-81c4-7e09d3ad69ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Call the model ----\n",
    "try:\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        inferenceConfig={\"maxTokens\": 200, \"temperature\": 0.2}\n",
    "    )\n",
    "\n",
    "    # ---- Extract and print model response ----\n",
    "    output_text = response['output']['message']['content'][0]['text']\n",
    "    print(\"✅ Model response:\")\n",
    "    print(output_text)\n",
    "\n",
    "except ClientError as e:\n",
    "    print(f\"❌ Error testing client: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c982704-46f2-4dd6-a6a8-4a2fe12fd6f8",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In case you want to check answer for some other question please enter the question again <a href='#rule'>here</a> and run the following steps again.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba96cad-5452-4391-aa77-2616b0ee6fc7",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>12. Cleanup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1c10e-50bb-4f3e-ba27-90c032fa7027",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'> <b>Work Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1d17d-a683-425b-a9ef-42e295ec7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['td_catalog_for_rag','rag_topics_embeddings_store','rag_topics_of_interest','rag_semantic_search_results']\n",
    "\n",
    "# Loop through the list of tables and execute the drop table command for each table\n",
    "for table in tables:\n",
    "    try:\n",
    "        db_drop_table(table_name=table)\n",
    "    except:\n",
    "        pass  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1895ad3-9dc5-47c9-a457-fcfb245b7ce2",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'> <b>Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcba03a-2d0a-4b7b-863a-8905f985a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_BankChurn');\"\n",
    "%run -i ../run_procedure.py \"call remove_data('DEMO_UAF');\"\n",
    "%run -i ../run_procedure.py \"call remove_data('DEMO_TelcoNetwork');\"\n",
    "%run -i ../run_procedure.py \"call remove_data('DEMO_5G');\"\n",
    "%run -i ../run_procedure.py \"call remove_data('DEMO_SalesForecasting');\"\n",
    "%run -i ../run_procedure.py \"call remove_data('DEMO_SLMRAG_Catalogue');\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d31d5-f54f-4fe9-b1f6-9279d15f6d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02e3f0-3980-4034-896b-71e7ab4fd880",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid #91A0Ab\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2024. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
