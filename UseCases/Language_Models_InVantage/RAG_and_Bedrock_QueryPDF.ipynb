{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d7b831-249f-4775-9d56-d9d9aec81f54",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background- padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       RAG solution with Embedding/Chunking in Vantage and AWS Bedrock integration\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Introduction</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demo we will explore how to do chunk PDFs, run embedding, try vector db style indexing in Vantage and than query LLM with context/prompts after semantic search</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We use IVSM functions for supporting text summarization and embedding models using Hugging Face PyTorch models. In Tok/Detok UDFs we are using the open source Java library originally developed by AWS named DeepJavaLibrary (DJL). This library has a restriction that certain classes could be only loaded into memory once during the JVM lifecycle. In a meanwhile Teradata Java UDF mechanism has a smart class loading mechanism which in a standard situation gives us performance and flexibility could lead to the violation of DJL restriction. To avoid this situation, we are using so called Java Extension Mechanism. This mechanism guarantees only once initialization of classes from some library.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Teradata has Integration with LLMs with Amazon BedRock etc., and also emerging Open Analytics Framework in the Cloud Lake, where you can host a Language Model etc.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Known challenges of LLMs include:</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial'>Presenting false information when it does not have the answer.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Presenting out-of-date or generic information when the user expects a specific, current response.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Creating a response from non-authoritative sources.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.</li>\n",
    "</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demo, we will work with some PDF parsing and chunking with a Teradata SEC-10K PDF/LLM to answer prompts</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d10764-d095-45bb-9017-f4d599a74414",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>1. Configuring the environment</b>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.1 Install the required libraries</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66e62d-a6d2-4af8-9802-004bbfb2de7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain_community pypdf\n",
    "!pip install boto3 awscli\n",
    "!pip install pyopenssl --upgrade --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b214bd0-c408-48cb-9ec2-b843eafa7207",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Note: </b><i>Please restart the kernel after executing these two lines. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e6f39-2b02-465a-aa33-b3c2d36bc327",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.2 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4228628-d8bb-459c-bdfc-4088902a57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "import pandas as pd\n",
    "from teradataml import *\n",
    "import getpass\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import display, Markdown\n",
    " \n",
    "display.max_rows=5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c3596-bc64-45b2-a828-53fd8228177b",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>1.3. Connect to Vantage</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01baa577-2526-405a-90f0-a1977a20a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host='host.docker.internal', username='demo_user', password=password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e462d-9e75-47f5-b281-6c171b0c4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql(\"SET query_band='DEMO=Language_Model_RAG_PDF_Python.ipynb;' UPDATE FOR SESSION;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6917041c-b7f7-428e-93de-6a8f16d08ee6",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>2. Confirmation for functions</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Before starting let us confirm that the required functions are installed.</p>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f08f5be-0504-4c2c-9c09-44edd6dfc2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check= DataFrame.from_query('''select count(*) as cnt from dbc.tablesV where databasename = 'ivsm';''')\n",
    "if df_check.get_values()[0][0] >= 10:\n",
    "    print('Functions are installed, please continue.')\n",
    "else:\n",
    "    print('Functions are not installed, please go to Instalization notebook before proceeding further')\n",
    "    display(Markdown(\"[Initialization Notebook](./Initialization_and_Model_Load.ipynb)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c15a4-a3bd-4bcb-98e7-efced66c3424",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>3. Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo on cloud storage. Since we are using embeddings stored in Vantage for this demo we are only using the local storage for the demo. We will only use the option of creating table locally.</p>   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b94e0-bb25-46a2-a6a9-5eedaa1d39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call get_data('DEMO_SLM_RAG_local');\"\n",
    " # Takes about 2 minutes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad64b842-fd24-44f2-8e20-f35150e56299",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Optional step – We should execute the below step only if we want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ba64d-11f6-4fb5-a52f-51cb6f379955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88494f15-4d3b-466e-bde7-9d458032b3e8",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>4. Parse PDF and Create Chunks</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here we will parse the PDF which is available to us in the left pane: <b>Teradata-sec10k.pdf</b>. Then we will use the <b>RecursiveCharacterTextSplitter</b> from langchain package. This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text. The text is split by list of characters and the chunk size is measured by number of characters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9d870-dfde-4b37-9e5a-c63ac31c891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATH = \"./Teradata-sec10k.pdf\"\n",
    "\n",
    "# load your pdf doc\n",
    "loader = PyPDFLoader(DOC_PATH)\n",
    "pages = loader.load()\n",
    "\n",
    "\n",
    "# split the doc into smaller chunks i.e. chunk_size=500\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=75)\n",
    "chunks = text_splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e741c-f9cd-445c-b42a-645271e1553f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>From the chunks created we will extract the text and create a dataframe from the extracted texts.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a268ad-e48e-4c6f-94ab-38c24edccab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract chunk info to a Dataframe with a column named 'txt'\n",
    "\n",
    "df = pd.DataFrame(chunks)\n",
    "df['junk'], df['txt'] = zip(*df[2])\n",
    "df['txt'] = df['txt'].astype(str)\n",
    "df = df['txt']\n",
    "#df = df.replace(r'\\n',' ', regex=True)\n",
    "df = df.reset_index(drop=True).to_frame(name='txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c5a05-8642-4ccc-8244-f21e2b9d1b63",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will load these texts in a Vantage table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69143a77-e612-4e7b-aede-0b43566b4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chunks to the table called 'pdf_contents'\n",
    "copy_to_sql(df, table_name='pdf_contents', index=True, if_exists='replace')\n",
    "\n",
    "# Fix the id column\n",
    "execute_sql(\"ALTER TABLE pdf_contents rename index_label to id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc26f0d-d66a-4dee-9840-8e72caf61a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame('pdf_contents')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb7dce8-4a9f-4631-8807-0a985eaf4d35",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>5. Generate embeddings from the chunks</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will create prompts for different questions that can be answered from the document. Below are some sample questions that can be asked.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca8f97-c763-4969-980c-cdd1ff0b0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"replace view v_pdf_tokenized_for_embeddings  as (\n",
    "    select\n",
    "        id,\n",
    "        txt,\n",
    "        IDS as input_ids,\n",
    "        attention_mask\n",
    "    from ivsm.tokenizer_encode(\n",
    "        on (select * from pdf_contents)\n",
    "        on (select model as tokenizer from embeddings_tokenizers where model_id = 'bge-small-en-v1.5') DIMENSION\n",
    "        USING\n",
    "            ColumnsToPreserve('id', 'txt')\n",
    "            OutputFields('IDS', 'ATTENTION_MASK')\n",
    "            MaxLength(1024)\n",
    "            PadToMaxLength('True')\n",
    "            TokenDataType('INT64')\n",
    "    ) a\n",
    ");\"\"\"\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "    print('View Created')\n",
    "except:\n",
    "    print(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97934bc5-cf41-4ce7-be7f-2e06dc5ceb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry=\"\"\"replace view pdf_embeddings as (\n",
    "    select \n",
    "            *\n",
    "    from ivsm.IVSM_score(\n",
    "            on v_pdf_tokenized_for_embeddings  -- table with data to be scored\n",
    "            on (select * from embeddings_models where model_id = 'bge-small-en-v1.5') dimension\n",
    "            using\n",
    "                ColumnsToPreserve('id', 'txt') -- columns to be copied from input table\n",
    "                ModelType('ONNX') -- model format\n",
    "                BinaryInputFields('input_ids', 'attention_mask') -- enables binary input vector\n",
    "                BinaryOutputFields('sentence_embedding')\n",
    "                Caching('inquery') -- tun on model caching within the query\n",
    "        ) a \n",
    ");\"\"\"\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "    print('View Created')\n",
    "except:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed2fb9-76fe-450d-a8d8-af9862a9cf06",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'><b> Do you want to generate the embeddings?</b></p>    \n",
    "<p style = 'font-size:16px;font-family:Arial'>Generating embeddings will take around <b>35-40 minutes.</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have already generated embeddings for the pdf and stored them in <b>Vantage</b> table.</p>\n",
    " \n",
    "<center><img src=\"images/decision_emb_gen_2.svg\" alt=\"embeddings_decision\" width=300 height=400/></center>\n",
    " \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><i><b>Note: If you would like to skip the embedding generation step to save the time and move quickly to next step, please enter \"No\" in the next prompt.</b></i></p>\n",
    "</div>\n",
    " \n",
    "<p style = 'font-size:16px;font-family:Arial'>To save time, you can move to the already generated embeddings section. However, if you would like to see how we generate the embeddings, or if you need to generate the embeddings for a different dataset, then continue to the following section.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d3592-ad4b-4afa-832c-902e610d36f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# Request user's input\n",
    "generate = input(\"Do you want to generate embeddings? ('yes'/'no'): \")\n",
    "\n",
    "# Check the user's input\n",
    "if generate.lower() == 'yes':\n",
    "    print(\"\\nGreat! We'll start by generating embeddings.\")\n",
    "\n",
    "    print(\"\\nGenerating embeddings and Saving to the database, please wait...\")\n",
    "    # start = time.time()\n",
    "    qry=\"\"\"create table pdf_embeddings_store as (\n",
    "    select \n",
    "    *\n",
    "    from ivsm.vector_to_columns(\n",
    "            on pdf_embeddings\n",
    "            using\n",
    "                ColumnsToPreserve('id', 'txt') \n",
    "                VectorDataType('FLOAT32')\n",
    "                VectorLength(384)\n",
    "                OutputColumnPrefix('emb_')\n",
    "                InputColumnName('sentence_embedding')\n",
    "        ) a ) with data ;\"\"\"\n",
    "\n",
    "    try:\n",
    "        execute_sql(qry)\n",
    "        # end = time.time()\n",
    "        print('Table Created')\n",
    "        # print(end-start)\n",
    "        \n",
    "    except:\n",
    "        db_drop_table('pdf_embeddings_store')\n",
    "        execute_sql(qry)\n",
    "        # end = time.time()\n",
    "        print('Table Created')\n",
    "        # print(end-start)\n",
    "\n",
    "\n",
    "    print(\"\\nEmbeddings generated and saved successfully!\")\n",
    "\n",
    "elif generate.lower() == 'no':\n",
    "    print(\"\\nLoading embeddings from the Vantage table\")\n",
    "    # Save them to SQL\n",
    "    df_emb = DataFrame(in_schema(\"DEMO_SLM_RAG\",\"Pdf_Embedding_Data\"))\n",
    "    copy_to_sql(\n",
    "        df = df_emb,\n",
    "        table_name = 'pdf_embeddings_store',\n",
    "        if_exists = 'replace'\n",
    "    )\n",
    "\n",
    "    print(\"\\nEmbeddings loaded and saved successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nInvalid input. Please enter 'yes' or 'no' to proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dece6f-cdc7-4714-b75d-8e8384c2b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame('pdf_embeddings_store')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ec437-e0ce-4f67-8aad-a7a4f7de9064",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>6. Insert Prompts into a Table</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceade411-5a69-46b3-b2e1-7e50da961d45",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will create the required table and than we will insert different values for the prompts.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319d050-6735-4838-b7a9-703c6071c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''CREATE MULTISET TABLE pdf_topics_of_interest(\n",
    "      txt VARCHAR(1024) CHARACTER SET UNICODE NOT CASESPECIFIC,\n",
    "      id INT) NO PRIMARY INDEX''' ;\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "except:\n",
    "    db_drop_table('pdf_topics_of_interest')\n",
    "    execute_sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396f9b4-576d-4a62-aae9-86cdbb1520df",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will create prompts for different questions that can be answered from the document. Below are some sample questions that can be asked.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b50dc0-95e8-443f-87b9-27d538a32388",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"What is this document about ? Which company is this document for ?\",\n",
    "           \"Where is Teradata HQ located ? Which countries are Teradata has offices ?\",\n",
    "           \"What is the annual revenue of Teradata and when does the fiscal year end ?\",\n",
    "           \"What products does Teradata sell ? What is Teradatas business ?\",\n",
    "           \"How does Teradata think about leveraging Artificial Intelligence into its products ? What about Cloud Strategy ?\\\n",
    "Can you explain this both in Spanish and English ?\",\n",
    "           \"What is the revenue breakdown of products sold across different countries/regions ? \\\n",
    "Which countries/regions performed the best ? Format the output in markdown HTML\",\n",
    "           \"Is Teradata profitable ? How ? Whats the secret ?\"]\n",
    "\n",
    "for idx, prompt in enumerate(prompts, start=1):\n",
    "    execute_sql(f'''INSERT into pdf_topics_of_interest values ('{prompt}', {idx});''')\n",
    "    # print(f'''INSERT into pdf_topics_of_interest values ('{prompt}', {idx});''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def5178-aaaa-4443-896c-4895b4bcb1cb",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>7. Generate Embeddings from the Prompts</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will create embeddings for the prompts which we have inserted into the table above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ea4a4-77d9-42da-8156-681dc1b3af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"replace view v_pdf_topics_tokenized_for_embeddings as (\n",
    "    select\n",
    "        id,\n",
    "        txt,\n",
    "        IDS as input_ids,\n",
    "        attention_mask\n",
    "    from ivsm.tokenizer_encode(\n",
    "        on (select * from pdf_topics_of_interest)\n",
    "        on (select model as tokenizer from embeddings_tokenizers where model_id = 'bge-small-en-v1.5') DIMENSION\n",
    "        USING\n",
    "            ColumnsToPreserve('id', 'txt')\n",
    "            OutputFields('IDS', 'ATTENTION_MASK')\n",
    "            MaxLength(1024)\n",
    "            PadToMaxLength('True')\n",
    "            TokenDataType('INT64')\n",
    "    ) a\n",
    ");\"\"\"\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "    print('View Created')\n",
    "except:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84458efc-d355-4d2e-aec0-c8513e299553",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry=\"\"\"replace view pdf_topics_embeddings as (\n",
    "    select \n",
    "            *\n",
    "    from ivsm.IVSM_score(\n",
    "            on v_pdf_topics_tokenized_for_embeddings  -- table with data to be scored\n",
    "            on (select * from embeddings_models where model_id = 'bge-small-en-v1.5') dimension\n",
    "            using\n",
    "                ColumnsToPreserve('id', 'txt') -- columns to be copied from input table\n",
    "                ModelType('ONNX') -- model format\n",
    "                BinaryInputFields('input_ids', 'attention_mask') -- enables binary input vectors\n",
    "                BinaryOutputFields('sentence_embedding')\n",
    "                Caching('inquery') -- tun on model caching within the query\n",
    "        ) a \n",
    ");\"\"\"\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "    print('View Created')\n",
    "except:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a41444f-1b9f-4110-874d-6fb858add974",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"create table pdf_topics_embeddings_store as (\n",
    "    select \n",
    "            *\n",
    "    from ivsm.vector_to_columns(\n",
    "            on pdf_topics_embeddings\n",
    "            using\n",
    "                ColumnsToPreserve('id', 'txt') \n",
    "                VectorDataType('FLOAT32')\n",
    "                VectorLength(384)\n",
    "                OutputColumnPrefix('emb_')\n",
    "                InputColumnName('sentence_embedding')\n",
    "        ) a \n",
    ") with data;\"\"\"\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "    print('Table Created')\n",
    "except:\n",
    "    db_drop_table('pdf_topics_embeddings_store')\n",
    "    execute_sql(qry)\n",
    "    print('Table Created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60e750-76ab-4d37-8a7d-49de9a3adadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame('pdf_topics_embeddings_store')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c38ea0-3aad-4582-82ce-642ad5c07913",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>8. Find top 10 matching chunks</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will find the top 10 chunks that match the queries using the TD_VectorDistance. The TD_VectorDistance function accepts a table of target vectors and a table of reference vectors and returns a table that contains the distance between target-reference pairs. The function computes the distance between the target pair and the reference pair from the same table. We must have the same column order in the TargetFeatureColumns argument and the RefFeatureColumns argument. The function ignores the feature values during distance computation if the value is either NULL, NAN, or INF.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3c5b4-bd88-43f4-958f-290c26f942ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry=\"\"\"create multiset table pdf_semantic_search_results\n",
    "as (\n",
    "SELECT \n",
    "    dt.target_id,\n",
    "    dt.reference_id,\n",
    "    e_tgt.txt as target_txt,\n",
    "    e_ref.txt as reference_txt,\n",
    "    (1.0 - dt.distance) as similarity \n",
    "FROM\n",
    "    TD_VECTORDISTANCE (\n",
    "        ON pdf_topics_embeddings_store AS TargetTable\n",
    "        ON pdf_embeddings_store AS ReferenceTable DIMENSION\n",
    "        USING\n",
    "            TargetIDColumn('id')\n",
    "            TargetFeatureColumns('[emb_0:emb_383]')\n",
    "            RefIDColumn('id')\n",
    "            RefFeatureColumns('[emb_0:emb_383]')\n",
    "            DistanceMeasure('cosine')\n",
    "            topk(10)\n",
    "    ) AS dt\n",
    "JOIN pdf_topics_embeddings_store e_tgt on e_tgt.id = dt.target_id\n",
    "JOIN pdf_embeddings_store e_ref on e_ref.id = dt.reference_id\n",
    ") with data;\"\"\"\n",
    "\n",
    "try:\n",
    "    execute_sql(qry)\n",
    "    print('Table Created')\n",
    "except:\n",
    "    db_drop_table('pdf_semantic_search_results')\n",
    "    execute_sql(qry)\n",
    "    print('Table Created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c344b850-fc45-4d79-aae1-cb42f86862cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame('pdf_semantic_search_results').to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e1d3ba-5d34-4ca7-b0d2-2b421d0bde3b",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<a id=\"rule\"></a>\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>9. Create Context and Prompt for LLM</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will create context and prepare instructions and prompt to the LLM.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870d938b-9710-4dae-af7c-8f84bf068313",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"Where is Teradata HQ located ? Which countries are Teradata has offices ?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b264eb-e512-4585-ab9e-17a03c6e1aa6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Below are some options available.</p>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"What is this document about ? Which company is this document for ?\"]</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"Where is Teradata HQ located ? Which countries are Teradata has offices ?\"]</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"What is the annual revenue of Teradata and when does the fiscal year end ?\"]</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"What products does Teradata sell ? What is Teradata's business ?\"]</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"How does Teradata think about leveraging Artificial Intelligence into its products ? What about Cloud Strategy ? Can you explain this both in Spanish and English ?\"]</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"What is the revenue breakdown of products sold across different countries/regions ? \\\n",
    "Which countries/regions performed the best ? Format the output in markdown HTML\"]</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'> prompt = [\"Is Teradata profitable ? How ? Whats the secret ?\"]</li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770bca6-732f-4302-98ee-9f83a05bfd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = str.join('\\n',df['reference_txt'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ea00b-e539-466e-b729-1f25e338114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_query = \"Answer the question based only on the following context: \" + context + \\\n",
    "\"Answer the question based on the above context: \" + prompt[0] + \\\n",
    "\"\"\" \n",
    "Provide a detailed answer.\n",
    "Don’t justify your answers.\n",
    "Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
    "Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2104d119-7169-485e-8bf1-d3212ed75ff6",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>10. Configuring AWS CLI and Initialize Bedrock Model</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following cell will prompt us for the following information:</p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "<li><b>aws_access_key_id</b>: Enter your AWS access key ID</li>\n",
    "<li><b>aws_secret_access_key</b>: Enter your AWS secret access key</li>\n",
    "<li><b>region name</b>: Enter the AWS region you want to configure (e.g., us-east-1)</li>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d36de-9527-41d3-926f-8cb8c1b8f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_aws():\n",
    "    print(\"configure the AWS CLI\")\n",
    "    # enter the access_key/secret_key\n",
    "    access_key = getpass.getpass(\"aws_access_key_id \")\n",
    "    secret_key = getpass.getpass(\"aws_secret_access_key \")\n",
    "    region_name = getpass.getpass(\"region name\")\n",
    "\n",
    "    #set to the env\n",
    "    !aws configure set aws_access_key_id {access_key}\n",
    "    !aws configure set aws_secret_access_key {secret_key}\n",
    "    !aws configure set default.region {region_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd6906-f576-49b5-a155-eced7ee47493",
   "metadata": {},
   "outputs": [],
   "source": [
    "does_access_key_exists = !aws configure get aws_access_key_id\n",
    "\n",
    "if len(does_access_key_exists) == 0:\n",
    "    configure_aws()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9e1b8-9032-429f-88c1-e6bbdbd0e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841ce98-844c-4467-8831-2e244f193f19",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial'>Initialize the Bedrock Model</b>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "<li>The code below initializes a Boto3 client for the “bedrock-runtime” service.</li>\n",
    "<li>The get_llm() function creates a Bedrock language model with specific configuration options.</li>\n",
    "<li>The model can be used for natural language generation tasks.</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a164a-fa34-4ee4-982e-a82a1c7bb38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Boto3 client for the \"bedrock-runtime\" service in the us-east-1 region\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\", region_name='us-east-1')\n",
    "\n",
    "def get_llm():\n",
    "    # Create a Bedrock model with specific configuration options\n",
    "    return Bedrock(\n",
    "        model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
    "        client=bedrock,\n",
    "        model_kwargs={\n",
    "            'temperature': 0.2,\n",
    "            'max_tokens' : 100\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Get the Bedrock model\n",
    "\n",
    "llm = get_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019d1ba-cdde-4a5b-b1c3-532a551e878d",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>11. Pass the question and get Answer from the PDF</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following cell will pass the question to the llm model and get the answer using the embeddings created from the pdf and the prompts.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87784d5-9422-4721-845a-0746670616ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = llm(llm_query)\n",
    "print(\"Prompt: \"+prompt[0]+\"\\n\\n\"+\"Answer:\\n\\n\"+s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9fb1e1-8cec-4b99-919f-88854490fc1a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>In case you want to check answer for some other question please enter the question again <a href='#rule'>here</a> and run the following steps again.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd537d9-cfa0-40e5-8f14-5e8806a083c0",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>12. Cleanup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aabf96-bde5-4d40-9ab3-bb525bfb4ee7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'> <b>Work Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612fd68c-0db2-40d3-b3ba-5b80ff300355",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['pdf_contents', 'pdf_embeddings_store','pdf_topics_of_interest','pdf_topics_embeddings_store',\n",
    "          'pdf_semantic_search_results']\n",
    "\n",
    "# Loop through the list of tables and execute the drop table command for each table\n",
    "for table in tables:\n",
    "    try:\n",
    "        db_drop_table(table_name=table)\n",
    "    except:\n",
    "        pass  \n",
    "    \n",
    "views = ['v_pdf_tokenized_for_embeddings','pdf_embeddings','v_pdf_topics_tokenized_for_embeddings',\n",
    "         'pdf_topics_embeddings']   \n",
    "\n",
    "for view in views:\n",
    "    try:\n",
    "        db_drop_view(view_name=view)\n",
    "    except:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775caa6c-3067-4de5-8857-3c38aaa4a97a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'> <b>Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155d0af-0d3a-4247-a97d-13b5f9101267",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_SLM_RAG');\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c9505-635f-44bf-a379-d6862488a1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53bf1c-cbd3-4aca-a174-7f4edbf13c9d",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; background:#91A0AB; \">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2024. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
