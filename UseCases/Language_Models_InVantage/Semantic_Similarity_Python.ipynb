{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee96203-f8ae-4648-b550-5844f2fa17f1",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Semantic Similarity using Open Source Language Models in Database\n",
    "  <br>\n",
    "              <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac817186-c9b9-48c5-9150-025e5e752878",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Introduction</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>\n",
    "Semantic similarity refers to the degree to which two pieces of text, words, or concepts have similar meanings. It measures how much two entities are related based on their meanings rather than just their surface forms or literal text. The similarity can be with synonyms e.g car and automobile, with realted concepts e.g doctor and nurse or with phrases e.g \"she enjoys reading books\" and \"she loves to read\" .\n",
    "</p>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Applications of Semantic Similarity:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>There are various applications which use semantic similarity e.g\n",
    "            <li>Natural Language Processing (NLP): Used in tasks like text summarization, question-answering, and machine translation. </li>\n",
    "            <li>Information Retrieval: Helps search engines return results that are conceptually related to the user's query. </li>\n",
    "            <li>Recommendation Systems: Suggests similar items based on their semantic meaning  </li></ul>\n",
    "    </li>\n",
    " </ul>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>Why Vantage?</b></p>  \n",
    "<p style = 'font-size:16px;font-family:Arial'>Teradata has Integration with LLMs with Amazon BedRock etc and also emerging Open Analytics Framework in the Cloud Lake where we can host a Language Model etc. For many on-prem customers it is not practical to move the big NLP data out of Teradata such as complaints/emails, score it and put it back even if HF models run outside the DB. Moving huge volume of historical data from Vantage for the NLP models to transform does not provide much advantage as my latency is high. Moreover on-prem customers sometimes may not have even access to Cloud/LLMs and even Open Analytics Framework and can't get any AI going today. By bringing the language models within Vantage we can bridge the gap and enable on-prem customers to run NLP models in database.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968b80c-57ed-4f61-9b16-4aea1f7457da",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>1. Connect to Vantage</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baca5f27-67cc-4d7c-9e1e-7fa0d429832a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Let's start by importing the libraries needed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066391b5-367d-4101-8857-b47b6d767a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import getpass\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Teradata libraries\n",
    "from teradataml import *\n",
    "display.max_rows = 5\n",
    "\n",
    "#other libraries\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "configure.byom_install_location = \"mldb\"\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ad4c9-874a-479b-853c-f8e7478605ec",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14195ef8-872e-486a-80fa-361f7861ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd85a2e-7076-4594-9c98-bd32d4d393e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql(\"SET query_band='DEMO=Language_Model_Semantic_Similarity_Python.ipynb;' UPDATE FOR SESSION;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe258d2-fefb-4628-92e5-499f85fedd05",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>2. Confirmation for Model</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Before starting let us confirm that the required model is installed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a373776-f10c-4707-83e6-ee6abdddcaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bge-small-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b100ee-4de1-468b-8ee0-913518594b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "df_check= DataFrame.from_query(f'''select (select 1 as cnt from embeddings_models where model_id = '{model_name}') +\n",
    "(select 1 as cnt from embeddings_tokenizers where model_id =  '{model_name}') as cnt''')\n",
    "if df_check.get_values()[0][0] == 2:\n",
    "    print('Model is installed, please continue.')\n",
    "else:\n",
    "    print('Model is not installed, please go to Instalization notebook before proceeding further')\n",
    "    display(Markdown(\"[Initialization Notebook](./Initialization_and_Model_Load.ipynb)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ecf84-6aac-4eda-9f00-95eed92cff82",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'> <b>3. Getting Data for This Demo</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo on cloud storage. Since we are using embeddings stored in Vantage for this demo we will only use the option of creating table locally.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8ad4f-1174-46b2-b0af-1e64ff24ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call get_data('DEMO_ComplaintAnalysis_local');\"\n",
    "# takes about 30 seconds, estimated space: 3 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760e3fa-5a74-4e54-96e3-f7b25a0041b6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Optional step – We should execute the below step only if we want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b78c80-32c3-4919-9f5c-521b99b8feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13261da-bbfb-499a-be6c-169bd3d82bee",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>4. Creating Embeddings on Source Data</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d262d-fc93-4434-85be-54bd41e593d9",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The data is from Consumers Complaints from <a href = 'https://www.consumerfinance.gov'>CFPB website</a> which we have loaded in table for our demo. Let us see how the data looks like.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5424ce-9d3e-4f33-94b7-b715befa3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = DataFrame('\"DEMO_ComplaintAnalysis\".\"Consumer_Complaints\"')\n",
    "tdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c295d1b-19c2-423a-ba20-9381e6ed1062",
   "metadata": {},
   "source": [
    "<b style = 'font-size:18px;font-family:Arial;'>4.1 Generate Embeddings with ONNXEmbeddings</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8010e-ddc9-4212-a20e-e1f42177fadd",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "Now it's time to generate the embeddings using <b>ONNXEmbeddings</b>.<br>We run the ONNXEmbeddings function to generate embeddings for a small subset of records. The model is <b>loaded into the cache memory on each node</b>, and Teradata's <b>Massively Parallel Processing (MPP)</b> architecture ensures that embeddings are computed in parallel using <b>ONNX Runtime</b> on each node.  <br>Having said that, generating embeddings for the entire training set can be time-consuming, especially when working on a system with limited resources. In the <b>ClearScape Analytics experience</b>, only a <b>4 AMP system</b> with constrained RAM and CPU power is available. To ensure smooth execution, we test embedding generation on a small sample and use <b>pre-calculated embeddings</b> for the remainder of demo. In a real-life scenario you would tyipically encounter multiple hundred AMPs with much more compute power!<br>Also have a look at the most important input parameters of this <b>ONNXEmbeddings</b> function.\n",
    "<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "<li><b>InputTable</b>: The source table containing the text to be embedded. </li>\n",
    "<li><b>ModelTable</b>: The table storing the ONNX model.                    </li>\n",
    "<li><b>TokenizerTable</b>: The table storing the tokenizer JSON file.       </li>\n",
    "<li><b>Accumulate</b>: Specifies additional columns to retain in the output </li>  \n",
    "<li><b>OutputFormat</b>: Specifies the data format of the output embeddings (<b>FLOAT32(354)</b>, matching the model's output dimension).</li>\n",
    "</ul>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>\n",
    "Since embedding generation is computationally expensive, we only process <b>100 records for testing</b> and rely on precomputed embeddings for further analysis.  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c95661-94f7-486a-9daf-27de33da3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_sample = tdf.iloc[:100, :]\n",
    "tdf_sample=tdf_sample.assign(drop_columns = True,\n",
    "                             id = tdf_sample.complaint_id,\n",
    "                             txt= tdf_sample.consumer_complaint_narrative)\n",
    "tdf_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a0336-5c8e-4282-9ebd-8c9312bb1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = DataFrame.from_query(f\"select * from embeddings_models where model_id = '{model_name}'\")\n",
    "my_tokenizer = DataFrame.from_query(f\"select model as tokenizer from embeddings_tokenizers where model_id = '{model_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f0fe9-e098-4749-9d81-a05b143d5680",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_dimensions_output = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294318d7-8fc6-4d9c-8673-baeb92560714",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_embeddings_sample=ONNXEmbeddings(\n",
    "    newdata = tdf_sample,\n",
    "    modeldata = my_model, \n",
    "    tokenizerdata = my_tokenizer, \n",
    "    accumulate = [\"id\", \"txt\"],\n",
    "    model_output_tensor = \"sentence_embedding\",\n",
    "    output_format = f'FLOAT32({number_dimensions_output})',\n",
    "    enable_memory_check = False\n",
    ").result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c59ea-2446-45eb-a632-0cec082f9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_embeddings_sample.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31125596-6cab-49b3-9764-6d0e11aca91d",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'> Here we can see how the embeddings are generated for the consumer_complaint_narrative. For further analysis we will use the precomputed embeddings</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4280a-8f48-4a80-8ea3-402aee1ced8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_embeddings_store = DataFrame(in_schema('DEMO_ComplaintAnalysis', 'Complaints_Embeddings_Store'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da326562-9e6a-401d-9c16-81d1a45ed40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_embeddings_store.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6050c5b4-83df-4e21-b4c8-657197124d00",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>5. Topics Data</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751c79b-849b-4666-8fa7-e19ee115f490",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'> Now let us create a list of topics for which we will do our search.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186b0f4-7246-494d-b239-fdedab7c3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic = pd.DataFrame({'id': [1,2,3,4,5,6],\n",
    "      'txt': ['Fradulent activity with Debit Cards at Wells Fargo',\n",
    "              'Identity theft issues at Citibank',\n",
    "              'Multiple account openings without authorization',\n",
    "              'Irresponsible behavior by customer support',\n",
    "              'App issues when transacting with bank',\n",
    "              'Cant get money out of ATM',\n",
    "              ]})\n",
    "\n",
    "copy_to_sql(df_topic,table_name='topics_of_interest', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440063a-00f7-4763-8b6d-45ab0fcfdf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181d98e-f515-421d-99f7-d0d4c31244db",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>6. Generating Embedding for Topics Data</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will generate the embeddings for the Topics data as we did for source_data in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b149dd5-ee9c-4996-84c7-11977221c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_topic_embeddings = ONNXEmbeddings(\n",
    "    newdata = DataFrame('topics_of_interest'),\n",
    "    modeldata = my_model, \n",
    "    tokenizerdata = my_tokenizer, \n",
    "    accumulate = [\"id\", \"txt\"],\n",
    "    model_output_tensor = \"sentence_embedding\",\n",
    "    output_format = f'FLOAT32({number_dimensions_output})',\n",
    "    enable_memory_check = False\n",
    ").result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2532ff2-24fc-4477-ba08-379c584ca848",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_sql(DF_topic_embeddings,table_name='topics_embeddings_store', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e4f2cf-23f3-43d2-9c6b-e74f2a205ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic = DataFrame('topics_embeddings_store')\n",
    "df_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd7595-b5dd-4fa4-a611-29bba5b4280c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'> As we can see from the above, we have generated embeddings for the topic data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be16901-a641-4096-af39-69634b70e5cd",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>9. Semantic Similarity</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Now we will run Semantic Similarity of the Topics Embeddings against the Complaints Embeddings table. Vector Distance is a measure of the similarity or dissimilarity between two vectors in multidimensional space. We will use Vantage's TD_VectorDistance function. The <b>TD_VectorDistance</b> function accepts a table of target vectors and a table of reference vectors and returns a table that contains the distance between target-reference pairs. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9823b88-6863-4fd9-a6ed-4b654cab846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry= '''\n",
    "create multiset table semantic_search_results\n",
    "as (\n",
    "SELECT \n",
    "    dt.target_id,\n",
    "    dt.reference_id,\n",
    "    e_tgt.consumer_complaint_narrative as target_txt,\n",
    "    e_ref.txt as reference_txt,\n",
    "    (1.0 - dt.distance) as similarity \n",
    "FROM\n",
    "    TD_VECTORDISTANCE (\n",
    "        ON (select * from DEMO_ComplaintAnalysis.Complaints_Embeddings_Store a) AS TargetTable\n",
    "        ON topics_embeddings_store AS ReferenceTable DIMENSION\n",
    "        USING\n",
    "            TargetIDColumn('id')\n",
    "            TargetFeatureColumns('[emb_0:emb_383]')\n",
    "            RefIDColumn('id')\n",
    "            RefFeatureColumns('[emb_0:emb_383]')\n",
    "            DistanceMeasure('cosine')\n",
    "            topk(1) -- Only want the best match per complaint. If you want multi-label/multi-class - you can increase it\n",
    "    ) AS dt\n",
    "JOIN DEMO_ComplaintAnalysis.Consumer_Complaints e_tgt on e_tgt.complaint_id = dt.target_id\n",
    "JOIN topics_embeddings_store e_ref on e_ref.id = dt.reference_id\n",
    "WHERE dt.distance < 0.3 -- Cosine Similarity of 0.7 or greater\n",
    ") with data;\n",
    "'''\n",
    "\n",
    "try:\n",
    "        execute_sql(qry)\n",
    "        print(\"Semantic Search Results table created\")\n",
    "except:\n",
    "        db_drop_table('semantic_search_results')\n",
    "        execute_sql(qry)\n",
    "        print(\"Semantic Search Results table created\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727e11bd-47a0-4335-bdf5-414b73bdc0a0",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>10. Check Matches</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903352c-83fe-4e1a-9386-5cb6a9ee55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = DataFrame('semantic_search_results')\n",
    "\n",
    "#displaying the results with most simialrity first\n",
    "df_results.sort(['similarity'],ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3d0f3-ff31-4c81-94f5-20d7ca4a3cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the top 2 records for each reference_id from the similarity result created\n",
    "window = df_results.window(partition_columns=\"reference_id\",\n",
    "                           order_columns=\"similarity\",\n",
    "                           sort_ascending=False)\n",
    "df = window.rank()\n",
    "df[df.col_rank.isin([1,2])].sort(['reference_id','col_rank']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5cbed6-a573-4429-bfbf-d37d1d9e44b6",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Conclusion</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demo we have seem that how we can run HuggingFace Embedding Model (BAAI/bge-small-1.5) in ONNX format and run it in database parallelly to create embeddings. We have done Cosine Similarity match using TD_VectorDistance function to find the similar topics.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764fdb44-dec4-4d2c-9479-d90bee60adb4",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>11. Cleanup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c86fee-041e-412b-b6bf-7773aa8bc55f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'> <b>Work Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45aaabf-36e2-4301-94d2-06bbda56513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['topics_embeddings_store','semantic_search_results']\n",
    "\n",
    "# Loop through the list of tables and execute the drop table command for each table\n",
    "for table in tables:\n",
    "    try:\n",
    "        db_drop_table(table_name=table)\n",
    "    except:\n",
    "        pass  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72802d0-aeee-442e-a920-9d16d9663188",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'> <b>Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac5a13-3658-4932-9491-d93a226a2ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_ComplaintAnalysis');\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fece83-0aac-4e95-99ba-f73aa81e6588",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b33117-fa3a-45ac-bd67-3f6aa76be063",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid #91A0Ab\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2024. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
