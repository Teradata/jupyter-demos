{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bc532d-c78c-4c89-b191-242da0733f39",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background- padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "      Generative Question Answering using Generative AI with Vantage\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff71661-19b4-423a-867a-7c815b064c81",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Introduction:</b></p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>In our Question-Answering system using Generative AI demo, we leverage the combination of <b>RAG, Langchain, and LLM models</b> to enable users to ask queries in layman's terms, retrieve relevant information from the <b>Vantage</b> tables, and generate accurate and concise answers based on the retrieved data. This integration of retrieval-based and generative-based approaches provides a powerful tool for extracting knowledge from structured sources and delivering user-friendly responses.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>In this demo, we will build Generative Question-Answering using LangChain, a powerful library for working with LLMs like GPT-3.5, GPT-4, Google's Gemini, Claude 3.5 Sonnet, etc. and JumpStart in ClearScape notebooks. We are building a system where users can ask business questions in natural English and receive answers with data drawn from the relevant databases.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>The following diagram illustrates the architecture.</p>\n",
    "\n",
    "<center><img src=\"images/vantage_qa_gen.png\" alt=\"Generative_QA_architecture\"  width=800 height=800/></center>\n",
    "\n",
    "<br>\n",
    "<p style='font-size:16px;font-family:Arial'>Before going any farther, let's get a better understanding of RAG, LangChain, and LLM.</p>\n",
    "\n",
    "<ul style='font-size:16px;font-family:Arial'><li> <b>Retrieval-Augmented Generation (RAG):</b></li></ul>\n",
    "<p style='font-size:16px;font-family:Arial'> &emsp;  &emsp;We use RAG, a framework that combines the strengths of retrieval-based and generative-based approaches in question-answering systems. It utilizes both a retrieval model and a generative model to generate high-quality answers to user queries. The retrieval model is responsible for retrieving relevant information from a knowledge source, such as a database or documents. The generative model then takes the retrieved information as input and generates concise and accurate answers in natural language.</p>\n",
    "\n",
    "<ul style='font-size:16px;font-family:Arial'><li> <b>Langchain:</b></li></ul>\n",
    "<p style='font-size:16px;font-family:Arial'> &emsp;  &emsp;We leverage LangChain, a framework that facilitates the integration and chaining of large language models with other tools and sources to build more sophisticated AI applications. LangChain does not serve its own LLMs; instead, it provides a standard way of communicating with a variety of LLMs, including those from OpenAI and HuggingFace. LangChain accelerates the development of AI applications with building blocks. We learn to leverage the following building blocks in this notebook:</p>\n",
    "\n",
    "<ol style='font-size:16px;font-family:Arial'>\n",
    "    <li> <b> LLMs</b> – LangChain's <code>llm</code> class is designed to provide a standard interface for all LLM it supports.   </li>\n",
    "    <li> <b> PromptTemplate</b>  - LangChain’s <code>PromptTemplate</code> class are predefined structures for generating prompts for LLM’s. They can be reused across different LLM's.</li>\n",
    "    <li> <b> Chains</b> – When we build complex AI applications, we may need to combine multiple calls to LLM’s and to other components. LangChain’s <code>chain</code> class allows us to link calls to LLM’s and components. The most common type of chaining in any LLM application is combining a prompt template with an LLM and optionally an output parser. </li>\n",
    "</ol>\n",
    "\n",
    "<ul style='font-size:16px;font-family:Arial'><li> <b>LLM Models (Large Language Models):</b></li></ul>\n",
    "<p style='font-size:16px;font-family:Arial'> &emsp;  &emsp;We work with LLM models, which refer to the large-scale language models that are trained on vast amounts of text data. These models, such as GPT-3.5, GPT-4, HuggingFace BLOOM, LlaMA 3, Google's Gemini, etc. are capable of generating human-like text responses. LLM models have been pre-trained on diverse sources of text data, enabling them to learn patterns, grammar, and context from a wide range of topics. They can be fine-tuned for specific tasks, such as question-answering, natural language understanding, and text generation. LLM models have achieved impressive results in various natural language processing tasks and are widely used in AI applications for generating human-like text responses.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca49b9d-df0b-400a-acf8-0252ab8a2618",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><b>Steps in the analysis:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Configuring the environment</li>\n",
    "    <li>Connect to Vantage</li>\n",
    "    <li>Data Exploration</li>\n",
    "    <li>LLM</li>\n",
    "    <li>Run the query function</li>\n",
    "    <li>Cleanup</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833b5bf-74af-42be-8543-3782e1da95dc",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:28px;font-family:Arial'>1. Configuring the environment</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6027a7-888d-441f-abc7-a6ea1c45f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# '%%capture' suppresses the display of installation steps of the following packages\n",
    "\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b160ce-5ace-4116-86b6-394d6502553b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><b>Note: </b><i>The above statements will install the required libraries to run this demo. Be sure to restart the kernel after executing the above lines to bring the installed libraries into memory. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61067c88-2e9a-4c92-985b-34dc4ab74a13",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.1 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50a4aa-1211-44fc-8166-317c35253207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# teradata lib\n",
    "from teradataml import *\n",
    "\n",
    "# LLM\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from langchain import PromptTemplate, SQLDatabase, LLMChain\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "display.max_rows = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59718f8-7af4-4d1a-abc7-a860eb7cbae3",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:28px;font-family:Arial'>2. Connect to Vantage and OpenAI</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e4d23-3f17-4d43-b9b9-7e125f59b8c7",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>2.1 Connect to Vantage</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ed6c7-1a82-4d5d-9a04-bf01debfb1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4ec9a-6d50-47e4-ac3a-da36e85b9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql('''SET query_band='DEMO= Generative_Question_Answering_Python.ipynb;' UPDATE FOR SESSION;''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22db506-84a7-406b-be9f-9fe69d268ba4",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Begin running steps with Shift + Enter keys. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d7aa6-3c54-4e8b-92ee-3b8147a60613",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>2.2 Get the OpenAI API key</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30cd503-2ef8-478f-936a-61434947635e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>To utilize this demo, we need an OpenAI API key. If we don't have one yet, we can refer to the instructions provided in this guide to obtain our OpenAI API key. </p>\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"..//Openai_setup_api_key/Openai_setup_api_key.md\" style=\"text-decoration:none;\" target=\"_blank\"><button style=\"font-size:16px;font-family:Arial;color:#fff;border:none;border-radius:5px;cursor:pointer;height:50px;line-height:50px;display:flex;align-items:center;\">OpenAI API Key Guide <span style=\"margin-left:10px;\">&#8658;</span></button>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd64269-9393-434a-ac26-0b8386fc0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "# enter your openai api key\n",
    "api_key = getpass.getpass(prompt=\"\\n Please Enter OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beec557-411b-4418-acf3-2f28d3c6beac",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>2.3 Getting Data for This Demo</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have provided data for this demo on cloud storage. We can either run the demo using foreign tables to access the data without any storage on your environment or download the data to local storage, which may yield faster execution. Still, there could be considerations of available storage. Two statements are in the following cell, and one is commented out. We may switch which mode to choose by changing the comment string.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26ca7c-fa9e-4200-8e9e-27f5886aa5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call get_data('DEMO_MarketingCamp_local');\"        # Takes 20 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a16b5-da34-4326-b66b-5f7d5a9d9b04",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Next is an optional step – if we want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79252a5e-b2c1-404a-ae22-6c3d698510c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\"        # Takes 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f3fe2-ed63-4838-bb19-4c0d0157453d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:28px;font-family:Arial'>3. Data Exploration</b>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Our goal in predicting Marketing Campaign Effectiveness is to reduce marketing resources by identifying customers who would purchase the product and thereby directing our marketing efforts to them.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>We are working with data from the last marketing campaign, which includes thousands of rows of customer data such as age, job, marital status, education, and more.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Each row in the dataset represents a snapshot of data taken during the last marketing campaign, and each column represents a different variable. We can categorize the input dataset into three main categories:</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>\n",
    "    <ol style='font-size:16px;font-family:Arial'>\n",
    "        <li>Customer data, including age, profession, education, monthly income, and more.</li>\n",
    "        <li>Attributes related to the last contact of the current campaign, such as contact, month, day, and so on.</li>\n",
    "        <li>Other attributes, including campaign, previous outcome, payment methods, and more.</li>\n",
    "        <li>The target attribute - whether the customer purchased the product.</li>\n",
    "    </ol>\n",
    "</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>We have loaded the source data from <a href=\"https://www.kaggle.com/datasets/janiobachmann/bank-marketing-dataset\">Kaggle</a> into Vantage and supplemented it with additional information such as city, monthly income, family members, and more. The data is stored in a Vantage table named <i>Retail_Marketing</i>.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'><b><i><i>*Please click <a href=\"#section62\">here</a>  scroll down to the end of the notebook for detailed column descriptions of the dataset.</i></b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda8e565-b58b-4cc6-8d9c-50790ca3dcab",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>3.1 Examine the Retail Marketing Campaign table</b></p>    \n",
    "<p style = 'font-size:16px;font-family:Arial'>Let's look at the sample data in the Retail_Marketing table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9951cf-e2b0-40d7-9a8f-a01c7f48ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = DataFrame(in_schema(\"DEMO_MarketingCamp\", \"Retail_Marketing\"))\n",
    "print(\"Data information: \\n\", tdf.shape)\n",
    "tdf.sort(\"customer_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6d954-b72f-41c3-9f8c-bc15cf74d672",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>There are 11K records in all, and there are 23 variables. Purchased is the target variable. We shall classify the purchased variable in accordance with the remaining features.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89367d27-22ca-4a0e-b299-0f8e1a669157",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:28px;font-family:Arial'>4. LLM </b>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.1 Connect to databases using SQL Alchemy</b></p>    \n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Under the hood, we use SQLAlchemy to connect to SQL databases. This means that the SQLDatabaseChain can be used with any SQL dialect supported by SQLAlchemy, such as Teradata Vantage, MS SQL, MySQL, MariaDB, PostgreSQL, Oracle SQL, and SQLite. For more information about the requirements for connecting to our database, we recommend referring to the <a href=\"https://docs.sqlalchemy.org/en/20/\">SQLAlchemy documentation</a>.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Important: The code below establishes a database connection for our data sources and Large Language Models. Please note that the solution will only work if we define the database connection for our sources in the cell below.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>We build a consolidated view of the Table Data Catalog by combining metadata stored for the database and table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b5bb7-d040-47ae-ad40-a5daea9d8167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create the vantage SQLAlchemy engine\n",
    "database = \"DEMO_MarketingCamp_db\"\n",
    "db = SQLDatabase(\n",
    "    eng,\n",
    "    schema=database,\n",
    "    include_tables=[\"Retail_Marketing\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11464fd9-fc1f-4b30-9491-3a853b05c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_schema():\n",
    "    table_dicts = []\n",
    "    database_schema_dict = {\n",
    "        \"database_name\": database,\n",
    "        \"table_name\": \"Retail_Marketing\",\n",
    "        \"column_names\": tdf.columns,\n",
    "    }\n",
    "    table_dicts.append(database_schema_dict)\n",
    "\n",
    "    database_schema_string = \"\\n\".join(\n",
    "        [\n",
    "            f\"Database: {table['database_name']}\\nTable: {table['table_name']}\\nColumns: {', '.join(table['column_names'])}\"\n",
    "            for table in table_dicts\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return database_schema_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17df4fe-a295-4e92-82c8-04bf81f22abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_schema = get_db_schema()\n",
    "print(database_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781b83c-0907-4bbd-8fe1-1067e587512b",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b> 4.2 Format the answer and Display</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>To view the answer in proper format with markdown</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c27d29-1bde-4908-b7f1-309dcfd89eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def response_template(response):\n",
    "    if \"output\" in response:\n",
    "        return f\"<p style = 'font-size:16px;font-family:Arial'>Answer:  <b>{response['output']}<b>\"\n",
    "    else:\n",
    "        return f\"<p style = 'font-size:16px;font-family:Arial'>Answer:  <b>{response}<b>\"\n",
    "\n",
    "\n",
    "def error_template():\n",
    "    return f\"<p style = 'font-size:16px;font-family:Arial'>Sorry, there was an error while generating the SQL query. The GenAI may have made a mistake in the syntax of the query.  <br>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680228a2-3bfa-44f4-8a02-b73fa8bf8e95",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.3 Define LLM model</b></p>  \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In OpenAI's language models, we understand that the <b>Temperature</b> parameter affects the determinism of the results. The lower the temperature, the more deterministic the results, meaning the highest probable next token is always picked. Increasing the temperature could lead to more randomness, which encourages more diverse or creative outputs. We are essentially increasing the weights of the other possible tokens. In terms of application, we might want to use a lower temperature value for tasks like fact-based QA to encourage more factual and concise responses. For poem generation or other creative tasks, it might be beneficial to increase the temperature value.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45829875-a8a9-465a-ba10-d5a3a3393d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "# set LLM model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5753a719-44da-460e-91c4-a17b665b25b2",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.4 Setup SQLAgent</b></p> \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>An agent is a sophisticated component that leverages a suite of tools, including a Large Language Model (LLM), to make informed decisions based on user input. This advanced functionality allows agents to utilize the appropriate tools until they achieve a satisfactory answer. For instance, in the context of text-to-SQL, the LangChain SQLAgent exhibits resilience by recovering from errors in executing generated SQL queries. Instead of giving up, it interprets the error in a subsequent LLM call and rectifies the issue. This robustness theoretically makes SQLAgent more productive and accurate compared to a simple SQLChain.</p>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial'>WWe can think of agents as enabling tools for LLMs, much like how humans use calculators for math or perform Google searches for information. Agents empower LLMs to perform tasks more efficiently and effectively.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe3f75-c7ff-4142-9848-7aa89530a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main prompt\n",
    "generated_prompt = f\"\"\"You are a Teradata Database expert and you are tasked with generating SQL queries for Teradata based on user questions. \n",
    "    Your response should ONLY be based on the given context and follow the response guidelines and format instructions.\n",
    "\n",
    "        Utilize the following tables and columns exclusively when creating SQL queries:\\n{database_schema}\n",
    "\n",
    "        Here are some tips for writing Teradata style queries: \n",
    "        * Always use table aliases when your SQL statement involves more than one source\n",
    "        * Aggregated fields like COUNT(*) must be appropriately named\n",
    "        * Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 3 results by using SELECT TOP 3, note that LIMIT function does not works in Teradata DB.\n",
    "        * Remove unnecessary ORDER BY clauses unless required. \n",
    "        * Remember: Do not use 'LIMIT' or 'FETCH' keyword in the SQLQuery, instead of TOP keyword, For Example: To select top 3 results, use TOP keyword instead of LIMIT or FETCH. \n",
    "        * Important: If you received the error \"Bad character in format or data\", change the value of columns, get the values from table only.\n",
    "        * Remember: purchased column have only 2 values: 'yes' or 'no'\n",
    "        * Critical Instruction: Use default database as 'DEMO_MarketingCamp_db'\n",
    "        \n",
    "        Few examples of SQL:\n",
    "        Example1: SELECT count(*) as total_count FROM DEMO_MarketingCamp_db.Retail_Marketing\n",
    "        Example2: SELECT TOP 1 city, AVG(monthly_income_in_thousand) AS avg_income FROM DEMO_MarketingCamp.Retail_Marketing\n",
    "                WHERE monthly_income_in_thousand IS NOT NULL GROUP BY city ORDER BY avg_income DESC;\n",
    "\n",
    "        Response Guidelines: \n",
    "        * Whenever possible, give the answer in bulleted points and proper markup.\n",
    "        * Critical Instruction: Ensure responses are exclusively derived from query results. Refrain from generating or adding synthetic data in any form.\n",
    "        * Most important: Always create a syntactically correct Teradata-style query that addresses the question, \n",
    "        even if it has been asked and answered previously. Ensure the query is generated from scratch and does not rely on any pre-existing data stored in memory.\n",
    "      \n",
    "\n",
    "        Given a user's question about this data, write a valid Teradata SQL query that accurately extracts or calculates the requested information from these tables and adheres to SQL best practices for Teradata database, optimizing for readability and performance where applicable.\n",
    "        Most important: Execute the SQL and return the final answer only in simple english statement. \n",
    "        Critical Instruction: Do not return json or SQL.\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    AIMessage(content=generated_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "\n",
    "agent_executor = create_sql_agent(\n",
    "    llm,\n",
    "    db=db,\n",
    "    agent_type=\"openai-tools\",\n",
    "    verbose=True,\n",
    "    prompt=prompt,\n",
    "    max_iterations=10,\n",
    "    max_execution_time=20,\n",
    "    handle_parsing_errors=True,\n",
    "    return_intermediate_steps=True,\n",
    "    handle_sql_errors=True,\n",
    "    max_tokens=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fe1c6a-191a-4dc7-9b3f-7c282ec94541",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:28px;font-family:Arial'>5. Execute the user queries on SQLAgent</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Run the run_query function that in turn calls the Langchain SQL Database chain to convert 'text to sql' and runs the query against the source database</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f938e3-acab-4799-bb0e-b550fa035bc3",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.1 Query 1</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>For example, for the user query <b>How many married customers have purchased the product?</b> the answer is as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a21be-1038-4796-b2b7-b50579d39b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Enter the query\n",
    "    query = \"\"\"How many married customers have purchased the product?\"\"\"\n",
    "\n",
    "    # Response from Langchain\n",
    "    response = agent_executor.invoke(query)\n",
    "\n",
    "    display(Markdown(response_template(response)))\n",
    "except:\n",
    "    display(Markdown(error_template()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1659e-e1e0-49bc-91d7-66f1904764c6",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.2 Query 2</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>For example, another user query <b>What is the number of purchases made by customers who are in management professions?</b> the answer is as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9098a-921b-4363-92f4-3c49c4d48283",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Enter the query\n",
    "    query = \"\"\"What is the number of purchases made by customers who are in management professions?\"\"\"\n",
    "\n",
    "    # Response from Langchain\n",
    "    response = agent_executor.invoke(query)\n",
    "\n",
    "    display(Markdown(response_template(response)))\n",
    "except:\n",
    "    display(Markdown(error_template()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc58553-78d9-4b94-8409-b1e66aff9518",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.3 Query 3</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>For example, for little bit complex user query <b>Which are the most common purchasing behaviors of customers?</b> the answer is as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c57a87-ab22-4655-91c9-61fe36502759",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Enter the query\n",
    "    query = \"\"\"Which are the most common purchasing behaviors of customers?\"\"\"\n",
    "\n",
    "    # Response from Langchain\n",
    "    response = agent_executor.invoke(query)\n",
    "\n",
    "    display(Markdown(response_template(response)))\n",
    "except:\n",
    "    display(Markdown(error_template()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc93f6-fd54-4dea-bbcd-f367c8237923",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.4 We encourage you to try formulating your own question.</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here are some sample questions that you can try out:</p>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>What is the average income for Phoenix?</li>\n",
    "    <li>Which city has the highest average income?</li>\n",
    "    <li>What is the average age of married people?</li>\n",
    "    <li>Which profession has the most married people in Phoenix?</li>\n",
    "    <li>What is the month with the lowest sales?</li>\n",
    "    <li>What is the month with the highest number of marketing engagements?</li>\n",
    "    <li>What is the payment method distribution?</li>\n",
    "    <li>What is the average number of days between a customer's last contact and their next purchase?</li>\n",
    "    <li>What is the relationship between marital status and purchase frequency?</li>\n",
    "    <li>What is the most effective communication method for reaching customers who have not purchased from our company in the past 6 months?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c0567-f2e0-45c7-94f1-2324ccc1fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    query = input(prompt=\"\\n We invite you to enter your natural language query: \")\n",
    "    # Response from Langchain\n",
    "    response = agent_executor.invoke(query)\n",
    "\n",
    "    display(Markdown(response_template(response)))\n",
    "except:\n",
    "    display(Markdown(error_template()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8b817-c0ff-42b3-a651-c8268ac40942",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:28px;font-family:Arial'>6. Cleanup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aacb87c-0d81-40f0-8754-608d47e602cc",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'> <b>6.1 Databases and Tables </b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following code will clean up tables and databases created above.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b398148-9486-4535-8c06-d3f77669bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_MarketingCamp');\"        # Takes 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef4cbe-1beb-4c65-ac60-ffd6250668a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae971f90-bef1-4228-bcbf-c333e9843284",
   "metadata": {},
   "source": [
    "<a id=\"section62\"></a>\n",
    "<b style = 'font-size:28px;font-family:Arial'>Dataset:</b>\n",
    "\n",
    "- `customer_id`: Unique row customer id\n",
    "- `age`: customer age (numeric)\n",
    "- `profession` : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\")\n",
    "- `marital` : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" meansdivorced or widowed)\n",
    "- `education` customer eduction (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\n",
    "- `city`: city of customer (categorical: 'New York','Los Angeles','Chicago','Houston','Phoenix','Philadelphia','San Antonio','San Diego','Dallas','San Jose')\n",
    "- `monthly_income_in_thousand`: customer's monthly income, in dollar (numeric)\n",
    "- `family_members`: number of family members (numeric)\n",
    "- `communication_type`: communication type (categorical: \"unknown\",\"telephone\",\"cellular\")\n",
    "- `last_contact_day`: last contact day of the month (numeric)\n",
    "- `last_contact_month`: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n",
    "- `credit_card`: does customer have a credit card? (binary: 'yes','no')\n",
    "- `num_of_cars`: number of cars (numeric)\n",
    "- `last_contact_duration`: last contact duration, in seconds (numeric)\n",
    "- `campaign`: number of contacts performed during this campaign and for this client (categorical,includes last contact)\n",
    "- `days_from_last_contact`: number of days that passed by after the client was last contacted from a previouscampaign (numeric, -1 means client was not previously contacted)\n",
    "- `prev_contacts_performed`: number of contacts performed before this campaign and for this client (numeric)\n",
    "- `prev_campaign_outcome`: outcome of the previous marketing campaign (categorical:\"unknown\",\"other\",\"failure\",\"success\")\n",
    "- `payment_method`: payment method use by customer (categorical: 'cash','credit_card','debit_card','ewallets', 'payment_links', 'QRcodes')\n",
    "- `purchase_frequency`: how frequently customer is purchasing (categorical: 'daily','weekly','biweekly','monthly','quarterly','yearly')\n",
    "- `gender`: gender of customer? (binary: 'male','female')\n",
    "- `recency`: number of days since the last purchase (numeric)\n",
    "\n",
    "\n",
    "Output variable (desired target):\n",
    "- `purchased`: does customer did a purchase - target column (binary: 'yes','no')\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Dataset source:</b> <a href = 'https://www.kaggle.com/datasets/janiobachmann/bank-marketing-dataset'>kaggle</a></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Links:</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Teradataml Python reference: <a href = 'https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/Teradata-Package-for-Python-User-Guide-17.20'>here</a></li>\n",
    "    <li>Langchain Python reference: <a href='https://python.langchain.com/docs/get_started/introduction/'>here</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5463848-592f-4321-b852-287e133872dd",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; background:#91A0AB; \">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2023, 2024. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
