{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0020600f-221f-4780-ae47-d07121de1bcd",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Retail Product Hierarchy Clustering with In-Database K-means Clustering\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>Introduction</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The retail industry faces challenges with their product hierarchies. Current hierarchies typically group products in a way that makes sense for procurement but often do not reflect how customers shop.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To align product hierarchies with customer shopping behavior, products should be grouped based on similar sales and demand patterns, indicating a strong correlation in their sales time series. However, the existing hierarchies are based on business rules rather than time series analysis. For example, all types of bread are placed in the same subgroup. While this approach works well for some product families—perhaps up to 80%—the remaining 20% are less straightforward and require further investigation. This can be achieved by applying AI/ML models to uncover the diverse dynamics within a subgroup  </p> \n",
    "\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>Business Value </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Aligning product hierarchies with customer shopping behavior using AI/ML models can offer several business benefits:</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial'><b>Improved Inventory Management:</b> Understanding the true demand patterns of products allows for better inventory forecasting and management. This can reduce stockouts and overstock situations, leading to cost savings and more efficient operations.</li>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial'><b>Optimized Product Placement:</b> In physical stores, aligning product hierarchies with customer behavior can improve product placement strategies. Products that are frequently bought together can be placed near each other, enhancing the shopping experience and potentially increasing sales.</li> \n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial'><b>Relevant Recommendations:</b> Improved product hierarchies can lead to better product recommendations. When customers see suggestions that are relevant to their interests and needs, they feel understood and valued.</li>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial'><b>Efficient Shopping:</b> A well-organized store layout, whether online or in physical stores, saves customers time. When they can quickly find related products, it makes their shopping trip more efficient and pleasant.</li>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial'><b>Enhanced Loyalty:</b> Satisfied customers are more likely to return. By consistently meeting their needs and expectations, retailers can build stronger customer loyalty.</li>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Business value is achieved through grouping products with similar sales/demand behaviors and deriving the correlation between the time series of the sales unit by product. \n",
    "</p>\n",
    "\n",
    "  \n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>Why Vantage? </b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To build more effective ML and AI models, developers and data scientists must explore unconventional data sources, tools, and techniques to continuously improve their models’ accuracy, speed, and efficacy. However, this creativity often comes at a cost. Additionally, integrating diverse analytics and data into the development pipeline typically increases complexity, fragility, and operational challenges</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Teradata Vantage provides ClearScape Analytics functions which allow users to seamlessly combine a wide range of behavioral, text processing, statistical analysis, and advanced analytic functions with model training and deployment tools on the same platform.</p> \n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>This allows for rapid development, testing, and validation of new techniques at scale in near-real time so new, more accurate models can easily be deployed to production.</p>\n",
    "\n",
    "<p></p>    \n",
    " \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The demonstration  aims to show how to enhance product hierarchies in the retail industry by leveraging time series analysis and clustering techniques. The objective is to identify clusters of products exhibiting similar sales/demand behaviors across the product hierarchy, often defined according to product descriptions or business rules. The first part focuses on data exploration and visualization to understand product relationships and assess clustering strategies. The second part involves clustering by correlation, using a coclustering algorithm and in-database correlation matrix computations to group products with similar sales patterns. The third part covers specific feature engineering, utilizing functions like TD_NORMALIZE_OVERLAP and applying K-means clustering to refine product groupings based on engineered features.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce0031d-dec1-4adb-be2d-4023572cbfba",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>1. Connect to Vantage.</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25dfae-906f-4989-bb9c-1378c867b073",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We start by importing the required libraries and set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba131c-fc5f-4a20-8e11-2eb5d8efea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install --upgrade tdnpathviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47892e7b-2ff8-4db3-89d0-ab689d2d8a20",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Note: </b><i>The above library needs to be upgraded for some of the functions used in this demonstration. Please be sure to restart the kernel after installing/upgrading the library. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60e9f9-ede2-4c45-99fd-2682207e0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from teradataml import (\n",
    "    create_context,\n",
    "    execute_sql,\n",
    "    DataFrame,\n",
    "    in_schema,\n",
    "    configure,\n",
    "    remove_context)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import getpass\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "display.max_rows=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee565846-f424-4304-b866-fa119ba79d31",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a136ba5-4ef0-4433-aeaf-aab2e0429cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e540d63-c83f-46b7-b0ca-3c256825c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql('''SET query_band='DEMO=PP_Retail_Product_Hierarchy_Clustering_Python.ipynb;' UPDATE FOR SESSION; ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76140ae-18ce-44f6-896c-ccbadc4e4e9b",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>2. Getting Data for This Demo </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We have provided data for this demo on cloud storage. We have the option of either running the demo using foreign tables to access the data without using any storage on our environment or downloading the data to local storage, which may yield somewhat faster execution. However, we need to consider available storage. There are two statements in the following cell, and one is commented out. We may switch which mode we choose by changing the comment string.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4df0bc-0224-4c5b-bb5e-3e67005dc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ../run_procedure.py \"call get_data('DEMO_ProductHierarchy_cloud');\"\n",
    " # Takes about 30 seconds\n",
    "%run -i ../run_procedure.py \"call get_data('DEMO_ProductHierarchy_local');\"\n",
    " # Takes about 70 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc2d1d-9988-43a2-b5fd-8a2db8852684",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Optional step – We should execute the below step only if we want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762bfd0-55ac-44b0-b0a6-462eda3199ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6120d8-9523-4631-8104-1b21f84dd529",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>3. Analyze Raw Data.</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Let us start by creating a \"Virtual DataFrame\" that points directly to the dataset in Vantage.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c32ab3-fb28-4ba8-9b7a-06404e175ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataFrame(in_schema(\"DEMO_ProductHierarchy\",\"Retail_Product_Hierarchy\"))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec336f-b9cc-495f-9452-b6b69acf02f0",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The table retail product hierarchy has division as it's main hierarchy. Than we have department, sections, product code and than there is a subgroup code. The Product ID can be an id for any product like tea, beans, salt, paper etc. These products are sold by many stores on same dates or different dates. It also stores the no of units sold by different stores on a particular date. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b3c57-affb-4c89-8ff5-f4b1affad1cc",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>3.1 - Explore Product Hierarchy (Plotly icicle)</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa86ae-2ce0-4571-b1f7-df583d99163c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will try to visualize this product hierarchy to get a better idea of the hierarchy.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here we will import the `ProductHierarchySelector` class from the `tdnpathviz` package, which is used for hierarchical data exploration and define the list of columns that represent the product hierarchy in the dataset. We will than initialize the `ProductHierarchySelector` with the dataset and the defined product hierarchy.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda48b8-af72-43f5-8931-f7d69e12cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnpathviz.hierarchy_exploration import ProductHierarchySelector\n",
    "product_hierarchy = [\n",
    "    'Division_Code',\n",
    "    'Department',\n",
    "    'Section',\n",
    "    'Product_Code',\n",
    "    'Subgroup_Code'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de760a6d-7fe2-4b93-888e-23d923f44f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ProductHierarchySelector(dataset, product_hierarchy)\n",
    "selector.visualize_only()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b04975-3575-4e1f-893c-3e58b023b9ad",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We see that the values in the first row of records for the first 3 columns in the hierarchy are GQP, GQP_DDP and GQP_DDP_WDH (Division_Code, Department, Section).  The first bar does not display a color because it doesn't have a parent.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We can interactively select specific parts of the product hierarchy. Once you select any blue box in the above figure it will show you the layers in the product hierarchy. You can select any box based on which the hierarchy will be selected.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f739b2-5006-4a54-b274-bca06564e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.select()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f09713-4a01-414d-bede-14863f825c2a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will than retrieve the filtered dataset and plot the sales units.</p> \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will filter the data based on the level of hierarchy selected and than use that data in further analysis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54affbb1-7f8c-45f1-9134-6c625801d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered = selector.get_filtered_dataset()\n",
    "print(dataset_filtered.shape)\n",
    "dataset_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799a568-ea20-4891-ae4f-f5f7a86610d7",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>3.2 In-database plotting</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e7dde-1bd7-4a7d-b5a6-21323216f5b9",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will visualize the data for each product by CALENDAR_DATE  and calculated the sum of ACTUAL_SALES_SINGLES for each group. The objective is to prepare a time series dataset for further analysis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0352d4-40e0-4ca1-8841-d9570556b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = dataset_filtered[['Date_Calendar','Base_Product_ID','Sales_Units']]\\\n",
    "                .groupby(['Date_Calendar','Base_Product_ID']).sum()\n",
    "time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b93057-24b2-4bb3-9d35-ad78fb934dee",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Below plot shows the sales for all products selected in the hierarchy selected.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1db35-dc27-4bb3-95a5-d2a907753896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnpathviz.visualizations import plotcurves\n",
    "plotcurves(\n",
    "    time_series, \n",
    "    field='sum_Sales_Units', \n",
    "    row_axis = 'Date_Calendar', \n",
    "    series_id = 'Base_Product_ID', \n",
    "    row_axis_type = 'TIMECODE',\n",
    "    legend='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc598cb7-3546-4bef-96b3-53c237ff72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.to_sql(table_name = 'time_series',if_exists = 'replace', primary_index = 'Date_Calendar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c9608-bbaf-4ef9-bf93-3d15a9572688",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>4. Compute Correlation between Time Series</b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>4.1 Pivoting the time series</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will use the pivot function to check the no of units sold for each product on all the dates. So we keep the Date as is, and use the Product id to get the sum of units sold for  further analysis.</p>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "The time series data is pivoted to create a matrix where each column represents a different product's sales over time.</li> \n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Missing values in the pivoted data are filled with the mean of the respective columns</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Common substrings are removed from feature names to simplify the dataset.</li></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>First, we exclude 0 sales that will artificially increase the correlation</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221d7d6-c857-49ea-a59e-014c4cf92c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series0 = DataFrame('time_series')\n",
    "time_series = time_series0[time_series0.sum_Sales_Units > 0].dropna() \n",
    "time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bbe97b-6733-45fc-9c62-f1c925165c0b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will use pivot on <code>time_series</code> with <code>BASE_PRODUCT_NUMBER</code> as columns and summing <code>sum_ACTUAL_SALES_SINGLES</code>. Missing values in the pivoted DataFrame are filled with the mean of their respective columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b1cbb-c099-4221-8bd4-f43b45b06e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_pivoted = time_series.pivot(\n",
    "    columns=time_series.Base_Product_ID,\n",
    "    aggfuncs=time_series.sum_Sales_Units.sum()\n",
    ")\n",
    "\n",
    "time_series_pivoted = time_series_pivoted.fillna(\n",
    "    'MEAN', \n",
    "    time_series_pivoted.columns[1::]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a2bbf-9f3f-4803-b588-dc7c84c4843e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will than clean and rename the features in the pivoted DataFrame:</p>\n",
    "\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'><code>new_features</code> is created by removing common substrings from the feature names.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'><code>new_vars</code> is an OrderedDict that starts with the <code>CALENDAR_DATE</code> column.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>A loop iterates over <code>new_features</code> and the original columns to populate <code>new_vars</code> with the cleaned feature names.\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'><code>time_series_pivoted</code> is updated with the new feature names and the original columns are dropped.</li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20e02d-85a8-49da-8ddb-bd871094d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnpathviz.utils import remove_common_substring_from_features\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Remove common substring from feature names\n",
    "new_features = remove_common_substring_from_features(\n",
    "    features=time_series_pivoted.columns[1:]\n",
    ")\n",
    "\n",
    "# Create an ordered dictionary with the new features\n",
    "new_vars = OrderedDict({'Date_Calendar': time_series_pivoted.Date_Calendar})\n",
    "\n",
    "# Assign new feature names to corresponding old features\n",
    "for new_f, old_f in zip(new_features, time_series_pivoted.columns[1:]):\n",
    "    new_vars[new_f] = time_series_pivoted[old_f]\n",
    "\n",
    "# Assign the new variables to the DataFrame, dropping old columns\n",
    "time_series_pivoted = time_series_pivoted.assign(\n",
    "    drop_columns=True, **new_vars\n",
    ")\n",
    "\n",
    "# Return or print the updated DataFrame\n",
    "time_series_pivoted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3678b-8dff-4ee3-af80-f6e0a7c9d1d5",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>4.2 - Compute the correlation matrix </b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870e03f-996d-4235-acdf-77e71b3ef5c9",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In this analysis, we will utilize a previously created time series table to explore correlations within the data. Our goal is to identify and group products into clusters based on their correlation, potentially forming subclusters of highly correlated products.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>By using correlation as a clustering criterion, we aim to classify products into different sublevels within the product hierarchy. This classification will be based on sales patterns rather than traditional business descriptions, such as grouping similar items like beans together.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Additionally, this method offers an opportunity to evaluate the quality of the current product hierarchy. We will assess whether the tightest subgroups within the hierarchy make sense from a sales pattern perspective, providing insights into customer behavior and product relationships.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will compute the correlation matrix for the features in the pivoted DataFrame. Correlation matrix is created on the selected features and is displayed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc15c63-fd91-4f07-878c-b1964a70a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnpathviz.visualizations import (\n",
    "    plot_correlation_heatmap,\n",
    "    compute_correlation_matrix, \n",
    "    reorder_correlation_matrix)\n",
    "\n",
    "configure.val_install_location = \"VAL\"\n",
    "\n",
    "features = time_series_pivoted.columns[1::]\n",
    "features[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9b0fd-7cae-4a18-bbb4-8f24b85260d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = compute_correlation_matrix(time_series_pivoted[features])\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a56402e-056d-43cc-aacf-704731b0a5ba",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>4.3 - Visualize the correlation matrix</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c2f50-e97f-41e3-a6e3-4773a6a4c68c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><code>plot_correlation_heatmap</code> is called with the correlation matrix corr to generate and display the heatmap.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e66e1-21bc-4a48-b50e-21248a96f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(corr_matrix=corr) #, figsize=(40,32), fontsize=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6b3b3-e58c-4b40-8f8a-339a697d9df5",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Using a correlation heatmap, we can see the matrix with colors indicating the strength of the correlation. Dark blue represents anticorrelation, while red indicates a perfect correlation of one. In practice, perfect correlations are rare, but high correlations around 0.7 or 0.68 are quite good.\n",
    "The heatmap reveals that the correlations are scattered without a clear pattern. Zooming in on a smaller section of the matrix can help us better understand these relationships.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will zoom the correlation heatmap on a subset of the correlation matrix (first 10 rows and columns) to generate and display a zoomed-in heatmap.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7624f-a4d8-472d-bec3-cff27312b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom-in\n",
    "plot_correlation_heatmap(corr_matrix=corr.iloc[0:10,0:10]) #, figsize=(10,8), fontsize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50376c1d-d1bf-4fab-a488-d86f95ef169a",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>4.4 - Clustering by correlation (co-clustering)</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b423a5-7e35-44d0-ab42-c4feb07f84b9",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To identify clusters within our data, we can reorder the correlation matrix to place highly correlated products next to each other. This technique, known as co-clustering, helps reveal clusters within the data. By reordering the correlation matrix, we can test multiple methods to identify clusters. For instance, if we aim to create 10 clusters, we can run the clustering algorithm to classify products based on their correlations. The resulting reordered matrix will display blocks, each representing a cluster.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Each block index corresponds to a cluster number. For example, cluster 0 might contain several products, cluster 1 might have only two products, and cluster 2 could include more products. This visualization helps us understand the grouping of products based on their correlation patterns.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will reorder the correlation matrix using co-clustering. <code>ordered_corr</code> and <code>blocks</code> are created by calling <code>reorder_correlation_matrix</code> with the correlation matrix, specifying the co-clustering method and 10 clusters. The resulting blocks are displayed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57fcb9-8fc2-4a2b-bf1a-a3b5eff92bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ordered_corr, blocks = reorder_correlation_matrix(corr,method='coclustering', n_clusters=10)\n",
    "blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f42612-00f1-49dc-8bc3-fbbc1e365f32",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'>We will visualize the correlated matrix</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eafdb3-c97c-464d-81ea-c3caa3ce9eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(corr_matrix=ordered_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40fe72-f669-454b-8fe0-e3afe747612d",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will zoom the correlation matrix by setting the block_index to 2, selecting the indices of the features in the specified block and displaying the heatmap with the subset of the reordered correlation matrix corresponding to the selected block.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cddd7a1-81e5-4640-8887-ce5b936a3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom-in\n",
    "block_index = 2\n",
    "indices = blocks[blocks.block_index == block_index].index.to_list()\n",
    "plot_correlation_heatmap(corr_matrix=ordered_corr.iloc[indices,indices]) #, figsize=(10,8), fontsize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1715e07-600c-4372-a6fd-493ac22b1265",
   "metadata": {},
   "source": [
    "\n",
    "This code cell zooms in on the original correlation matrix for a specific block:\n",
    "- `indices` are created by selecting the feature indices of the specified block.\n",
    "- `plot_correlation_heatmap` is called with the subset of the original correlation matrix corresponding to the selected block to generate and display a zoomed-in heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689255b-984c-4b2b-ac8e-fefcee0996ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom-in\n",
    "indices = blocks[blocks.block_index == block_index].feature_index.to_list()\n",
    "plot_correlation_heatmap(corr_matrix=corr.iloc[indices,indices]) #, figsize=(10,8), fontsize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19edfa-ae19-4156-aec6-d96238e8cabb",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>4.5 - Compute, Cluster and Vizualize all in one</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023dfacc-e86e-4d67-b393-51b6b7430349",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will reorders the correlation matrix and output the results. <code>ordered_corr</code> and <code>blocks</code> are created by calling <code>plot_correlation_heatmap</code> with the pivoted DataFrame, specifying the co-clustering method, 15 clusters, and requesting output results.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bcdf37-972d-4c92-8491-f7f298027997",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_corr, blocks = plot_correlation_heatmap(\n",
    "    time_series_pivoted[features],\n",
    "    method              = 'coclustering',\n",
    "    n_clusters          = 15,\n",
    "    output_results      = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb52eea1-12d1-40c9-826a-a7c975d0a0a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>5. Time Series Clusters</b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>5.1 Cluster with large correlations</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will now check the block index which shows maximum correlation. We will aggregate and sort the blocks by median correlation. The resulting DataFrame is sorted by average_corr in descending order and displayed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455c1264-3e76-4220-945e-56df6c30098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks.groupby('block_index').agg({'average_corr':'median', 'feature' : 'count'})\\\n",
    "        .sort_values('average_corr',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653591f0-c4aa-4959-beb3-39e715cbea9b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here, we can see that the block_index 5 has the maximum average correlation, so for further analysis we will go ahead with this block_index. In case you want to change this values based on the results you have recieved or you wish to analysize any other block_index, it can be changed in the below cell and that will be used further.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82a3f6-f92a-40fa-ab05-b2c128ed0107",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>5.2 - Visualize a cluster </b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07565e2-b321-4798-a66f-343054dcf1fd",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will set the block_index to 5, indicating the cluster to be visualized in subsequent steps. We will filter the blocks DataFrame to select rows where the block_index column matches the specified block_index value. This is useful for isolating a specific subset of data based on the block index. It then extracts the feature column from the filtered DataFrame and converts it to a list, storing it in the selected_products variable. Finally, it displays the selected_products list.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29902982-b0a5-4a7b-900f-f5ed27185ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_index = 5 # the cluster we want to visualize\n",
    "blocks[blocks['block_index'] == block_index]\n",
    "selected_products = blocks[blocks['block_index'] == block_index].feature.to_list()\n",
    "selected_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94def7-6d01-4fcc-b84e-44be867a2e2e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will filter the time_series DataFrame to include only the rows where the BASE_PRODUCT_NUMBER column values are in the selected_products list. This helps in narrowing down the time series data to only the selected products. We will than visualize the filtered data. This will help us understand the sales for these base products on different dates which will be further used in our analysis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691894bf-4d4a-4757-a539-6fa72e27dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnpathviz.visualizations import plotcurves\n",
    "time_series[time_series.Base_Product_ID.isin(selected_products)]\n",
    "plotcurves(\n",
    "    time_series0,\n",
    "    select_id=selected_products, \n",
    "    field='sum_Sales_Units', \n",
    "    row_axis = 'Date_Calendar', \n",
    "    series_id = 'Base_Product_ID', \n",
    "    row_axis_type = 'TIMECODE',\n",
    "    legend='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb45ae-b99e-42c7-8f75-207ec9973958",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>5.3 - Time Series Normalization</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def737b-30a7-4d25-95e7-627a8219686a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will need to normalize the series for 0 sales to make the series more useful for modelling so we will calculate the mean and standard deviation of the actual sales for each product. This provides statistical insights into the sales data for each product. We will than normalize the sum_ACTUAL_SALES_SINGLES field in the time_series0 DataFrame. It joins the mean and standard deviation of sum_ACTUAL_SALES_SINGLES for each BASE_PRODUCT_NUMBER and then standardizes the sales data. The normalized DataFrame is then filtered to retain only the original columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d89d5-9780-41b2-ac6c-cfe4d0002f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping 'sum_Sales_Units' by 'Base_Product_ID' and aggregating mean and std\n",
    "aggregated_sales = time_series[['Base_Product_ID', 'sum_Sales_Units']].groupby(\n",
    "    'Base_Product_ID').agg({'sum_Sales_Units': ['mean', 'std']})\n",
    "\n",
    "# Joining the aggregated statistics back to the original time_series\n",
    "normalized_time_series = time_series0.join(\n",
    "    other=aggregated_sales,\n",
    "    on='Base_Product_ID',\n",
    "    how='inner',\n",
    "    rsuffix='r'\n",
    ")\n",
    "\n",
    "# Calculating normalized sales (z-score-like normalization)\n",
    "normalized_time_series = normalized_time_series.assign(\n",
    "    sum_ACTUAL_SALES_SINGLES=(\n",
    "        normalized_time_series['sum_Sales_Units']\n",
    "        - normalized_time_series['mean_sum_Sales_Units']\n",
    "    ) / (1e-10 + normalized_time_series['std_sum_Sales_Units'])\n",
    ")\n",
    "\n",
    "# Reordering columns to match the original time_series\n",
    "normalized_time_series = normalized_time_series[time_series.columns]\n",
    "\n",
    "# Output the normalized DataFrame\n",
    "normalized_time_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87e392-d11b-4d3a-9540-03aab6eaa165",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will use the plotcurves function to plot the normalized time series data. It visualizes the normalized sum_ACTUAL_SALES_SINGLES field over the CALENDAR_DATE for the selected products (selected_products).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054215b6-271f-45f2-8854-230ce0e0897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcurves(\n",
    "    normalized_time_series,\n",
    "    select_id= selected_products, \n",
    "    field='sum_Sales_Units', \n",
    "    row_axis = 'Date_Calendar', \n",
    "    series_id = 'Base_Product_ID', \n",
    "    row_axis_type = 'TIMECODE',\n",
    "    legend='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3179dd-1c58-4176-b34f-cc752dfae416",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>5.4 - Plot pair plot</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a6e133-b954-41da-b5ef-837cd19a539d",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>After getting the normalized series we will check the correlation by plotting the values for different products using the pair plots. If we have something highly correlated, we should see points aligned along a diagonal. In this matrix plot, we can observe the sales units of one product versus the sales units of another product on the same day within the same group. The other data points show a good correlation, as indicated by the trend.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bf850-21d0-4066-910a-34944e8c5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnpathviz.visualizations import pair_plot\n",
    "pair_plot(time_series_pivoted[selected_products],width = 900, height = 900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c266159-9147-486d-8e15-d8e1b2a49807",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will export the time_series_pivoted DataFrame for the selected_products to a CSV file named examples.csv. This allows for external analysis or sharing of the data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bca434-9646-4563-ad88-952c2dc16588",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_pivoted[selected_products].to_csv('examples.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ec85c-b53d-4a56-a4bf-4a20182dc154",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will also take a look at the results after resampling or smoothing. By looking at the smoothed data, we can gain a better understanding, and it will likely affect the correlation. There are many factors to enhance this correlation analysis.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will redo the analysis with a smoothing window of 3 days. </p>\n",
    "\n",
    "<code># Redo with smoothing on 3 days</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c1c16-b403-4bc9-90d3-9beb422870b3",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will calculate and display the minimum value for each column in the time_series DataFrame. This provides a quick overview of the lowest values in the dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc00927-bbf5-4e69-84ec-61117a3a366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef56a8e-d1c6-413f-ba01-80ee8623743f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will create a TDSeries object from the time_series DataFrame. The TDSeries function creates a series object from a teradataml DataFrame representing a SERIES in time series which is used as input to Unbounded Array Framework. Here, we create the time series by specifying the BASE_PRODUCT_NUMBER as the ID, CALENDAR_DATE as the row index, and sum_ACTUAL_SALES_SINGLES as the payload field.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The Resample() function transforms an irregular time series into a regular time series. It can also be used to alter the sampling interval for a time series. Here, we will resample the time series data to a daily frequency using linear interpolation, starting from a specified timestamp. The resampled DataFrame is adjusted to match the original column names.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c02709-b331-4dfc-81b8-5813f7ebd8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import TDSeries, Resample, Smoothma\n",
    "\n",
    "# Create TDSeries from the time_series DataFrame\n",
    "data_series_df = TDSeries(\n",
    "    data=time_series,\n",
    "    id=\"Base_Product_ID\",\n",
    "    row_index_style=\"TIMECODE\",\n",
    "    row_index=\"Date_Calendar\",\n",
    "    payload_field=\"sum_Sales_Units\",\n",
    "    payload_content=\"REAL\"\n",
    ")\n",
    "\n",
    "# Execute Resample for TDSeries with linear interpolation\n",
    "resampled_timeseries = Resample(\n",
    "    data=data_series_df,\n",
    "    interpolate='LINEAR',\n",
    "    timecode_start_value=\"TIMESTAMP '2023-05-22 00:00:00.000000'\",\n",
    "    timecode_duration=\"DAYS(1)\"\n",
    ").result\n",
    "\n",
    "# Assign the 'Date_Calendar' field from the row index and reorder columns\n",
    "resampled_timeseries = resampled_timeseries.assign(\n",
    "    Date_Calendar=resampled_timeseries.ROW_I\n",
    ")[time_series.columns]\n",
    "\n",
    "# Output the resampled DataFrame\n",
    "resampled_timeseries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a3895-ba87-4df0-9dd6-1dd24e895909",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The Smoothma() function applies a smoothing function to a time series which results in a series that highlights the time series mean. For non-stationary time series with non-constant means, the smoothing function is used to create a result series. When the result series is subtracted from the original series, it removes the non-stationary mean behavior.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here, we will create a TDSeries object from the resampled_timeseries DataFrame and than apply exponential moving average smoothing with a lambda value of 0.5 to the time series data. The smoothed DataFrame is adjusted to match the original column names.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98029ac0-a16e-4d77-8cad-8a7ab6043526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new TDSeries from the resampled timeseries DataFrame\n",
    "data_series_df = TDSeries(\n",
    "    data=resampled_timeseries,\n",
    "    id=\"Base_Product_ID\",\n",
    "    row_index_style=\"TIMECODE\",\n",
    "    row_index=\"Date_Calendar\",\n",
    "    payload_field=\"sum_Sales_Units\",\n",
    "    payload_content=\"REAL\"\n",
    ")\n",
    "\n",
    "# Apply exponential smoothing using Smoothma\n",
    "smoothed_time_series = Smoothma(\n",
    "    data=data_series_df,\n",
    "    ma='EXPONENTIAL',\n",
    "    lambda1=0.5\n",
    ").result\n",
    "\n",
    "# Assign the 'Date_Calendar' from the row index and reorder columns\n",
    "smoothed_time_series = smoothed_time_series.assign(\n",
    "    Date_Calendar=smoothed_time_series.ROW_I\n",
    ")[time_series.columns]\n",
    "\n",
    "# Output the smoothed DataFrame\n",
    "smoothed_time_series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79caba-010e-4d7c-9c30-0fbd9992d5a8",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will visualize the smoothed sum_ACTUAL_SALES_SINGLES field over the CALENDAR_DATE for the selected products (selected_products)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713609e-a7b7-498b-bd4e-8553bda935a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcurves(\n",
    "    smoothed_time_series,\n",
    "    select_id= selected_products, \n",
    "    field='sum_Sales_Units', \n",
    "    row_axis = 'Date_Calendar', \n",
    "    series_id = 'Base_Product_ID', \n",
    "    row_axis_type = 'SEQUENCE',\n",
    "    legend='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d3b78-b5e1-4418-a8e5-fa166f1a3f4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>6. Data Preparation for Clustering</b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>6.1 Identify NULL periods in Time Series</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We calculate the duration between consecutive sales dates for each product and the results are stored in a DataFrame named df_consecutive_NULLs, sorted by the number of consecutive days without sales in descending order.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f218ce-03fe-4685-9563-97c74475bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = DataFrame(in_schema(\"DEMO_ProductHierarchy\",\"Retail_Product_Hierarchy\"))\n",
    "time_series = time_series[['Date_Calendar','Base_Product_ID','Sales_Units']]\\\n",
    "                .groupby(['Date_Calendar','Base_Product_ID']).sum()\n",
    "time_series.materialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa628d-2033-410a-90d0-8ceb9259bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    Date_Calendar\n",
    "    ,Base_Product_ID \n",
    "    ,CAST((INTERVAL(DURATION) DAY(4)) AS FLOAT) AS FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES\n",
    "FROM (\n",
    "    SELECT\n",
    "          Date_Calendar\n",
    "          ,Base_Product_ID\n",
    "          ,sum_Sales_Units\n",
    "          ,LEAD(Date_Calendar) OVER (PARTITION BY Base_Product_ID ORDER BY Date_Calendar) AS NEXT_DATE\n",
    "          ,PERIOD(Date_Calendar, NEXT_DATE) AS DURATION\n",
    "    FROM {time_series._table_name}\n",
    "    QUALIFY NEXT_DATE IS NOT NULL\n",
    "    ) A\n",
    "\"\"\"\n",
    "\n",
    "df_consecutive_NULLs = DataFrame.from_query(query).sort('FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES', ascending=False)\n",
    "df_consecutive_NULLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc6f9c-29c6-421b-a1f5-9afd3fabb60c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We aggregate the data to compute various statistics for each product</p>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The data is grouped by <code>BASE_PRODUCT_NUMBER</code></li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Aggregated statistics (minimum, maximum, mean, standard deviation, count, and median) of the <code>FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES</code> are calculated for each product.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Data is filtered based on the <code>max_FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES</code> column.</li></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f70fd-7455-47e9-a223-de9c69522ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df_consecutive_NULLs.groupby('Base_Product_ID')\\\n",
    "            .agg({'FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES' : ['min','max','mean','std','count','median']})\n",
    "dataset = dataset[dataset.max_FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES>1]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9319297-990e-4aeb-a79c-cf447a3c3790",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>6.2 - K-means</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We’ll focus on identifying null periods or long periods where we do not have any data for a product. If we have an abandoned product, we expect that after its end of life, there will be no data for this product. It’s important to recognize this because it can be misleading if we compute correlations or other statistics on it. We need to spot these periods quickly, as they can indicate data quality issues. For instance, there might be no sales at all for a product, possibly due to a broken supply chain that later resumed. This affects the reliability of our correlation analysis, as we expect a regular time series of sales, whether seasonal or not. In reality, we might encounter missing values or long periods of zero sales, which disrupt the product’s dynamics.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here’s what we aim to do:</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Identify null periods in the time series.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Identify zero sales periods in the time series.</li>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We’ll use a SQL query to count and build periods of non-missing values. By using functions like LEAD and LAG, we can compute the duration of these zero sales periods.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>For example, for a product starting at a specific date, we might find a 218-day period without any data. Similarly, another product might have a 197-day period without data. Multiple periods with no data can occur for different products, resulting in scattered or sparse time series.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We’ll analyze each base product to determine the minimum, maximum, standard deviation, count, and median of these missing values. This analysis will be very insightful. For instance, a product might have at least two periods without sales, which could be at the beginning and the end of its lifecycle. We can also calculate the mean of these periods.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c65c79-b463-4aef-af92-188ea3467830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradatasqlalchemy.types import *\n",
    "dataset = dataset.assign(\n",
    "    count_FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES=dataset['count_FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES'].cast(FLOAT))\n",
    "\n",
    "dataset.to_sql(table_name = 'kmeans_dataset', temporary = True, primary_index = 'Base_Product_ID')\n",
    "dataset = DataFrame('kmeans_dataset')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f0adf-56f9-4f79-b277-d83030e10295",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will define a list of feature columns to be used in the clustering analysis. The list features includes the columns <code>max_FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES</code>, <code>mean_FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES</code>, <code>std_FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES</code>, and <code>count_FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES</code>.\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Next, we can run multiple K-means to plot the elbow, which is what we’ll do here. We can see the plot elbow function that calls the in-database K-means. We’ll loop over the number of clusters we ask for in K-means, and this is how you call it. First, we’ll find 100 K-means with two clusters, then three, and so on until ten. </p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'> The <code>plot_elbow</code> function uses the below parameters to determine the optimal number of clusters for K-means clustering:</p>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The <code>dataset</code> DataFrame is passed along with the list of feature columns.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The <code>BASE_PRODUCT_NUMBER</code> column is used as the index.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The maximum number of clusters to consider is set to 10.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Data scaling is enabled.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Additional arguments for the K-means algorithm, such as the random seed, are specified.</li></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e69a2-115e-496c-a4be-fbd4ae76d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnpathviz.visualizations import plot_elbow\n",
    "features = ['max_FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES','mean_FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES',\n",
    "            'std_FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES','count_FOLLOWING_NB_DAYS_WITHOUT_SALES_VALUES']\n",
    "plot_elbow(\n",
    "    tddf             = dataset,\n",
    "    features         = features,\n",
    "    index_columns    = 'Base_Product_ID',\n",
    "    nb_cluster_max   = 10,\n",
    "    scaling          = True,\n",
    "    tdml_KMeans_args = {'seed' : 42}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3a4fc-bf4a-4a89-a49c-bc96328465e5",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here it is, and we get the famous elbow plot that indicates the optimal number of clusters with the K-means method, which is where the elbow occurs. We can imagine an arm with the elbow here. We have two plots depending on whether we want to plot the distortion or the inertia. Sometimes the elbow is clearer on one plot than the other.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here, we can see that four clusters are the best for this product.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f10c1f-1596-4779-9234-4bf9158b3ff2",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>6.3 - Cluster with the best k</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The K-means() function groups a set of observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid). This algorithm minimizes the objective function, that is, the total Euclidean distance of all data points from the center of the cluster as follows:</p>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Specify or randomly select k initial cluster centroids.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Assign each data point to the cluster that has the closest centroid.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Recalculate the positions of the k centroids.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Repeat steps 2 and 3 until the centroids no longer move.</li>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The algorithm doesn't necessarily find the optimal configuration as it depends significantly on the initial randomly selected cluster centers. User can run the function multiple times to reduce the effect of this limitation.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will do K-means clustering on the dataset using the below parameters:\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The <code>KMeans</code> class from <code>teradataml</code> is used to perform clustering.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The <code>BASE_PRODUCT_NUMBER</code> column is used as the identifier.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The previously defined feature columns are used as the target columns.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The number of clusters is set to <code>k</code> <b>(4)</b>.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>A random seed is specified for reproducibility.</li>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46577837-d80c-40bf-9668-4a1cea333e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c692a-ef04-4de9-a195-de4defd1f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import KMeans, KMeansPredict\n",
    "KMeans_out = KMeans(\n",
    "    id_column      = \"Base_Product_ID\",\n",
    "    target_columns = features,\n",
    "    data           = dataset,\n",
    "    num_clusters   = k,\n",
    "    seed           = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b273cf-8bf6-44a9-91b4-cafe9a4e1977",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will predict the cluster assignments for the dataset using the trained K-means model. The trained K-means model (<code>KMeans_out.result</code>) and the dataset are passed as arguments.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f2cfe-8ad3-4a6e-ba0b-b34d409c79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeansPredict_out = KMeansPredict(\n",
    "    object = KMeans_out.result,\n",
    "    data   = dataset\n",
    ")\n",
    "KMeansPredict_out.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe01fbb-49de-466f-84de-d9f7c381393f",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>6.4 - Visualize the clusters</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bee32a-d387-4522-bdbe-5e0b9a1c1f2a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "We will merge the original dataset with the K-means prediction results and than visualize the clusters using the pair_plot</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e6d5e-7063-4264-aacb-4a0aa20e6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dataset.join(KMeansPredict_out.result, on='Base_Product_ID',lprefix='l', rprefix='r')\n",
    "res = res.assign(Base_Product_ID=res.l_Base_Product_ID)[['Base_Product_ID'] + features + ['td_clusterid_kmeans']]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0131cf0-db52-4ca5-a1fb-a56083f60dcf",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Visualize the clusters using pair_plot</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e01c4-2a1d-41fc-8064-22c8273af2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnpathviz.visualizations import scatter_plot, pair_plot\n",
    "pair_plot(res[features + ['td_clusterid_kmeans']], series_id='td_clusterid_kmeans', \n",
    "          width=2000, height = 2000,markersize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bb830-92d1-4786-ac64-a063ef54b527",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>7.  Identify ZERO SALES periods in Time Series</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c256049-7249-4792-b0b5-4f70f618d8b5",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The function <code>TD_NORMALIZE_OVERLAP_MEET</code> prepares the time series by identifying periods with zero sales. It merges consecutive periods of zero sales that overlap or touch each other into a single period. This is what TD_NORMALIZE_OVERLAP_MEET does. For example, if two periods overlap or meet, they collapse into a single period. This function is very powerful and has been used effectively in various scenarios, including at the bank.</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Prepare the periods.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Use the TD_NORMALIZE_OVERLAP_MEET function.</li>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>You can see the query and the results. For instance, a product might have a period of over nine months with zero sales. This feature is interesting because it allows us to combine and make statistics on these periods, checking if they occur at the beginning or end of the product’s lifecycle.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>This helps generate consistent and complete time series before returning to the correlation analysis we discussed earlier. You can also resample or smooth your time series before computing correlations and use a restricted time window to exclude zero sales from your computation. This ensures that the correlations you compute make sense and are not disturbed by other factors like stock shortages.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We have created a function add_overlaps.This function is designed to normalize overlapping time periods in a Teradata DataFrame using Teradata's built-in normalization functions. The function accepts a DataFrame and several optional parameters to specify the method of normalization, column names for unique identifiers, and start and end dates. It constructs and executes a SQL query to process the periods and returns a new DataFrame with normalized periods and additional details.\n",
    "</p>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The function first attempts to retrieve the table name from the DataFrame.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>It constructs a subquery to select the unique identifier and create a PERIOD type from the start and end dates.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>A main query is then built to compute overlaps using the specified normalization method.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The query is printed for debugging purposes.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Finally, the query is executed, and the result is returned as a new DataFrame.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f634ebb-48b7-4313-abcf-5127c8e16e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_overlaps(df, method='TD_NORMALIZE_OVERLAP', new_period_name='durations', ID_CTP='obligor_id', date_beg='Default_Phase_Start_Date', date_end='Default_Phase_End_Date'):\n",
    "    \"\"\"\n",
    "    Adds overlaps in time periods for a given DataFrame using Teradata's normalization functions.\n",
    "    \n",
    "    This function leverages Teradata's `TD_NORMALIZE_OVERLAP` or `TD_NORMALIZE_OVERLAP_MEET` to process \n",
    "    period data, ensuring that any overlapping or meeting periods are normalized. This can be useful for \n",
    "    financial, project management, or any domain where time period data requires such normalization.\n",
    "\n",
    "    Args:\n",
    "        df (teradataml.DataFrame): The input DataFrame containing period data to normalize.\n",
    "        method (str, optional): The Teradata function to use for normalization. Options are \n",
    "            'TD_NORMALIZE_OVERLAP' to handle overlapping periods and 'TD_NORMALIZE_OVERLAP_MEET' to handle \n",
    "            periods that either overlap or meet exactly. Defaults to 'TD_NORMALIZE_OVERLAP'.\n",
    "        new_period_name (str, optional): The name for the new period column created after normalization.\n",
    "            Defaults to 'durations'.\n",
    "        ID_CTP (str, optional): The column name for the unique identifier within the DataFrame.\n",
    "            Defaults to 'obligor_id'.\n",
    "        date_beg (str, optional): The column name for the start date of the period. Defaults to \n",
    "            'Default_Phase_Start_Date'.\n",
    "        date_end (str, optional): The column name for the end date of the period. Defaults to \n",
    "            'Default_Phase_End_Date'.\n",
    "    \n",
    "    Returns:\n",
    "        teradataml.DataFrame: A new DataFrame containing the normalized periods, with additional details\n",
    "        such as the normalized count and the end of each period.\n",
    "\n",
    "    Notes:\n",
    "        - `TD_NORMALIZE_OVERLAP` is used to process overlapping periods by merging them and providing \n",
    "          normalized counts.\n",
    "        - `TD_NORMALIZE_OVERLAP_MEET` handles both overlapping and exactly meeting periods, treating them\n",
    "          as continuous periods for normalization purposes.\n",
    "        - The function assumes the input DataFrame is a Teradata DataFrame, utilizing Teradata SQL \n",
    "          capabilities for the operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Attempt to get the table name from the DataFrame\n",
    "        view_name = df._table_name\n",
    "    except:\n",
    "        # Execute the node and set the table name if not already set\n",
    "        df._DataFrame__execute_node_and_set_table_name(df._nodeid, df._metaexpr)\n",
    "        view_name = df._table_name\n",
    "    \n",
    "    # Subquery to select the ID and create a PERIOD type from start and end dates\n",
    "    subquery = f\"\"\"\n",
    "                SELECT\n",
    "                           {ID_CTP}\n",
    "                       ,   PERIOD(\n",
    "                                   CAST({date_beg} AS TIMESTAMP(6) WITH TIME ZONE)\n",
    "                                ,  CAST({date_end} AS TIMESTAMP(6) WITH TIME ZONE)) as {new_period_name}\n",
    "                    FROM {view_name}\n",
    "                \"\"\"\n",
    "    \n",
    "    # Main query to compute overlaps using the specified method\n",
    "    query = f\"\"\"\n",
    "                WITH subtbl({ID_CTP}, {new_period_name}) AS\n",
    "                   ({subquery}\n",
    "                    )\n",
    "                SELECT \n",
    "                    A.{ID_CTP}\n",
    "                ,   A.{new_period_name}\n",
    "                ,   A.NrmCount\n",
    "                ,   INTERVAL(A.{new_period_name}) MONTH AS NB_MONTH\n",
    "                ,   BEGIN(A.{new_period_name}) AS BEGIN_PERIOD\n",
    "                ,   END(A.{new_period_name}) AS END_PERIOD\n",
    "                FROM\n",
    "                (SELECT {ID_CTP}, {new_period_name}, NrmCount\n",
    "                FROM TABLE (TD_SYSFNLIB.{method}(NEW VARIANT_TYPE(subtbl.{ID_CTP}),\n",
    "                                                             subtbl.{new_period_name})\n",
    "                RETURNS (id____ BIGINT, time_duration____ PERIOD(TIMESTAMP(6) WITH TIME ZONE), NrmCount INT)\n",
    "                HASH BY {ID_CTP}     \n",
    "                LOCAL ORDER BY {ID_CTP}, {new_period_name})     \n",
    "                AS DT({ID_CTP}, {new_period_name}, NrmCount)) A\n",
    "                \"\"\"\n",
    "    \n",
    "    # Print the query for debugging purposes\n",
    "    print(query)\n",
    "    \n",
    "    # Execute the query and return the result as a DataFrame\n",
    "    return DataFrame.from_query(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe726ad-410e-4fcb-b40b-3ec6532ba79e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will construct a subquery to identify periods where actual sales are zero for each product. It uses the LEAD window function to find the next calendar date for each product and creates a PERIOD type for the duration between the current and next date.</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The subquery selects the base product number and the duration period.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>It filters the results to include only those periods where actual sales are zero.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The query is executed, and the result is returned as a DataFrame.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52466b5-1ed3-45ed-8d4f-51405bcc4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "subquery = f\"\"\"\n",
    "SELECT CAST(Base_Product_ID AS BIGINT) AS Base_Product_ID, DURATION\n",
    "FROM (\n",
    "    SELECT\n",
    "      Date_Calendar\n",
    "      ,Base_Product_ID\n",
    "      ,sum_Sales_Units\n",
    "      ,LEAD(Date_Calendar) OVER (PARTITION BY Base_Product_ID ORDER BY Date_Calendar) AS NEXT_DATE\n",
    "      ,PERIOD(Date_Calendar, NEXT_DATE) AS DURATION\n",
    "    FROM {time_series._table_name}\n",
    "    QUALIFY NEXT_DATE IS NOT NULL\n",
    "    ) A\n",
    "    WHERE sum_Sales_Units = 0\n",
    "\"\"\"\n",
    "DataFrame.from_query(subquery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9b832-7ca1-4c97-a560-711fdd258754",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will construct a subquery to identify periods where actual sales are zero for each product, similar to the previous cell, but it includes additional details like the calendar date and the next date.</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The subquery selects the base product number, calendar date, and next date.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>It uses the LEAD window function to find the next calendar date for each product.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>It filters the results to include only those periods where actual sales are zero and the next date is not null.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The query is executed, and the result is returned as a DataFrame.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a859d95-b0aa-40c4-aefa-9fb775e49f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "subquery = f\"\"\"\n",
    "SELECT CAST(A.Base_Product_ID AS BIGINT) AS Base_Product_ID, A.Date_Calendar, A.NEXT_DATE\n",
    "FROM (\n",
    "    SELECT\n",
    "          Date_Calendar\n",
    "        , Base_Product_ID\n",
    "        , sum_Sales_Units\n",
    "        , LEAD(Date_Calendar) OVER (PARTITION BY Base_Product_ID ORDER BY Date_Calendar) AS NEXT_DATE\n",
    "    FROM {time_series._table_name}\n",
    "    ) A\n",
    "    WHERE sum_Sales_Units = 0 AND NEXT_DATE IS NOT NULL\n",
    "\"\"\"\n",
    "df = DataFrame.from_query(subquery)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7dc508-c7fc-4028-9754-0b42d66cac3a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will use the add_overlaps function to normalize periods of zero sales for each product. It specifies the method TD_NORMALIZE_OVERLAP_MEET to handle both overlapping and meeting periods.</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The add_overlaps function is called with the DataFrame from the previous cell and appropriate parameters.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The resulting DataFrame, df_zeros, contains the normalized periods.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The DataFrame is then sorted by the normalized count in descending order.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c377d-4e25-4b52-91ad-b35e8326dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zeros = add_overlaps(\n",
    "    df, \n",
    "    method='TD_NORMALIZE_OVERLAP_MEET',\n",
    "    new_period_name='durations', \n",
    "    ID_CTP='Base_Product_ID', \n",
    "    date_beg = 'Date_Calendar', \n",
    "    date_end='NEXT_DATE')\n",
    "\n",
    "df_zeros\n",
    "\n",
    "df_zeros.sort('NrmCount', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d690c-fa32-42cc-9fd0-414ddcf9a055",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>8.  Normalize Time Series</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b04a1ca-974f-441c-ac4a-0e846dce961c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>8.1 Normalize the time series</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'> We will normalize the sales units by dividing by the total sales units for each product and week. </p>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Adds new columns for the day of the week and a unique week identifier.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>Aggregates sales units by product, date, day, and week.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>The code normalizes the sales units by dividing by the total sales units for each product and week.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'>It filters out weeks that do not have sales data for all seven days.</li></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec5595-8dfd-4a6c-b889-114db120addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataFrame(in_schema(\"DEMO_ProductHierarchy\",\"Retail_Product_Hierarchy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4efdf7-3984-4239-bb32-482f320378cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.assign(day_ = dataset.Date_Calendar.day_of_week(), \n",
    "                         week_ =  dataset.Base_Product_ID*100000 + dataset.Date_Calendar.week() \n",
    "                         + 100*dataset.Date_Calendar.year())\n",
    "\n",
    "time_series = dataset[['Base_Product_ID','Date_Calendar','day_','week_','Sales_Units']]\\\n",
    "                .groupby(['Base_Product_ID','Date_Calendar','day_','week_']).sum()\n",
    "time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6466b1ce-e65e-40d6-825c-957f3eea7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_time_series = time_series.join(\n",
    "    other   = time_series[['Base_Product_ID','week_','sum_Sales_Units']].\n",
    "    groupby(['Base_Product_ID', 'week_']).\n",
    "    agg({'sum_Sales_Units':['sum','count']}),\n",
    "    on      =['Base_Product_ID','week_'],\n",
    "    how     ='inner',\n",
    "    rprefix ='r')\n",
    "\n",
    "normalized_time_series = normalized_time_series[(normalized_time_series.sum_sum_Sales_Units > 0) & \n",
    "                                                (normalized_time_series.count_sum_Sales_Units == 7)]\n",
    "\n",
    "normalized_time_series = normalized_time_series.assign(\n",
    "        normalized_Sales_Units = \n",
    "        normalized_time_series['sum_Sales_Units']/normalized_time_series['sum_sum_Sales_Units'])\n",
    "\n",
    "normalized_time_series = normalized_time_series[time_series.columns+['normalized_Sales_Units']]\n",
    "\n",
    "normalized_time_series.sort('normalized_Sales_Units', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94820bb9-217b-4930-8c1f-c79c6e3f0ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcurves(\n",
    "    normalized_time_series,\n",
    "    field     = 'normalized_Sales_Units',\n",
    "    row_axis  = 'day_', \n",
    "    series_id = 'week_',\n",
    "    row_axis_type = 'SEQUENCE',\n",
    "    legend=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852df1c-469d-4d6c-95f4-9be3693ca69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcurves(\n",
    "    normalized_time_series[normalized_time_series.Base_Product_ID == 6282552604],#8132739927],\n",
    "    field     = 'normalized_Sales_Units',\n",
    "    row_axis  = 'day_', \n",
    "    series_id = 'week_',\n",
    "    row_axis_type = 'SEQUENCE',\n",
    "    legend=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d19a2-918f-47d7-a4c4-42aad3c101ad",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>8.2 Filter Outliers using In-DB functions</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The OutlierFilterFit() function calculates the lower_percentile, upper_percentile, count of rows and median for all the \"target_columns\" provided by the user. These metrics for each column helps the function OutlierTransform() detect outliers in the input table. It also stores parameters from arguments into a FIT table used during transformation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3f64a-d9e0-425c-b817-db767a6daf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import OutlierFilterFit, OutlierFilterTransform\n",
    "OutlierFilterFit_out = OutlierFilterFit(\n",
    "                            data = normalized_time_series,\n",
    "                            target_columns = \"normalized_Sales_Units\",\n",
    "                            group_columns = 'day_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd54287b-ef74-4262-b1f5-58f15cb6c617",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>OutlierFilterTransform() function filters the outliers from the input teradataml DataFrame. OutlierFilterTransform() uses the result DataFrame from OutlierFilterFit() function to get statistics like median, count of rows, lower percentile and upper percentile for every column specified in target columns argument and filters the outliers in the input data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0173aee4-bc71-4f37-8f06-bdff2777b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_normalized_time_series = OutlierFilterTransform(\n",
    "                                            data=normalized_time_series,\n",
    "                                            object=OutlierFilterFit_out.result,\n",
    "                                            data_partition_column = 'day_',\n",
    "                                            object_partition_column = 'day_').result\n",
    "\n",
    "cleaned_normalized_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f94e3-a777-499e-9181-faca49709046",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotcurves(\n",
    "    cleaned_normalized_time_series[cleaned_normalized_time_series.Base_Product_ID == 6282552604],  #8132739927],\n",
    "    field     = 'normalized_Sales_Units',\n",
    "    row_axis  = 'day_', \n",
    "    series_id = 'week_',\n",
    "    row_axis_type = 'SEQUENCE',\n",
    "    legend=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a078e6-eccd-4859-8f6e-03641cf902d7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>After transforming the outliers from the time series data, we will count the no of days for which sales is available and cleanse the time series data by removing the data for which the day count is not equal to 7 days(week)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a347fcc-edf9-4193-98f8-0995332fa5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_normalized_time_series.groupby(['Base_Product_ID','week_']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22b95b-7b45-423d-9246-499ea3c22687",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_cleaned_time_series = cleaned_normalized_time_series.join(\n",
    "    other   = cleaned_normalized_time_series.groupby(['Base_Product_ID','week_']).count(),\n",
    "    on      =['Base_Product_ID','week_'],\n",
    "    how     ='inner',\n",
    "    rprefix ='r')\n",
    "\n",
    "super_cleaned_time_series = super_cleaned_time_series[super_cleaned_time_series.count_normalized_Sales_Units == 7]\n",
    "super_cleaned_time_series = super_cleaned_time_series[cleaned_normalized_time_series.columns]\n",
    "\n",
    "super_cleaned_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80acd6aa-c56d-49b9-9618-6976d8c47903",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will pivot the data for each base product id and week id and calculate the sum of sales based on these,</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b33fd5-3f74-4450-b01d-7ec6424de1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_dataset = super_cleaned_time_series[['Base_Product_ID','week_','day_','normalized_Sales_Units']]\\\n",
    "                    .pivot(columns = super_cleaned_time_series.day_,\n",
    "                           aggfuncs=super_cleaned_time_series.normalized_Sales_Units.sum())\n",
    "\n",
    "pivoted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe977a-a564-442a-9d0e-630b1af83bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_dataset.to_sql(table_name = 'pivoted_dataset', \n",
    "                       if_exists = 'replace', \n",
    "                       types={'sum_normalized_sales_units_'+str(i+1) : FLOAT() for i in range(7)})\n",
    "\n",
    "pivoted_dataset = DataFrame('pivoted_dataset')\n",
    "pivoted_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa7b93-0add-4394-8f3e-7a92db5fbdd7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will visualize the normalized and pivoted data using elbow plots and find the appropriate number of clusters</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de329b1-306d-4d0a-8e63-bcc892e2b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnpathviz.visualizations import plot_elbow\n",
    "plot_elbow(\n",
    "    tddf             = pivoted_dataset,\n",
    "    features         = pivoted_dataset.columns[2::],\n",
    "    index_columns    = 'Base_Product_ID',\n",
    "    nb_cluster_max   = 20,\n",
    "    scaling          = True,\n",
    "    tdml_KMeans_args = {'seed' : 42}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33323047-170a-4ca4-891c-0d873ee0e20e",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>8.3 Clustering data based on the sales</b></p> \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will add a 'DummyId' column to the 'pivoted_dataset' DataFrame using the FillRowId function from the Teradata ML library. The FillRowId() function adds a column of unique row identifiers to the input DataFrame.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694560a8-0b5a-4481-a4aa-f3235e16544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import FillRowId\n",
    "res = FillRowId(data=pivoted_dataset,\n",
    "                row_id_column='DummyId').result\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c590a26-7cc6-4ae4-b6b5-960a473d336e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Based on the elbow plot we set the number of clusters (k) to 7 and retrieves the column names from the third column to the second last column of the res DataFrame. This is likely in preparation for clustering.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f545f-d6b1-466a-a75f-78e7a9340843",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "res.columns[2:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1593b-0757-4424-aadd-3c96451128ff",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We perform K-Means clustering on the res DataFrame using the KMeans function using 'DummyId' as the ID column, the previously selected columns as target columns, and sets the number of clusters to 7 with a seed of 42 for reproducibility.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77498c6f-f5c8-437b-8274-2a920e179a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_out = KMeans(\n",
    "    id_column      = 'DummyId',\n",
    "    target_columns = res.columns[2:-1],\n",
    "    data           = res,\n",
    "    num_clusters   = k,\n",
    "    seed           = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65845918-e71d-43cb-b8b3-37e244bcf53c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The KMeansPredict function is used to predict cluster assignments for the res DataFrame based on the K-Means model stored in KMeans_out.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9587d2-0553-4480-b528-0e90ae7ee98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeansPredict_out = KMeansPredict(\n",
    "    object = KMeans_out.result,\n",
    "    data   = res\n",
    ")\n",
    "KMeansPredict_out.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959edf3-ae0f-4948-be75-8a9eea46a220",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will filter the K-Means clustering result to include only rows where the cluster ID is greater than -1, indicating valid cluster assignments. The normalized sales units are visualized for all these clusters to undersatnd the sales for each cluster.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d782d-9c5f-42da-a045-7b4d4e56d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "KMeans_out.result[KMeans_out.result.td_clusterid_kmeans>-1]\n",
    "KMeans_out.result[KMeans_out.result.td_clusterid_kmeans>-1]\\\n",
    "        [['sum_normalized_sales_units_'+str(i+1) for i in range(7)]].to_pandas().T.plot(figsize=(15,8))\n",
    "plt.xticks(rotation=60)\n",
    "plt.title(\"Sales Units By Clusters of Products\", fontdict = {'fontsize': 16})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb60ce-5cad-446d-a819-bd28adbe8de1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The main focus is on the central shape of the week for each cluster. Typically, the centroids of these clusters are well-formed around a central point, providing a good representation of the weekly pattern.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>From the plot of the shape of the week. The typical weekly shapes in this dataset show distinct patterns. For example, the blue line starts very low at the beginning, likely representing Sunday, and then shows a decrease on Monday. In contrast, the red line shows an increase towards the end of the week.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88082032-6fea-4a00-90be-edecc0f347d8",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>Conclusion:</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Thus, by aligning product hierarchies with customer shopping behavior, products are grouped based on similar sales and demand patterns, indicating a strong correlation in their sales time series. </p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>These patterns should be interpreted by examining which products belong to each weekly shape. It’s also important to consider narrowing the range in terms of months or seasons, as different shapes may emerge. By tagging the different weekly shapes for various products, we can determine if these patterns remain consistent throughout the year or vary with the seasons. This analysis should be compared with the seasonality of the products to provide a comprehensive understanding.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b88bc3-47a9-447c-aaf1-dfd736004cc4",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>9. Cleanup</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e8d00-7574-47eb-8f32-50cbd9e52e22",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>Databases and Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will use the following code to clean up tables and databases created for this demonstration.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df1d31b-c80e-49a5-a22c-c528c3eea64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_ProductHierarchy');\" \n",
    "#Takes 45 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477afad-aca6-4148-82ee-60f57142f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8189e89-05a2-4f66-af9a-d50cdb6e7d63",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; background:#f9f9f9; border-bottom:3px solid #00233C\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2024 All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
