{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bc532d-c78c-4c89-b191-242da0733f39",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Teradata Enterprise Vector Store : Vectorizing PDF, Audio and Text files\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff71661-19b4-423a-867a-7c815b064c81",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Introduction:</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In our chat with the documentation and database system using Generative AI, we have combined <b>RAG, Langchain, LLM models, and SQLAgents.</b> This allows us to ask queries in layman's terms, retrieve relevant information from the Vector store and/or Vantage Table, and generate accurate and concise answers based on the retrieved data. This integration of retrieval-based and generative-based approaches provides a powerful tool for extracting knowledge from structured or unstructured sources like PDFs, text, or audio files and delivering user-friendly responses.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demo we will build Chatbot type feature by using LangChain, a powerful library for working with LLMs like <b>OpenAI's GPT-4, Amazon's Titan, Anthropic Claude 3.5, etc.</b> and JumpStart in ClearScape notebooks, a system is built where users can ask business questions in natural English and receive answers with data drawn from the relevant databases.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following diagram illustrates the architecture.</p>\n",
    "\n",
    "<center><img src=\"images/rag1.png\" alt=\"architecture\"  width=1200 height=1000 style=\"border: 4px solid #404040; border-radius: 10px;\"/></center>\n",
    "\n",
    "<br>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Before going any farther, let's get a better understanding of RAG, LangChain, and LLM.</p>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial'><b><li> Retrieval-Augmented Generation (RAG):</li></b></ol>\n",
    "<p style = 'font-size:16px;font-family:Arial'> &emsp;  &emsp;RAG is a framework that combines the strengths of retrieval-based and generative-based approaches in question-answering systems.It utilizes both a retrieval model and a generative model to generate high-quality answers to user queries. The retrieval model is responsible for retrieving relevant information from a knowledge source, such as a database or documents. The generative model then takes the retrieved information as input and generates concise and accurate answers in natural language.</p>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>A typical RAG (Retrieval-and-Generation) application has two main components:</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Indexing:</b> a pipeline for ingesting data from a source and indexing it. This usually happens offline. The indexing process involves several steps, including loading the data, splitting it into smaller chunks, and storing and indexing the splits. This is often done using a VectorStore and Embeddings model.</p>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Retrieval and generation:</b> the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. The retrieval process involves searching the index for the most relevant data based on the user query, and then passing that data to the model for generation.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The most common full sequence from raw data to answer looks like:</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Indexing</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li><b>Load:</b> Load: First we need to load our data. We'll use <code>PyMuPDFLoader</code> for this.</li>\n",
    "    <li><b>Split:</b> Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't in a model's finite context window. Here, our pdf document will be splits into pages.</li>\n",
    "    <li><b>Store:</b> We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model</li>\n",
    "    </ul>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following diagram illustrates the architecture of load, split and store.</p>\n",
    "\n",
    "<center><img src=\"images/rag_load_store.png\" alt=\"rag indexing architecture\"  width=800 height=600 style=\"border: 4px solid #404040; border-radius: 20px;\"/></center>\n",
    "<center>image source: <a href=\"https://python.langchain.com/docs/use_cases/question_answering/\">langchain.com</a></center>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Retrieval and generation</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li><b>Retrieval:</b> During runtime, the user inputs a query. We first generate embeddings for it, which are then passed to the Vantage in-db function <b>TD_VectorDistance</b> to retrieve similar documents as context. This context is then fed into the LLM model.</li>\n",
    "    <li><b>Generation:</b> Finally, the model generates an answer based on the retrieved data. The answer is then presented to the user.</li>\n",
    "    </ul>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial'>The following diagram illustrates the architecture of retrieval and generation.</p>\n",
    "<center><img src=\"images/rag_retrieval_generation_td.png\" alt=\"retrieval generation architecture\" width=800 height=600 style=\"border: 4px solid #404040; border-radius: 10px;\"/></center>\n",
    "<center>image source: <a href=\"https://python.langchain.com/docs/use_cases/question_answering/\">langchain.com</a></center>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial' start=\"2\"><b><li> Langchain:</li></b></ol>\n",
    "<p style = 'font-size:16px;font-family:Arial'> &emsp;  &emsp; LangChain is a framework that facilitates the integration and chaining of large language models with other tools and sources to build more sophisticated AI applications. LangChain does not serve its own LLMs; instead, it provides a standard way of communicating with a variety of LLMs, including those from OpenAI and HuggingFace. LangChain accelerates the development of AI applications with building blocks. We learn the leverage the following building blocks in this notebook:</p>\n",
    " \n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li> <b> LLMs</b> – LangChain's <code>llm</code> class is designed to provide a standard interface for all LLM it supports.   </li>\n",
    "    <li> <b> PromptTemplate</b>  - LangChain’s <code>PromptTemplate</code> class are predefined structures for generating prompts for LLM’s. They can be reused across different LLM's.</li>\n",
    "    <li> <b> Chains</b> – When we build complex AI applications, we may need to combine multiple calls to LLM’s and to other components  LangChain’s <code>chain</code> class allows us to link calls to LLM’s and components. The most common type of chaining in any LLM application is combining a prompt template with an LLM and optionally an output parser. </li>\n",
    "</ol>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial' start=\"3\"><b><li> LLM Models (Large Language Models):</li></b></ol>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'> &emsp;  &emsp; LLM models refer to the large-scale language models that are trained on vast amounts of text data.\n",
    "These models, such as GPT-4, Llama 3,  Google's Gemini 1.5, etc. are capable of generating human-like text responses. LLM models have been pre-trained on diverse sources of text data, enabling them to learn patterns, grammar, and context from a wide range of topics. They can be fine-tuned for specific tasks, such as question-answering, natural language understanding, and text generation.\n",
    "LLM models have achieved impressive results in various natural language processing tasks and are widely used in AI applications for generating human-like text responses.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca49b9d-df0b-400a-acf8-0252ab8a2618",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><b>Steps in the analysis:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Configuring the environment</li>\n",
    "    <li>Connect to Vantage</li>\n",
    "    <li>Data Exploration Getting Data for This Demo</li>\n",
    "    <li>Read source data</li>\n",
    "    <li>Generate embeddings from the chunks</li>\n",
    "    <li>Insert Prompts into a Table</li>\n",
    "    <li>Generate Embeddings from the Prompts</li>\n",
    "    <li>Find top 10 matching chunks</li>\n",
    "    <li>Configuring AWS CLI and Initialize Bedrock Model</li>\n",
    "    <li>Test and Compare Results</li>\n",
    "    <li>Cleanup</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833b5bf-74af-42be-8543-3782e1da95dc",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>1. Configuring the environment</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94f1e8-489b-4084-80c6-c9cff1ef6ee8",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.1 Install the required libraries</b></p>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i><b>Note:</b>The installation of the required libraries will take approximately <b>4 to 5 minutes</b> for the first-time installation. However, if the libraries are already installed, the execution will complete within 5 seconds.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8bf123-c1c7-4b3a-8c42-404be0936ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install required libraries from requirements.txt\n",
    "!pip install --upgrade -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bd85d-a80f-4903-bc2d-71f542590450",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install additional ML and LLM libraries\n",
    "!pip install torchaudio transformers litellm langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1efdc-0e1b-4b74-9855-87f6b5732898",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install dataset and video processing libraries\n",
    "!pip install datasets torchcodec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b160ce-5ace-4116-86b6-394d6502553b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><b>Note: </b><i>The above statements will install the required libraries to run this demo. Be sure to restart the kernel after executing the above lines to bring the installed libraries into memory. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61067c88-2e9a-4c92-985b-34dc4ab74a13",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.2 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>\n",
    "\n",
    "<ul style=\"font-size: 16px; font-family: Arial; list-style-type: disc; padding-left: 20px;\">\n",
    "    <li>\n",
    "        <b>teradataml</b>: Enables enables us to establish a connection to our database using the <code>create_context()</code> function and allows us to create virtual DataFrames, which serve as references to database objects, allowing exploration of object storage data and enabling operations directly on Vantage without transferring entire datasets to the client, except when needed. For this demo, we will be exploring a dataset in S3 via a foreign table on Vantage.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>LangChain’s SQLDatabase class </b>: A wrapper around the SQLAlchemy engine to facilitate interactions with databases using SQLAlchemy’s Python SQL toolkit and ORM capabilities.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b> LangChain’s create_sql_agent function</b>: A LangChain function to build a SQL agent by providing a language model and a database connection.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>LangChain’s ChatBedrockConverse class</b>: A common interface for working with Amazons Bedrock's FM's that support chat functionalities.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50a4aa-1211-44fc-8166-317c35253207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "import timeit\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, Dropdown\n",
    "from dotenv import load_dotenv\n",
    "from litellm import completion\n",
    "\n",
    "# Data manipulation and visualization libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Teradata libraries\n",
    "from teradataml import (\n",
    "    create_context,\n",
    "    delete_byom,\n",
    "    execute_sql,\n",
    "    save_byom,\n",
    "    remove_context,\n",
    "    in_schema,\n",
    "    display,\n",
    "    DataFrame,\n",
    "    db_drop_table,\n",
    "    db_drop_view,\n",
    "    VectorDistance,\n",
    "    configure,\n",
    "    ONNXEmbeddings,\n",
    ")\n",
    "\n",
    "# Helper functions\n",
    "from utils.sql_helper_func import *\n",
    "from utils.transcripts_helper_func import *\n",
    "\n",
    "# LLM libraries - Updated for langchain 1.0.5\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "import bs4\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "display.max_rows = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30714e67-7ba2-449f-8970-3aac125e48d8",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.3 Load Audio model and test</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Let's load the <b>Small Language Model (SLM)</b> from <code>huggingface</code> and verify it output.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56af632-837c-4d94-a950-ca93106d4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure Hugging Face environment\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    # Load Whisper model and processor\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "    model.config.forced_decoder_ids = None\n",
    "\n",
    "    # Load dummy dataset for testing audio transcription\n",
    "    ds = load_dataset(\n",
    "        \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n",
    "    )\n",
    "    sample = ds[2][\"audio\"]\n",
    "\n",
    "    # Process audio sample\n",
    "    input_features = processor(\n",
    "        sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "\n",
    "    # Generate token ids and decode to text\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Display transcription result\n",
    "    print(\"--\" * 25)\n",
    "    print(\"Transcription: \\n\", transcription[0])\n",
    "    print(\"--\" * 25)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or testing Whisper model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11368d-2efb-4906-8e28-d50f0bca6429",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i>The code above will download the necessary models to generate the embeddings required to run this demo. The initial download may take approximately 50-60 seconds if you are running this demo for the first time in this environment. However, subsequent runs will be much faster since the models will already be available locally.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59718f8-7af4-4d1a-abc7-a860eb7cbae3",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>2. Connect to Vantage</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83770df6-b923-4cc4-a839-de55c62b32ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>2.1 Connect to Vantage</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164cfc91-93ed-45b9-98ba-73b91a50c28b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run startup script to get password\n",
    "%run -i ../startup.ipynb\n",
    "\n",
    "try:\n",
    "    # Create database connection context\n",
    "    eng = create_context(host='host.docker.internal', username='demo_user', password=password)\n",
    "    print(eng)\n",
    "    \n",
    "    # Set query band for session tracking\n",
    "    execute_sql('''SET query_band='DEMO=Teradata_Enterprise_VectorStore_VectorizingPDFs_GenAI_Python.ipynb;' UPDATE FOR SESSION;''')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Vantage: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f3fe2-ed63-4838-bb19-4c0d0157453d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>3. Data Exploration Getting Data for This Demo</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The Chat with documentation demo aims to demonstrate how users can interact with documents such as insurance policy wordings, invoices, and other similar documents through a conversational interface. Additionally in this demo, we have added Audio and text files as well to extract transcripts from audio and make it conversational.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b94e0-bb25-46a2-a6a9-5eedaa1d39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demo data (takes approximately 2 minutes)\n",
    "%run -i ../run_procedure.py \"call get_data('DEMO_ComplaintAnalysis_local');\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a762f2e5-fd9a-4306-8dd9-f635996a7ddb",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Optional step – We should execute the below step only if we want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ba64d-11f6-4fb5-a52f-51cb6f379955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: View space report for databases and tables\n",
    "%run -i ../run_procedure.py \"call space_report();\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20a791-cc79-45b6-bce7-56cb26c37ac7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We have a Customer 360 details table containing all the customers' personal and banking-related information. We will use this table to ask questions in natural language and retrieve answers from the Vantage Database.<p/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89760921-033b-4c67-bee4-68b85da3b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Customer 360 details table\n",
    "complaints_data = DataFrame(in_schema(\"DEMO_ComplaintAnalysis\", \"Customer_360_Details\"))\n",
    "complaints_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1626b5-1693-4683-a275-5aa80c862f8d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<a id='section4'></a>\n",
    "<b style = 'font-size:20px;font-family:Arial'>4. Read source data. </b>\n",
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.1 Run the data loader </b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfd968-aaac-45cd-8f3c-35fac1218981",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The Traveller Easy Single Trip - International insurance policy is a comprehensive travel insurance plan that provides cover for a wide range of risks, including medical expenses, trip cancellation, loss of luggage, and personal accident. The policy is designed to be affordable and flexible, and it can be purchased online or over the phone.<p/>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The source data from <a href=\"https://axa-com-my.cdn.axa-contento-118412.eu/axa-com-my/3d2f84a5-42b9-459b-911a-710546df0633_Policy+wording+-+SmartTraveller+Easy+Single+Trip+-+International+%280820%29.pdf\">AXA</a> is loaded in Teradata Vantage as Vector Database.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Now, let's use <code>PyMuPDFLoader</code> library to read the pdf document and split it into pages.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>For Audio files <code>openai/whisper-small</code> open source audio model we have used to extract the transcripts and split it into pages.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2386198-83cb-4c17-9eb2-597807b58720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_id(df, id_column=\"id\", start_id=1000):\n",
    "    \"\"\"\n",
    "    Get the next available ID for a new record.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing existing records\n",
    "        id_column: Name of the ID column\n",
    "        start_id: Starting ID value if DataFrame is empty\n",
    "\n",
    "    Returns:\n",
    "        Next available ID value\n",
    "    \"\"\"\n",
    "    if df.empty or df[id_column].max() < start_id:\n",
    "        return start_id\n",
    "    else:\n",
    "        return df[id_column].max() + 1\n",
    "\n",
    "\n",
    "def get_splitter():\n",
    "    \"\"\"\n",
    "    Create a text splitter for chunking documents.\n",
    "\n",
    "    Returns:\n",
    "        RecursiveCharacterTextSplitter configured for optimal chunk size\n",
    "    \"\"\"\n",
    "    return RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=30,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def read_document_content(raw_data_df, pages, file_name):\n",
    "    \"\"\"\n",
    "    Process document pages and convert to chunked text with metadata.\n",
    "\n",
    "    Args:\n",
    "        raw_data_df: Existing DataFrame with processed documents\n",
    "        pages: List of document pages to process\n",
    "        file_name: Source file name for tracking\n",
    "\n",
    "    Returns:\n",
    "        Updated DataFrame with new document chunks\n",
    "    \"\"\"\n",
    "    # Extract page content and split into chunks\n",
    "    docs = [p.page_content for p in pages]\n",
    "    docs = get_splitter().create_documents(docs)\n",
    "\n",
    "    # Convert chunks to text list\n",
    "    texts_data = [chunk.page_content for chunk in docs]\n",
    "\n",
    "    # Create DataFrame with text chunks\n",
    "    temp_df = pd.DataFrame(data=texts_data, columns=[\"txt\"])\n",
    "    next_id = get_next_id(raw_data_df)\n",
    "    temp_df[\"id\"] = range(next_id, len(temp_df.index) + next_id)\n",
    "    temp_df[\"txt\"] = texts_data\n",
    "    temp_df[\"file_name\"] = file_name\n",
    "\n",
    "    # Concatenate with existing data\n",
    "    return pd.concat([raw_data_df, temp_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658e404-6618-4379-b915-3f8e1b2b04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_files(directory_path):\n",
    "    \"\"\"\n",
    "    Read and process multiple file types (PDF, MP3, TXT) from a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path: Path to directory containing source files\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing all processed document chunks with metadata\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\", \"Due to a bug fix in https://github.com/huggingface/transformers/pull\"\n",
    "    )\n",
    "\n",
    "    # Define schema for output DataFrame\n",
    "    columns = {\n",
    "        \"id\": \"int64\",\n",
    "        \"txt\": \"object\",\n",
    "        \"file_name\": \"object\",\n",
    "    }\n",
    "\n",
    "    # Create loading spinner widget\n",
    "    loading_spinner = widgets.HTML(\n",
    "        value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Reading the raw data...\",\n",
    "    )\n",
    "\n",
    "    # Initialize empty DataFrame\n",
    "    raw_data_df = pd.DataFrame(\n",
    "        {col: pd.Series(dtype=dt) for col, dt in columns.items()}\n",
    "    )\n",
    "\n",
    "    # Walk through directory and process files\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        display(loading_spinner)\n",
    "\n",
    "        # Skip checkpoint directories\n",
    "        if \".ipynb_checkpoints\" in root:\n",
    "            continue\n",
    "\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "\n",
    "            try:\n",
    "                # Process MP3 audio files\n",
    "                if file_name.lower().endswith(\".mp3\"):\n",
    "                    print(f\"MP3 File: {file_name}\")\n",
    "                    transcripts = process_audio(file_path)\n",
    "                    texts = get_splitter().create_documents([transcripts])\n",
    "                    raw_data_df = read_document_content(raw_data_df, texts, file_name)\n",
    "\n",
    "                # Process PDF files\n",
    "                elif file_name.lower().endswith(\".pdf\"):\n",
    "                    print(f\"PDF File: {file_name}\")\n",
    "                    pages = PyMuPDFLoader(file_path).load_and_split()\n",
    "                    print(f\"Total pages: {len(pages)}\")\n",
    "                    raw_data_df = read_document_content(raw_data_df, pages, file_name)\n",
    "\n",
    "                # Process text files\n",
    "                elif file_name.lower().endswith(\".txt\"):\n",
    "                    print(f\"TXT File: {file_name}\")\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        text_content = file.read()\n",
    "                        texts = get_splitter().create_documents([text_content])\n",
    "                        raw_data_df = read_document_content(\n",
    "                            raw_data_df, texts, file_name\n",
    "                        )\n",
    "\n",
    "                else:\n",
    "                    print(f\"Skipping: {file_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(\"*\" * 70)\n",
    "    print(\"All source files have been read, and chunking has been completed.\")\n",
    "    print(\"*\" * 70)\n",
    "\n",
    "    # Hide the loading spinner\n",
    "    loading_spinner.value = \"\"\n",
    "\n",
    "    return raw_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0aa71d-457d-448a-ad11-da18ed77dd94",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>In the above cell, we will read all the pages of the PDF file and split them into pages. To process further, we will save documents to Vantage.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cac2b5-9e86-427a-9579-cbccebb1c2bd",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>5. Load HuggingFace Model</b>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>To generate embeddings, we need an ONNX model capable of transforming text into vector representations. We use a pretrained model from [Teradata's Hugging Face repository](https://huggingface.co/Teradata/gte-base-en-v1.5), such as gte-base-en-v1.5. The model and its tokenizer are downloaded and stored in Vantage tables as BLOBs using the save_byom function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1460110-5323-406b-b468-749449243fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Configure Hugging Face environment\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"bge-base-en-v1.5\"\n",
    "number_dimensions_output = 768\n",
    "model_file_name = \"model.onnx\"\n",
    "\n",
    "try:\n",
    "    # Download embedding model and tokenizer from Hugging Face\n",
    "    hf_hub_download(\n",
    "        repo_id=f\"Teradata/{model_name}\",\n",
    "        filename=f\"onnx/{model_file_name}\",\n",
    "        local_dir=\"./\",\n",
    "    )\n",
    "    hf_hub_download(\n",
    "        repo_id=f\"Teradata/{model_name}\", filename=\"tokenizer.json\", local_dir=\"./\"\n",
    "    )\n",
    "    print(f\"Successfully downloaded {model_name} model and tokenizer\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading model files: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823b1e2a-1c34-4e45-bd19-6e7d4057c6ab",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.1 Save the Model</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In above steps, we have checked that the model is working fine in ONNX format. Now we will save the model file.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3d2ff-3497-4a2c-80f6-35ba3d9a6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up existing model tables if they exist\n",
    "try:\n",
    "    db_drop_table(\"embeddings_models\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    db_drop_table(\"embeddings_tokenizers\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af356f-3e78-4af5-9b59-c5e66833e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Save embedding model to Vantage\n",
    "    save_byom(\n",
    "        model_id=model_name,  # Must be unique in the models table\n",
    "        model_file=f\"onnx/{model_file_name}\",\n",
    "        table_name=\"embeddings_models\",\n",
    "    )\n",
    "    print(f\"Successfully saved embedding model: {model_name}\")\n",
    "\n",
    "    # Save tokenizer to Vantage\n",
    "    save_byom(\n",
    "        model_id=model_name,  # Must be unique in the models table\n",
    "        model_file=\"tokenizer.json\",\n",
    "        table_name=\"embeddings_tokenizers\",\n",
    "    )\n",
    "    print(f\"Successfully saved tokenizer: {model_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model to Vantage: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f5eb7-e703-473b-bc39-58c1b355f8ca",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Recheck the installed model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dde44f-1aca-4f1e-be90-4c7a8703b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify embedding models saved successfully\n",
    "df_model = DataFrame(\"embeddings_models\")\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827989c-736f-462a-864d-763679415240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify tokenizer saved successfully\n",
    "df_token = DataFrame(\"embeddings_tokenizers\")\n",
    "df_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2501ce-3fe6-40d5-9f86-f696d51d033c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Load the mode that we have save to DB in previous notebook by passing Model ID.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc277932-60cf-4f14-92db-546f33c2dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and tokenizer from database\n",
    "my_model = DataFrame.from_query(\n",
    "    f\"SELECT * FROM embeddings_models WHERE model_id = '{model_name}'\"\n",
    ")\n",
    "my_tokenizer = DataFrame.from_query(\n",
    "    f\"SELECT model AS tokenizer FROM embeddings_tokenizers WHERE model_id = '{model_name}'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a95d65-8866-4423-9a52-2dec04c63ab4",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>6. Generate embeddings from the chunks.</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will create prompts for different questions that can be answered from the document. Below are some sample questions that can be asked.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46431bb8-5524-42cf-bc36-62863277b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure BYOM installation locations\n",
    "configure.val_install_location = \"val\"\n",
    "configure.byom_install_location = \"mldb\"\n",
    "\n",
    "\n",
    "def generate_embeddings_data(input_tdf, cols_to_preserve):\n",
    "    \"\"\"\n",
    "    Generate embeddings for text data using ONNX model.\n",
    "\n",
    "    Args:\n",
    "        input_tdf: Input DataFrame containing text data\n",
    "        cols_to_preserve: List of column names to preserve in output\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing embeddings and preserved columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ONNXEmbeddings(\n",
    "            newdata=input_tdf,\n",
    "            modeldata=my_model,\n",
    "            tokenizerdata=my_tokenizer,\n",
    "            accumulate=cols_to_preserve,\n",
    "            model_output_tensor=\"sentence_embedding\",\n",
    "            output_format=f\"FLOAT32({number_dimensions_output})\",\n",
    "            enable_memory_check=False,\n",
    "        ).result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embeddings: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d565b-5898-4859-b28e-0a2779c2cee9",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>6.1 Do you want to generate the embeddings?</b></p>    \n",
    "<p style = 'font-size:16px;font-family:Arial'>Generating embeddings will take around <b>35-40 minutes.</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have already generated embeddings for the pdf and stored them in <b>Vantage</b> table.</p>\n",
    " \n",
    "<center><img src=\"images/decision_emb_gen_2.svg\" alt=\"embeddings_decision\"  width=300 height=400/></center>\n",
    " \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><i><b>Note: If you would like to skip the embedding generation step to save the time and move quickly to next step, please enter \"No\" in the next prompt.</b></i></p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i><b>Note:</b>If you choose <b>\"yes\"</b> to run the embeddings generation step, you must first execute the <a href=\"./Initialization_and_Model_Load.ipynb\">Initialization_and_Model_Load.ipynb</a> file to install the ONNX model on the ClearScape machine.</i></p>\n",
    "</div>\n",
    "\n",
    " \n",
    "<p style = 'font-size:16px;font-family:Arial'>To save time, you can move to the already generated embeddings section. However, if you would like to see how we generate the embeddings, or if you need to generate the embeddings for a different dataset, then continue to the following section.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a6b6a-24a2-45d2-b0e1-269bc6d29b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emb():\n",
    "    \"\"\"\n",
    "    Generate embeddings from source documents (PDF, text, audio).\n",
    "\n",
    "    This function:\n",
    "    1. Reads raw data from various file formats\n",
    "    2. Saves raw data to SQL table\n",
    "    3. Generates embeddings for all chunks\n",
    "    4. Saves embeddings to database\n",
    "    \"\"\"\n",
    "    display(loading_spinner)\n",
    "\n",
    "    try:\n",
    "        print(\"*\" * 50)\n",
    "        print(\"Step 1: Reading raw data from PDF, TXT and audio files...\")\n",
    "        print(\"*\" * 50)\n",
    "\n",
    "        directory_path = \"./data\"\n",
    "        final_raw_data_df = read_data_files(directory_path)\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        print(\"Step 2: Saving raw data to SQL...\")\n",
    "        print(\"*\" * 50)\n",
    "\n",
    "        # Copy documents to Vantage\n",
    "        copy_to_sql(\n",
    "            final_raw_data_df,\n",
    "            table_name=\"docs_data\",\n",
    "            primary_index=\"id\",\n",
    "            if_exists=\"replace\",\n",
    "        )\n",
    "\n",
    "        tdf_docs = DataFrame(\"docs_data\")\n",
    "        print(f\"Data information: \\n{tdf_docs.shape}\")\n",
    "        tdf_docs.sort(\"id\")\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        print(\"Step 3: Generating embeddings...\")\n",
    "        print(\"*\" * 50)\n",
    "        display(loading_spinner)\n",
    "        display(Markdown(get_section5_desc_start(tdf_docs)))\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Generate embeddings with preserved columns\n",
    "        cols_to_preserve = [\"id\", \"txt\", \"file_name\"]\n",
    "        docs_data = DataFrame(\"docs_data\")\n",
    "        df_embeddings = generate_embeddings_data(docs_data, cols_to_preserve)\n",
    "\n",
    "        # Save embeddings to database\n",
    "        copy_to_sql(\n",
    "            df_embeddings,\n",
    "            table_name=\"pdf_embeddings_store\",\n",
    "            if_exists=\"replace\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start\n",
    "        print(f\"Embeddings generated successfully in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in embedding generation process: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb90189-e906-4e40-b3fd-79fd45faccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_emb():\n",
    "    \"\"\"\n",
    "    Load pre-generated embeddings from local parquet files.\n",
    "\n",
    "    This function loads both raw data and embeddings from compressed\n",
    "    parquet files and saves them to the database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"*\" * 60)\n",
    "        print(\"Step 1: Loading raw data from parquet file stored locally.\")\n",
    "        print(\"*\" * 60)\n",
    "\n",
    "        # Load raw data from parquet\n",
    "        raw_data_prq = pd.read_parquet(\"./embeddings/all_source_data_v1.parquet.gzip\")\n",
    "\n",
    "        # Save raw data to database\n",
    "        delete_and_copy_embeddings(\n",
    "            table_name=\"docs_data\",\n",
    "            tdf=raw_data_prq,\n",
    "            eng=eng,\n",
    "        )\n",
    "\n",
    "        print(\"*\" * 60)\n",
    "        print(\"Step 2: Loading embeddings from parquet file stored locally.\")\n",
    "        print(\"*\" * 60)\n",
    "\n",
    "        # Load embeddings from parquet\n",
    "        embeddings_prq = pd.read_parquet(\"./embeddings/all_embeddings_v3.parquet.gzip\")\n",
    "\n",
    "        # Save embeddings to database\n",
    "        delete_and_copy_embeddings(\n",
    "            table_name=\"pdf_embeddings_store\",\n",
    "            tdf=embeddings_prq,\n",
    "            eng=eng,\n",
    "        )\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        print(\"Embeddings loaded and saved successfully!\")\n",
    "        print(\"*\" * 50)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Parquet files not found. Please check file paths: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embeddings from parquet: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb446a-daac-4bc7-81f5-7a8559da2f95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create loading spinner widget\n",
    "loading_spinner = widgets.HTML(\n",
    "    value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Generating embeddings for documents...\",\n",
    ")\n",
    "\n",
    "\n",
    "def get_section5_desc_start(tdf):\n",
    "    \"\"\"\n",
    "    Generate informational message about embedding generation time.\n",
    "\n",
    "    Args:\n",
    "        tdf: DataFrame containing documents to process\n",
    "\n",
    "    Returns:\n",
    "        HTML formatted information message\n",
    "    \"\"\"\n",
    "    return f\"\"\"<div class=\"alert alert-info\">\n",
    "    <p style='font-size:16px;font-family:Arial'><i><b>Please be patient:</b> Generating embeddings for {tdf.shape[0]} document contents may take up to 35 to 40 minutes depending on the number of AMPs in the database. Since the volume of data is large and the machine is small, going through the below code could take up to 40 minutes.</i></p>\n",
    "</div>\"\"\"\n",
    "\n",
    "\n",
    "# Prompt user for their preference\n",
    "generate = input(\"Do you want to generate embeddings? ('yes'/'no'): \").strip().lower()\n",
    "\n",
    "try:\n",
    "    if generate == \"yes\":\n",
    "        generate_emb()\n",
    "    elif generate == \"no\":\n",
    "        load_data_emb()\n",
    "    else:\n",
    "        print(\"\\nInvalid input. Please enter 'yes' or 'no' to proceed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Hide loading spinner\n",
    "    loading_spinner.value = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae7c12f-d304-4b4d-b497-1c3319f09bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display embeddings table\n",
    "tdf_embeddings_store = DataFrame(\"pdf_embeddings_store\")\n",
    "tdf_embeddings_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508be45c-ebdf-4292-badf-72507128c3d1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Let's view the shape of embeddings table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf215b71-2758-406a-b962-b1bf5651ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display shape of embeddings table (rows, columns)\n",
    "tdf_embeddings_store.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb61754-527e-467b-86f1-4ec6f98e0d45",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>7. Insert Prompts into a Table</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0664918-7abc-4c30-8fc3-7af415475475",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will create the required table and than we will insert different values for the prompts.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d9aac-60e9-4422-aafb-205e3d00bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_to_ask_table():\n",
    "    \"\"\"\n",
    "    Create table to store questions for query generation.\n",
    "\n",
    "    Drops existing table if present and creates new one.\n",
    "    \"\"\"\n",
    "    qry = \"\"\"CREATE MULTISET TABLE question_to_ask(\n",
    "        txt VARCHAR(1024) CHARACTER SET UNICODE NOT CASESPECIFIC,\n",
    "        query_id INT\n",
    "    ) NO PRIMARY INDEX\"\"\"\n",
    "\n",
    "    try:\n",
    "        execute_sql(qry)\n",
    "        print(\"Table 'question_to_ask' created successfully\")\n",
    "    except Exception:\n",
    "        # Drop and recreate if table already exists\n",
    "        db_drop_table(\"question_to_ask\")\n",
    "        execute_sql(qry)\n",
    "        print(\"Table 'question_to_ask' recreated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b19d1-80cc-4397-aee5-d43fb1fa5656",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will create prompts for different questions that can be answered from the document. Below are some sample questions that can be asked.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761d88d-813f-40ae-b8b2-a9500bbb04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_questions():\n",
    "    \"\"\"\n",
    "    Insert sample questions into the question_to_ask table.\n",
    "\n",
    "    These questions cover both structured data queries and document-based questions.\n",
    "    \"\"\"\n",
    "    prompts = [\n",
    "    \"Does this policy cover  Loss of or Damage to the Insured’s Articles?\",\n",
    "    \"What is the reimbursement limit per Baggage?\",\n",
    "    \"What is the sum insured amount in the case Accidental Death in domestic and international for adult as well as child?\",\n",
    "    \"What documents are required for Rental Car Excess?\",\n",
    "    \"Where can I submit my complaints or feedback?\",\n",
    "    \"What is the bank tenure of customer 789456123?\",\n",
    "    \"What is the Total credit balance of customer 456789123?\",\n",
    "    \"How many customers have only Credit Card as product holdings?\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        for idx, prompt in enumerate(prompts, start=100):\n",
    "            execute_sql(f\"\"\"INSERT into question_to_ask values ('{prompt}', {idx});\"\"\")\n",
    "        # for idx, prompt in enumerate(prompts, start=100):\n",
    "        #     execute_sql(f\"\"\"INSERT INTO question_to_ask VALUES ('{prompt}', {idx});\"\"\")\n",
    "        print(f\"Successfully inserted {len(prompts)} questions\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting questions: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244cf33-0a9c-44a2-87cb-3599897fa484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_que_emb(table_name):\n",
    "    \"\"\"\n",
    "    Load pre-generated question embeddings from parquet file to database.\n",
    "\n",
    "    Args:\n",
    "        table_name: Name of the target table for embeddings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"*\" * 50)\n",
    "        print(\"Loading question embeddings from parquet file stored locally.\")\n",
    "        print(\"*\" * 50)\n",
    "\n",
    "        # Load embeddings from parquet\n",
    "        embeddings_prq = pd.read_parquet(\n",
    "            \"./embeddings/questions_embeddings.parquet.gzip\"\n",
    "        )\n",
    "\n",
    "        # Save to database\n",
    "        copy_to_sql(\n",
    "            embeddings_prq,\n",
    "            table_name=table_name,\n",
    "            primary_index=\"query_id\",\n",
    "            if_exists=\"replace\",\n",
    "        )\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        print(\"Question embeddings loaded and saved successfully!\")\n",
    "        print(\"*\" * 50)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Question embeddings parquet file not found: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading question embeddings: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd2227-0019-4ff4-b9f0-d62e7e1e1086",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>8. Generate Embeddings from the Prompts</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will create embeddings for the prompts which we have inserted into the table above.</p>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i><b>Note:</b>If you choose <b>\"yes\"</b> to run the embeddings generation step, you must first execute the <a href=\"./Initialization_and_Model_Load.ipynb\">Initialization_and_Model_Load.ipynb</a> file to install the ONNX model on the ClearScape machine.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e0de0-62f0-41b5-90ca-36d60d880f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loading spinner\n",
    "loading_spinner = widgets.HTML(\n",
    "    value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Generating embeddings for documents...\",\n",
    ")\n",
    "\n",
    "# Create question table and add sample questions\n",
    "create_question_to_ask_table()\n",
    "add_questions()\n",
    "\n",
    "# Request user input\n",
    "generate = input(\"Do you want to generate embeddings? ('yes'/'no'): \").strip().lower()\n",
    "\n",
    "try:\n",
    "    if generate == \"yes\":\n",
    "        # Generate embeddings for questions\n",
    "        loading_spinner = widgets.HTML(\n",
    "            value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Generating embeddings for questions...\",\n",
    "        )\n",
    "\n",
    "        display(loading_spinner)\n",
    "\n",
    "        # Define columns to preserve in output\n",
    "        cols_to_preserve = [\"query_id\", \"txt\"]\n",
    "        question_to_ask = DataFrame(\"question_to_ask\")\n",
    "        df_embeddings_que = generate_embeddings_data(question_to_ask, cols_to_preserve)\n",
    "\n",
    "        # Save to database\n",
    "        copy_to_sql(\n",
    "            df_embeddings_que,\n",
    "            table_name=\"question_to_ask_embeddings\",\n",
    "            if_exists=\"replace\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "        loading_spinner.value = \"\"\n",
    "        print(\"Question embeddings generated successfully\")\n",
    "\n",
    "    elif generate == \"no\":\n",
    "        load_que_emb(table_name=\"question_to_ask_embeddings\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nInvalid input. Please enter 'yes' or 'no' to proceed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in question embedding process: {e}\")\n",
    "    loading_spinner.value = \"\"\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026aa09-8275-44c7-ad55-8a9b4040631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display question embeddings table\n",
    "tdf_question_embeddings_store = DataFrame(\"question_to_ask_embeddings\")\n",
    "tdf_question_embeddings_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd839251-4c35-48e7-9427-712680348ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display shape of question embeddings table\n",
    "tdf_question_embeddings_store.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b28b45-680a-4dd9-8113-15092bf01254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save question embeddings to parquet file for future use\n",
    "# df = tdf_question_embeddings_store.to_pandas().reset_index()\n",
    "# df.drop(\"index\", axis=1, inplace=True)\n",
    "# df.to_parquet('./embeddings/questions_embeddings.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098357a-9a91-4df6-adef-b8fc79003a64",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>9. Find top 10 matching chunks</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will find the top 10 chunks that match the queries using the <b>TD_VectorDistance</b>. The TD_VectorDistance function accepts a table of target vectors and a table of reference vectors and returns a table that contains the distance between target-reference pairs. The function computes the distance between the target pair and the reference pair from the same table. We must have the same column order in the TargetFeatureColumns argument and the RefFeatureColumns argument. The function ignores the feature values during distance computation if the value is either NULL, NAN, or INF.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe2b4f-2fce-4a28-9352-00ee495f6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vector_distance(target_table, reference_table, emb_column_names, topk):\n",
    "    \"\"\"\n",
    "    Calculate cosine distance between target and reference vectors.\n",
    "\n",
    "    Args:\n",
    "        target_table: DataFrame containing target (query) vectors\n",
    "        reference_table: DataFrame containing reference (document) vectors\n",
    "        emb_column_names: List of embedding column names\n",
    "        topk: Number of top matching results to return\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing top-k matches with distances\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        VectorDistance_out = VectorDistance(\n",
    "            target_id_column=\"query_id\",\n",
    "            target_feature_columns=emb_column_names,\n",
    "            ref_id_column=\"id\",\n",
    "            ref_feature_columns=emb_column_names,\n",
    "            distance_measure=[\"Cosine\"],\n",
    "            topk=topk,\n",
    "            target_data=target_table,\n",
    "            reference_data=reference_table,\n",
    "        )\n",
    "\n",
    "        elapsed_time = timeit.default_timer() - start\n",
    "        print(f\"Vector distance calculation time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "        return VectorDistance_out.result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating vector distance: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb1eeb-262b-48d4-9bb9-88a81549da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding column names (exclude id and text columns)\n",
    "emb_column_names = DataFrame(\"question_to_ask_embeddings\").columns[2:]\n",
    "\n",
    "# Define number of top matches to retrieve\n",
    "number_of_recommendations = 10\n",
    "\n",
    "# Calculate vector distances between questions and document chunks\n",
    "vector_distance_df = calculate_vector_distance(\n",
    "    target_table=tdf_question_embeddings_store,\n",
    "    reference_table=tdf_embeddings_store,\n",
    "    emb_column_names=emb_column_names,\n",
    "    topk=number_of_recommendations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7757e751-fba5-45c0-8b41-845bd0662fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_matching_chunks(\n",
    "    vector_distance_df, tdf_embeddings_store, tdf_question_embeddings_store\n",
    "):\n",
    "    \"\"\"\n",
    "    Join vector distances with original text and metadata.\n",
    "\n",
    "    Args:\n",
    "        vector_distance_df: DataFrame with vector distance results\n",
    "        tdf_embeddings_store: DataFrame with document embeddings\n",
    "        tdf_question_embeddings_store: DataFrame with question embeddings\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with matched chunks including text, metadata, and distances\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Select relevant columns from embeddings table\n",
    "        embeddings_df_selected_columns = tdf_embeddings_store.select(\n",
    "            [\"id\", \"txt\", \"file_name\"]\n",
    "        )\n",
    "\n",
    "        # Join vector-distance results with document text\n",
    "        vec_prod_join_result = vector_distance_df.merge(\n",
    "            right=embeddings_df_selected_columns,\n",
    "            left_on=\"reference_id\",\n",
    "            right_on=\"id\",\n",
    "            lsuffix=\"t1\",\n",
    "            rsuffix=\"t2\",\n",
    "        )\n",
    "\n",
    "        # Select columns for final output\n",
    "        vec_prod_join_result_selected = vec_prod_join_result[\n",
    "            [\"id\", \"txt\", \"file_name\", \"target_id\", \"distancetype\", \"distance\"]\n",
    "        ]\n",
    "\n",
    "        # Get question text\n",
    "        df_que_selected = tdf_question_embeddings_store.select([\"query_id\", \"txt\"])\n",
    "\n",
    "        # Join with question text to get complete results\n",
    "        df_matched_chunks = df_que_selected.merge(\n",
    "            right=vec_prod_join_result_selected,\n",
    "            left_on=\"query_id\",\n",
    "            right_on=\"target_id\",\n",
    "            how=\"inner\",\n",
    "            lsuffix=\"que\",\n",
    "            rsuffix=\"matched\",\n",
    "        )\n",
    "\n",
    "        # Filter out low-quality matches (distance threshold)\n",
    "        df_matched_chunks = df_matched_chunks[df_matched_chunks.distance > 0.001]\n",
    "\n",
    "        # Sort by query_id and distance (ascending = most similar first)\n",
    "        df_matched_chunks = df_matched_chunks.sort(\n",
    "            [\"query_id\", \"distance\"], ascending=True\n",
    "        )\n",
    "\n",
    "        return df_matched_chunks[\n",
    "            [\"query_id\", \"txt_que\", \"txt_matched\", \"id\", \"file_name\", \"distance\"]\n",
    "        ]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing matching chunks: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f697808-ba33-4d79-90f5-6e3acccb2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final matching chunks with text and metadata\n",
    "tdf_matching_chunks = get_final_matching_chunks(\n",
    "    vector_distance_df, tdf_embeddings_store, tdf_question_embeddings_store\n",
    ")\n",
    "\n",
    "# Copy results to SQL for improved performance\n",
    "copy_to_sql(tdf_matching_chunks, table_name=\"df_matching_chunks\", if_exists=\"replace\")\n",
    "\n",
    "# Reload from database\n",
    "tdf_matching_chunks = DataFrame(\"df_matching_chunks\")\n",
    "tdf_matching_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b04db-fe87-4ed2-8ca4-21c8311ce990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document data for reference\n",
    "tdf_docs = DataFrame(\"docs_data\")\n",
    "\n",
    "\n",
    "def get_similarity_search_context(target_id):\n",
    "    \"\"\"\n",
    "    Retrieve context, file names, and reference IDs for a given query.\n",
    "\n",
    "    Args:\n",
    "        target_id: Query ID to retrieve context for\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - context: Concatenated text from matching chunks\n",
    "        - file_names: List of unique source file names\n",
    "        - ref_ids: List of reference document IDs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get matching chunks for the target query\n",
    "        tmp1 = tdf_matching_chunks.loc[tdf_matching_chunks[\"query_id\"] == target_id][\n",
    "            [\"txt_matched\", \"query_id\", \"file_name\", \"id\"]\n",
    "        ]\n",
    "\n",
    "        # Extract unique file names from matching chunks\n",
    "        ref_ids = tmp1[[\"id\"]].get_values().flatten()\n",
    "        file_names = list(\n",
    "            set(\n",
    "                tdf_docs[tdf_docs.id.isin(list(ref_ids))][[\"file_name\"]]\n",
    "                .get_values()\n",
    "                .flatten()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Concatenate matched text as context\n",
    "        context = \"\\n\".join(tmp1[[\"txt_matched\"]].get_values().flatten())\n",
    "\n",
    "        return context, file_names, ref_ids\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving similarity search context: {e}\")\n",
    "        return \"\", [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853abd90-5f61-4f02-be41-bf1fef65b325",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>10. Configuring LiteLLM Proxy Environment</b>\n",
    "\n",
    "<p style=\"font-size:16px;font-family:Arial;color:#00233C\">\n",
    "The following cell configures the LiteLLM environment for this demo by enabling debug logging, loading credentials from a secure environment file, and routing all LLM requests through a centralized LiteLLM proxy.\n",
    "</p>\n",
    "\n",
    "<ol style=\"font-size:16px;font-family:Arial;color:#00233C\">\n",
    "  <li>\n",
    "    <b>LITELLM_LOG</b>: Enables detailed debug-level logging for request tracing and troubleshooting\n",
    "  </li>\n",
    "  <li>\n",
    "    <b>LITELLM_API_KEY</b>: Loads the LiteLLM API key securely from the environment file\n",
    "  </li>\n",
    "  <li>\n",
    "    <b>LITELLM_BASE_URL</b>: Defines the LiteLLM proxy endpoint through which all model requests are routed\n",
    "  </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd085be-9012-43e8-9a02-5975d32d5c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LiteLLM environment\n",
    "os.environ[\"LITELLM_LOG\"] = \"DEBUG\"\n",
    "\n",
    "try:\n",
    "    # Load environment variables from config file\n",
    "    load_dotenv(\"/home/jovyan/JupyterLabRoot/VantageCloud_Lake/.config/.env\")\n",
    "\n",
    "    # Set LiteLLM API key\n",
    "    os.environ[\"LITELLM_API_KEY\"] = os.getenv(\"litellm_key\")\n",
    "    litellm_key = os.environ[\"LITELLM_API_KEY\"]\n",
    "\n",
    "    # Set LiteLLM proxy base URL\n",
    "    os.environ[\"LITELLM_BASE_URL\"] = os.getenv(\"litellm_base_url\")\n",
    "    base_url = os.environ[\"LITELLM_BASE_URL\"]\n",
    "\n",
    "    if not litellm_key or not base_url:\n",
    "        raise ValueError(\"LiteLLM API key or base URL not found in environment\")\n",
    "\n",
    "    print(\"LiteLLM environment configured successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring LiteLLM environment: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f6369-811f-45a3-8bbb-28e853695e7c",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.1 Connect to databases using SQL Alchemy</b></p>    \n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Under the hood, we use SQLAlchemy to connect to SQL databases. This means that the SQLDatabaseChain can be used with any SQL dialect supported by SQLAlchemy, such as Teradata Vantage, MS SQL, MySQL, MariaDB, PostgreSQL, Oracle SQL, and SQLite. For more information about the requirements for connecting to our database, we recommend referring to the <a href=\"https://docs.sqlalchemy.org/en/20/\">SQLAlchemy documentation</a>.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Important: The code below establishes a database connection for our data sources and Large Language Models. Please note that the solution will only work if we define the database connection for our sources in the cell below.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>We build a consolidated view of the Table Data Catalog by combining metadata stored for the database and table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5842c-e7f1-4333-820a-2c7430110276",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create Vantage SQLAlchemy engine\n",
    "    database = \"DEMO_ComplaintAnalysis_db\"\n",
    "    db = SQLDatabase(\n",
    "        eng,\n",
    "        schema=database,\n",
    "        include_tables=[\"Customer_360_Details\"],\n",
    "    )\n",
    "\n",
    "    print(f\"Database dialect: {db.dialect}\")\n",
    "    print(f\"Available tables: {db.get_usable_table_names()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating SQL database connection: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b7e93-a952-4794-9e78-e292c54eafd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table schema mapping\n",
    "main_d = {\n",
    "    \"Customer_360_Details\": complaints_data.columns,\n",
    "}\n",
    "\n",
    "\n",
    "def get_db_schema():\n",
    "    \"\"\"\n",
    "    Generate database schema string for LLM prompts.\n",
    "\n",
    "    Returns:\n",
    "        Formatted string containing table names and column names\n",
    "    \"\"\"\n",
    "    table_dicts = []\n",
    "    for k in main_d:\n",
    "        table_dicts.append(\n",
    "            {\n",
    "                \"table_name\": k,\n",
    "                \"column_names\": main_d[k],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    database_schema_string = \"\\n\".join(\n",
    "        [\n",
    "            f\"Table: {table['table_name']}\\nColumns: {', '.join(table['column_names'])}\"\n",
    "            for table in table_dicts\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return database_schema_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81a2fc-990e-4f44-be64-f6a198f27329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display database schema\n",
    "database_schema = get_db_schema()\n",
    "print(database_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae78e9-76ce-42ce-9c89-722a372ef3e0",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.2 Define LLM model</b></p>  \n",
    "\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "    Define the LLM using the <code>ChatBedrockConverse</code> interface. When defining <code>ChatBedrockConverse</code>, set the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns\">Amazon Bedrock base model ID</a>, the client as <code>boto3_bedrock</code>, and the common inference parameters.\n",
    "</p> \n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "    We use the optional parameter <b>temperature</b> to make our Teradata SQL outputs more predictable.\n",
    "</p>\n",
    "\n",
    "<div style=\"margin-left: 16px; font-size: 16px; font-family: Arial;\">\n",
    "    <b>- Temperature:</b> which can range from 0.0 to 2 and controls how creative our results will be, Setting it to 0.1 ensures the model favors higher-probability (more predictable) words, resulting in more consistent and less varied outputs.<br>\n",
    "</div>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "    For a complete list of optional parameters for base models provided by Amazon Bedrock, visit the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\"> AWS docs</a>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f793607-bbf5-4599-81b0-9b16b6d3698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "try:\n",
    "    # Initialize ChatOpenAI LLM with LiteLLM proxy\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_base=base_url,  # LiteLLM Proxy URL\n",
    "        model=\"AWS-Bedrock-anthropic.claude-opus-4-1-20250805-v1:0\",\n",
    "        temperature=0.1,  # Low temperature for more deterministic SQL generation\n",
    "        api_key=litellm_key,\n",
    "    )\n",
    "    print(\"LLM initialized successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing LLM: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb82f7-ea5c-48c5-a91d-c4e090c3ce7e",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.3 Define SQL Agent</b></p>  \n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "    With the connection to Teradata Vantage established and our database (<code>db</code>) and Large Language Model (<code>LLM</code>) defined, we are ready to create and invoke our SQL Agent using the <code>create_sql_agent()</code> function. \n",
    "    </p>\n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "    We pass in our <code>llm</code> and <code>db</code> as required parameters and set <code>agent_type</code> to \"zero-shot-react-description\" to instruct the agent to perform a reasoning step before acting.  \n",
    "    </p>\n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "    We set <code>verbose</code> to true so that the agent can output detailed information of intermediate steps. Additionally, we set <code>handle_parsing_errors</code> to <code>True</code>, ensuring that errors are sent back to the LLM as observations, for the LLM to attempt handling the errors.\n",
    "    </p>\n",
    "    \n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    " We can optimize the agents performance with additional prompt engineering. \n",
    "</p>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "We import a <code>ChatPromptTemplate</code> class to build flexible reusable prompts in our agent. Here we define a prefix, format instructions, and a suffix and join them to create a custom prompt. The prefix has unique rules that apply to Teradata. The format guides it's Question, thought, observation behavior and the suffix cues it to begin. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f06e9-9fb2-492d-a679-c3081062d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom prompt template for Teradata SQL Agent\n",
    "prefix = (\n",
    "    \"\"\"You are a helpful and expert TeradataSQL database admin. TeradataSQL shares many similarities to SQL, with a few key differences.\n",
    "Given an input question, first create a syntactically correct TeradataSQL query to run, then look at the results of the query and return the answer.\n",
    "Given an input question, create a syntactically correct {dialect} query to run,\n",
    "then look at the results of the query and return the answer. Unless the user\n",
    "specifies a specific number of examples they wish to obtain, always limit your\n",
    "query to at most {top_k} results.\n",
    "\n",
    "IMPORTANT: Unless the user specifies an exact number of rows they wish to obtain, you must always limit your query to at most {top_k} results by using \"SELECT TOP {top_k}\".\n",
    "\n",
    "The following keywords do not exist in TeradataSQL: \n",
    "1. LIMIT \n",
    "2. FETCH\n",
    "3. FIRST\n",
    "Instead of LIMIT or FETCH, use the TOP keyword. The TOP keyword should immediately follow a \"SELECT\" statement.\n",
    "For example, to select the top 3 results, use \"SELECT TOP 3 FROM <table_name>\"\n",
    "Enclose all value identifiers in quotes to prevent errors from restricted keywords. Append an underscore to all alias keywords (e.g., AS count_).\n",
    "Always use double quotation marks (\" \") for column names in SQL queries to avoid syntax errors.\n",
    "Do NOT make any DML statements (INSERT, UPDATE, DELETE, DROP, etc.) to the database. \n",
    "If the question does not seem related to the database, just return \"I don't know\" as the answer\n",
    "\n",
    "IMPORTANT: Use default database as 'DEMO_ComplaintAnalysis_db'\n",
    "\n",
    "IMPORTANT: Use the following Tables: \\n\n",
    "\"\"\"\n",
    "    + database_schema\n",
    "    + \"\"\"\n",
    "\n",
    "Few examples of Question-SQL Pairs:\n",
    "Question: What is the Total credit balance of customer 456789123?\n",
    "SQL: SELECT TOP 1 \"Total Credit Balance\" FROM DEMO_ComplaintAnalysis_db.Customer_360_Details WHERE \"Customer Identifier\" = '456789123'\n",
    "\n",
    "IMPORTANT: Here are some tips for writing Teradata style queries:\n",
    "\n",
    "Always use table aliases when your SQL statement involves more than one source\n",
    "Aggregated fields like COUNT(*) must be appropriately named \n",
    "Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 3 results by using SELECT TOP 3, note that LIMIT function does not work in Teradata DB.\n",
    "[Best] If the question can be answered with the available tables: {{'sql': <sql here>}} \n",
    "If the question cannot be answered with the available tables: {{'error': <explanation here>}} \n",
    "Remove unnecessary ORDER BY clauses unless required. \n",
    "Remember: Do not use 'LIMIT' or 'FETCH' keyword in the SQLQuery, instead use TOP keyword. For Example: To select top 3 results, use TOP keyword instead of LIMIT or FETCH.  \n",
    "\\nResponse Guidelines: \n",
    "If the provided context is insufficient, please explain why it can't be generated. \n",
    "Most important: Always give property options with details like PropertyID, Property Type, Building Size, Price, Address, Bedroom Count. PropertyID is mandatory in the response.\n",
    "Critical Instruction: Ensure responses are exclusively derived from query results. Refrain from generating or adding synthetic data in any form.\n",
    "Most important: The function should return the relevant answer for the question asked only based on Query results.\n",
    "Given a user's question about this data, write a valid Teradata SQL query that accurately extracts or calculates the requested information from these tables \n",
    "and adheres to SQL best practices for Teradata database, optimizing for readability and performance where applicable. Do not try to make any answer\n",
    "\n",
    "You have access to the following tools:\"\"\"\n",
    ")\n",
    "\n",
    "format_instructions = \"\"\"You must always use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Don't forget to prefix your final answer with the string, \"Final Answer:\"!\"\"\"\n",
    "\n",
    "suffix = \"\"\"Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "# Create custom prompt template\n",
    "custom_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\\n\\n\".join(\n",
    "        [\n",
    "            prefix,\n",
    "            \"{tools}\",\n",
    "            format_instructions,\n",
    "            suffix,\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Create SQL Agent with custom prompt\n",
    "    agent = create_sql_agent(\n",
    "        llm=llm,\n",
    "        db=db,\n",
    "        agent_type=\"zero-shot-react-description\",\n",
    "        verbose=True,\n",
    "        handle_parsing_errors=True,\n",
    "        prompt=custom_prompt,\n",
    "        max_iterations=10,\n",
    "    )\n",
    "    print(\"SQL Agent created successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating SQL agent: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5546204-b5f3-430b-805d-c58b28962af7",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.4 Setup Hybrid RAG</b></p>  \n",
    "<p style=\"font-size: 16px; font-family: Arial;\">We have source data stored in both VectorDB and Vantage Database. Our hybrid RAG system is designed to automatically identify the appropriate source and query it accordingly. In some cases, responses to certain questions may be derived from both sources.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13949f98-c2c8-4a98-acb2-f51e7a8e6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_classifier(query):\n",
    "    \"\"\"\n",
    "    Create a chain to classify the query type.\n",
    "\n",
    "    Args:\n",
    "        query: User's question to classify\n",
    "\n",
    "    Returns:\n",
    "        LangChain chain for query classification\n",
    "    \"\"\"\n",
    "    query_classifier_prompt = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=\"\"\"Classify if the following query requires:\n",
    "            1. SQL database (if it's asking about structured data like Customer_360_Details, Customer Identifier, Name, City, State, Customer Type, Product Holdings, Total Deposit Balance, Total Credit Balance, Total Investments AUM, Customer Profitability,\n",
    "             Customer Lifetime Value, Bank Tenure, Affluence Segment, Digital Banking Segment, Branch Banking Segment.)\n",
    "            2. Vector database (if it's asking about document content, general knowledge, customer complaints)\n",
    "            3. Both (if it needs to combine information from structured data and documents)\n",
    "            \n",
    "            Query: {query}\n",
    "            \n",
    "            Return only one word: SQL, VECTOR, or BOTH\n",
    "            \"\"\",\n",
    "    )\n",
    "    chain = query_classifier_prompt | llm | StrOutputParser()\n",
    "    return chain\n",
    "\n",
    "\n",
    "def _query_vector_store(query_id, query):\n",
    "    \"\"\"\n",
    "    Query the vector store and return relevant content.\n",
    "\n",
    "    Args:\n",
    "        query_id: Unique identifier for the query\n",
    "        query: User's question text\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing response, references, and reference IDs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get similar document chunks from vector store\n",
    "        context, file_names, ref_ids = get_similarity_search_context(query_id)\n",
    "\n",
    "        # Create prompt for response generation\n",
    "        response_prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "            template=\"\"\"Using the following context, answer the question:\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {query}\n",
    "\n",
    "            Answer:\"\"\",\n",
    "        )\n",
    "\n",
    "        # Generate response using context\n",
    "        response_chain = response_prompt | llm | StrOutputParser()\n",
    "        response = response_chain.invoke({\"context\": context, \"query\": query})\n",
    "\n",
    "        return {\"response\": response, \"reference\": file_names, \"ref_ids\": ref_ids}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying vector store: {e}\")\n",
    "        return {\"response\": f\"Error: {e}\", \"reference\": [], \"ref_ids\": []}\n",
    "\n",
    "\n",
    "def _combine_responses(sql_response, vector_response, query):\n",
    "    \"\"\"\n",
    "    Combine responses from SQL and vector stores.\n",
    "\n",
    "    Args:\n",
    "        sql_response: Response from SQL database query\n",
    "        vector_response: Response from vector store query\n",
    "        query: Original user question\n",
    "\n",
    "    Returns:\n",
    "        Combined response string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        combination_prompt = PromptTemplate(\n",
    "            input_variables=[\"sql_response\", \"vector_response\", \"query\"],\n",
    "            template=\"\"\"Combine the following information to provide a complete answer:\n",
    "\n",
    "            SQL Database Info: {sql_response}\n",
    "            Document Info: {vector_response}\n",
    "            Original Question: {query}\n",
    "\n",
    "            Combined Answer:\"\"\",\n",
    "        )\n",
    "\n",
    "        combination_chain = combination_prompt | llm | StrOutputParser()\n",
    "        combined_response = combination_chain.invoke(\n",
    "            {\n",
    "                \"sql_response\": sql_response,\n",
    "                \"vector_response\": vector_response,\n",
    "                \"query\": query,\n",
    "            }\n",
    "        )\n",
    "        return combined_response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining responses: {e}\")\n",
    "        return f\"Error combining responses: {e}\"\n",
    "\n",
    "\n",
    "def process_query(query_id):\n",
    "    \"\"\"\n",
    "    Process user query and return appropriate response using hybrid RAG.\n",
    "\n",
    "    Args:\n",
    "        query_id: Query identifier or custom query string\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing response, references, and reference IDs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(query_id, str):\n",
    "            # Get query text from query_id\n",
    "            query = tdf_question_embeddings_store[\n",
    "                tdf_question_embeddings_store[\"query_id\"] == query_id\n",
    "            ][[\"txt\"]].get_values()[0][0]\n",
    "\n",
    "            # Classify query type\n",
    "            query_type = _get_classifier(query).invoke(query).strip().upper()\n",
    "\n",
    "            if query_type == \"SQL\":\n",
    "                # Query SQL database only\n",
    "                return {\n",
    "                    \"response\": agent.invoke(query)[\"output\"],\n",
    "                    \"reference\": [\"Customer_360_Details\"],\n",
    "                    \"ref_ids\": [],\n",
    "                }\n",
    "\n",
    "            elif query_type == \"VECTOR\":\n",
    "                # Query vector store only\n",
    "                return _query_vector_store(query_id, query)\n",
    "\n",
    "            elif query_type == \"BOTH\":\n",
    "                # Query both sources and combine\n",
    "                sql_response = agent.invoke(query)\n",
    "                vector_response = _query_vector_store(query_id, query)\n",
    "                return {\n",
    "                    \"response\": _combine_responses(\n",
    "                        sql_response, vector_response[\"response\"], query\n",
    "                    ),\n",
    "                    \"reference\": vector_response[\"reference\"],\n",
    "                    \"ref_ids\": vector_response[\"ref_ids\"],\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                return {\n",
    "                    \"response\": \"Unable to classify query type. Please rephrase your question.\",\n",
    "                    \"reference\": [],\n",
    "                    \"ref_ids\": [],\n",
    "                }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing query: {e}\")\n",
    "        return {\n",
    "            \"response\": f\"Error processing query: {e}\",\n",
    "            \"reference\": [],\n",
    "            \"ref_ids\": [],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8df1cb-0cf6-48ba-8a3e-94e5987b1fb3",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<a id=\"rule\"></a>\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>10. Test and Compare Results</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>To test and compare our results let's invoke the agent by selecting question from dropdown.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0138427-167c-42e4-8599-406833564f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_template(response):\n",
    "    \"\"\"\n",
    "    Generate HTML template for displaying query response.\n",
    "\n",
    "    Args:\n",
    "        response: Dictionary containing response text and references\n",
    "\n",
    "    Returns:\n",
    "        HTML formatted string for display\n",
    "    \"\"\"\n",
    "    view = \"\"\"<p style='font-size:18px;font-family:Arial;'><b>Here is your response:</b></p>\"\"\"\n",
    "    view += f\"\"\"<ul style='font-size:16px;font-family:Arial;'>\n",
    "    <li><strong>{response['response']}</strong><ul>\n",
    "    <li>References: \"\"\"\n",
    "\n",
    "    for ref in response[\"reference\"]:\n",
    "        view += f\"\"\"<ul style='font-size:16px;font-family:Arial;'><li>{ref}</li></ul>\"\"\"\n",
    "\n",
    "    view += \"\"\"</ul></ul>\"\"\"\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41024a54-5c9f-4c26-b147-03e9ab7aff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vectors_2d(question_vector, matching_chunks, doc_chunks, loading_spinner2):\n",
    "    \"\"\"\n",
    "    Create 2D visualization of question and document embeddings using PCA.\n",
    "\n",
    "    Args:\n",
    "        question_vector: Question embedding vector\n",
    "        matching_chunks: Embeddings of matching document chunks\n",
    "        doc_chunks: Embeddings of all document chunks\n",
    "        loading_spinner2: Loading spinner widget to hide after plotting\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Combine all vectors for PCA transformation\n",
    "        all_vectors = np.vstack(\n",
    "            [question_vector.reshape(1, -1), matching_chunks, doc_chunks]\n",
    "        )\n",
    "\n",
    "        # Reduce dimensionality to 2D using PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        vectors_2d = pca.fit_transform(all_vectors)\n",
    "\n",
    "        # Split back into separate arrays\n",
    "        question_2d = vectors_2d[0]\n",
    "        matching_2d = vectors_2d[1 : len(matching_chunks) + 1]\n",
    "        docs_2d = vectors_2d[len(matching_chunks) + 1 :]\n",
    "\n",
    "        # Create plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "\n",
    "        # Plot all document chunks in background\n",
    "        plt.scatter(\n",
    "            docs_2d[:, 0],\n",
    "            docs_2d[:, 1],\n",
    "            c=\"gray\",\n",
    "            alpha=0.5,\n",
    "            label=\"All document embeddings\",\n",
    "        )\n",
    "\n",
    "        # Plot matching chunks\n",
    "        plt.scatter(\n",
    "            matching_2d[:, 0],\n",
    "            matching_2d[:, 1],\n",
    "            c=\"green\",\n",
    "            marker=\"s\",\n",
    "            s=100,\n",
    "            label=\"Matching embeddings\",\n",
    "        )\n",
    "\n",
    "        # Plot question vector\n",
    "        plt.scatter(\n",
    "            question_2d[0],\n",
    "            question_2d[1],\n",
    "            c=\"red\",\n",
    "            marker=\"*\",\n",
    "            s=200,\n",
    "            label=\"Question embeddings\",\n",
    "        )\n",
    "\n",
    "        # Add labels and legend\n",
    "        plt.title(\"2D Visualization of Question with Matched Documents\")\n",
    "        plt.xlabel(\"Principal Component 1\")\n",
    "        plt.ylabel(\"Principal Component 2\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add arrows from question to matching chunks\n",
    "        for match in matching_2d:\n",
    "            plt.arrow(\n",
    "                question_2d[0],\n",
    "                question_2d[1],\n",
    "                match[0] - question_2d[0],\n",
    "                match[1] - question_2d[1],\n",
    "                color=\"blue\",\n",
    "                alpha=0.3,\n",
    "                head_width=0.001,\n",
    "                head_length=0.001,\n",
    "            )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating 2D visualization: {e}\")\n",
    "    finally:\n",
    "        loading_spinner2.value = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f509b29-6753-4323-a934-0f11734f9acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Setup loading spinner for initial interface preparation\n",
    "loading_spinner3 = widgets.HTML(\n",
    "    value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Please wait while we prepare the question-answer interface and get the answer for first question...\",\n",
    ")\n",
    "\n",
    "# Load question embeddings data\n",
    "pdf_question_embeddings_store = tdf_question_embeddings_store.to_pandas().reset_index()\n",
    "\n",
    "display(loading_spinner3)\n",
    "\n",
    "# Create dropdown options from questions\n",
    "op = list(\n",
    "    zip(\n",
    "        pdf_question_embeddings_store[[\"txt\"]].values.flatten(),\n",
    "        pdf_question_embeddings_store[[\"query_id\"]].values.flatten(),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create dropdown widget for question selection\n",
    "prod_dw = Dropdown(\n",
    "    options=op,\n",
    "    description=\"Please select the query:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    display=\"flex\",\n",
    "    flex_flow=\"column\",\n",
    "    align_items=\"stretch\",\n",
    "    layout=widgets.Layout(width=\"50%\", height=\"50px\"),\n",
    "    value=101,\n",
    ")\n",
    "\n",
    "# Load all document chunks for 2D visualization\n",
    "doc_chunks = tdf_embeddings_store.loc[:, \"emb_0\":\"emb_767\"].get_values()\n",
    "\n",
    "\n",
    "@interact(query_id=prod_dw)\n",
    "def print_product(query_id):\n",
    "    \"\"\"\n",
    "    Process selected query and display results with visualization.\n",
    "\n",
    "    Args:\n",
    "        query_id: Selected query ID from dropdown\n",
    "    \"\"\"\n",
    "    # Create loading spinner for query processing\n",
    "    loading_spinner = widgets.HTML(\n",
    "        value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Thinking...\",\n",
    "    )\n",
    "\n",
    "    # Create loading spinner for plot generation\n",
    "    loading_spinner2 = widgets.HTML(\n",
    "        value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Drawing...\",\n",
    "    )\n",
    "\n",
    "    if query_id != \"\":\n",
    "        display(loading_spinner)\n",
    "\n",
    "    try:\n",
    "        # Process the query\n",
    "        response = process_query(query_id)\n",
    "\n",
    "        if response is not None:\n",
    "            loading_spinner.value = \"\"\n",
    "            display(Markdown(response_template(response)))\n",
    "\n",
    "            # Generate 2D visualization if document references exist\n",
    "            if len(response[\"ref_ids\"]) > 0:\n",
    "                print(\"Drawing a 2D plot, please wait.\")\n",
    "                display(loading_spinner2)\n",
    "\n",
    "                # Filter matching chunks for the query\n",
    "                matching_chunks = (\n",
    "                    tdf_embeddings_store.loc[\n",
    "                        tdf_embeddings_store[\"id\"].isin(list(response[\"ref_ids\"]))\n",
    "                    ]\n",
    "                    .iloc[:, 3:]\n",
    "                    .get_values()\n",
    "                )\n",
    "\n",
    "                # Get question embedding\n",
    "                question_vector = (\n",
    "                    tdf_question_embeddings_store.loc[\n",
    "                        tdf_question_embeddings_store[\"query_id\"] == query_id\n",
    "                    ]\n",
    "                    .iloc[:, 2:]\n",
    "                    .get_values()\n",
    "                )\n",
    "\n",
    "                # Generate 2D plot\n",
    "                plot_vectors_2d(\n",
    "                    question_vector, matching_chunks, doc_chunks, loading_spinner2\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        loading_spinner.value = \"\"\n",
    "        loading_spinner2.value = \"\"\n",
    "        print(f\"Error processing query: {e}\")\n",
    "    finally:\n",
    "        loading_spinner3.value = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b9d245-50a1-4ae4-bdb2-d8212950fe39",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>11. Integrated data with customer 360</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following is an example of the output from LLM integrated with existing customer360 data. Please scroll to the right to see all the columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ad647-0d71-4317-8904-a30ae9d552b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create augmented Customer 360 data with chatbot interaction metrics\n",
    "cusomter360_augmented = complaints_data.to_pandas().reset_index()\n",
    "\n",
    "# Add simulated chatbot interaction columns\n",
    "cusomter360_augmented[\"num_of_question_asked\"] = np.random.randint(\n",
    "    8, 40, size=len(cusomter360_augmented)\n",
    ")\n",
    "cusomter360_augmented[\"types of question\"] = np.random.choice(\n",
    "    [\"insurance\", \"personal banking\"], size=len(cusomter360_augmented)\n",
    ")\n",
    "\n",
    "# Add recommended bank strategy based on interaction patterns\n",
    "cusomter360_augmented[\"bank strategy\"] = [\n",
    "    \"Insurance Manager to contact customer immediately\",\n",
    "    \"Send Policy Letter from Insurance Servicing\",\n",
    "    \"Insurance Manager to follow-up with Title Company for documentation and contact customer\",\n",
    "    \"Insurance Manager to contact customer immediately\",\n",
    "    \"All answered by chatbot, no action required\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78573e7-5f4d-46bd-98cd-dda87e1f4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full column content without truncation\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "cusomter360_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8b817-c0ff-42b3-a651-c8268ac40942",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>11. Cleanup</b>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>11.1 Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Cleanup work tables to prevent errors next time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed8c3d7-3772-419f-b462-3e8ab8cc5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up work tables created during the demo\n",
    "tables_to_drop = [\n",
    "    \"question_to_ask\",\n",
    "    \"question_to_ask_embeddings\",\n",
    "    \"df_matching_chunks\",\n",
    "    \"docs_data\",\n",
    "    \"pdf_embeddings_store\",\n",
    "]\n",
    "\n",
    "for table in tables_to_drop:\n",
    "    try:\n",
    "        db_drop_table(table_name=table, schema_name=\"demo_user\")\n",
    "        print(f\"Dropped table: {table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not drop table {table}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d26a8a-23a9-4324-8379-1e6b426ce6e5",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'> <b>11.2 Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will use the following code to clean up tables and databases created for this demonstration.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e4803-5b5f-464b-8004-15df96c99d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove demo data and databases (takes approximately 5 seconds)\n",
    "%run -i ../run_procedure.py \"call remove_data('DEMO_ComplaintAnalysis');\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30521be8-1d18-48b2-a857-d5377ac0e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection and remove context\n",
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5463848-592f-4321-b852-287e133872dd",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid #91A0Ab\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2023-2026. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
