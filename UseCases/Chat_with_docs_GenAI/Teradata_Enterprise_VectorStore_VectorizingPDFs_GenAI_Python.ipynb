{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bc532d-c78c-4c89-b191-242da0733f39",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Teradata Enterprise Vector Store : Vectorizing PDF, Audio and Text files\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff71661-19b4-423a-867a-7c815b064c81",
   "metadata": {},
   "source": [
    "<p style = 'font-size:20px;font-family:Arial'><b>Introduction:</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In our chat with the documentation and database system using Generative AI, we have combined <b>RAG, Langchain, LLM models, and SQLAgents.</b> This allows us to ask queries in layman's terms, retrieve relevant information from the Vector store and/or Vantage Table, and generate accurate and concise answers based on the retrieved data. This integration of retrieval-based and generative-based approaches provides a powerful tool for extracting knowledge from structured or unstructured sources like PDFs, text, or audio files and delivering user-friendly responses.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>In this demo we will build Chatbot type feature by using LangChain, a powerful library for working with LLMs like <b>OpenAI's GPT-4, Amazon's Titan, Anthropic Claude 3.5, etc.</b> and JumpStart in ClearScape notebooks, a system is built where users can ask business questions in natural English and receive answers with data drawn from the relevant databases.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following diagram illustrates the architecture.</p>\n",
    "\n",
    "<center><img src=\"images/rag1.png\" alt=\"architecture\"  width=1200 height=1000/></center>\n",
    "\n",
    "\n",
    "<br>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Before going any farther, let's get a better understanding of RAG, LangChain, and LLM.</p>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial'><b><li> Retrieval-Augmented Generation (RAG):</li></b></ol>\n",
    "<p style = 'font-size:16px;font-family:Arial'> &emsp;  &emsp;RAG is a framework that combines the strengths of retrieval-based and generative-based approaches in question-answering systems.It utilizes both a retrieval model and a generative model to generate high-quality answers to user queries. The retrieval model is responsible for retrieving relevant information from a knowledge source, such as a database or documents. The generative model then takes the retrieved information as input and generates concise and accurate answers in natural language.</p>\n",
    "\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>A typical RAG (Retrieval-and-Generation) application has two main components:</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Indexing:</b> a pipeline for ingesting data from a source and indexing it. This usually happens offline. The indexing process involves several steps, including loading the data, splitting it into smaller chunks, and storing and indexing the splits. This is often done using a VectorStore and Embeddings model.</p>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Retrieval and generation:</b> the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. The retrieval process involves searching the index for the most relevant data based on the user query, and then passing that data to the model for generation.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The most common full sequence from raw data to answer looks like:</p>\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Indexing</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li><b>Load:</b> Load: First we need to load our data. We'll use <code>PyMuPDFLoader</code> for this.</li>\n",
    "    <li><b>Split:</b> Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't in a model's finite context window. Here, our pdf document will be splits into pages.</li>\n",
    "    <li><b>Store:</b> We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model</li>\n",
    "    </ul>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following diagram illustrates the architecture of load, split and store.</p>\n",
    "\n",
    "<center><img src=\"images/rag_load_store.png\" alt=\"rag indexing architecture\"  width=800 height=600/></center>\n",
    "<center>image source: <a href=\"https://python.langchain.com/docs/use_cases/question_answering/\">langchain.com</a></center>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'><b>Retrieval and generation</b></p>\n",
    "<ul style = 'font-size:16px;font-family:Arial'>\n",
    "    <li><b>Retrieval:</b> During runtime, the user inputs a query. We first generate embeddings for it, which are then passed to the Vantage in-db function <b>TD_VectorDistance</b> to retrieve similar documents as context. This context is then fed into the LLM model.</li>\n",
    "    <li><b>Generation:</b> Finally, the model generates an answer based on the retrieved data. The answer is then presented to the user.</li>\n",
    "    </ul>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial'>The following diagram illustrates the architecture of retrieval and generation.</p>\n",
    "<center><img src=\"images/rag_retrieval_generation_td.png\" alt=\"retrieval generation architecture\" width=800 height=600/></center>\n",
    "<center>image source: <a href=\"https://python.langchain.com/docs/use_cases/question_answering/\">langchain.com</a></center>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial' start=\"2\"><b><li> Langchain:</li></b></ol>\n",
    "<p style = 'font-size:16px;font-family:Arial'> &emsp;  &emsp; LangChain is a framework that facilitates the integration and chaining of large language models with other tools and sources to build more sophisticated AI applications. LangChain does not serve its own LLMs; instead, it provides a standard way of communicating with a variety of LLMs, including those from OpenAI and HuggingFace. LangChain accelerates the development of AI applications with building blocks. We learn the leverage the following building blocks in this notebook:</p>\n",
    " \n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li> <b> LLMs</b> – LangChain's <code>llm</code> class is designed to provide a standard interface for all LLM it supports.   </li>\n",
    "    <li> <b> PromptTemplate</b>  - LangChain’s <code>PromptTemplate</code> class are predefined structures for generating prompts for LLM’s. They can be reused across different LLM's.</li>\n",
    "    <li> <b> Chains</b> – When we build complex AI applications, we may need to combine multiple calls to LLM’s and to other components  LangChain’s <code>chain</code> class allows us to link calls to LLM’s and components. The most common type of chaining in any LLM application is combining a prompt template with an LLM and optionally an output parser. </li>\n",
    "</ol>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial' start=\"3\"><b><li> LLM Models (Large Language Models):</li></b></ol>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'> &emsp;  &emsp; LLM models refer to the large-scale language models that are trained on vast amounts of text data.\n",
    "These models, such as GPT-4, Llama 3,  Google's Gemini 1.5, etc. are capable of generating human-like text responses. LLM models have been pre-trained on diverse sources of text data, enabling them to learn patterns, grammar, and context from a wide range of topics. They can be fine-tuned for specific tasks, such as question-answering, natural language understanding, and text generation.\n",
    "LLM models have achieved impressive results in various natural language processing tasks and are widely used in AI applications for generating human-like text responses.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca49b9d-df0b-400a-acf8-0252ab8a2618",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'><b>Steps in the analysis:</b></p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "    <li>Configuring the environment</li>\n",
    "    <li>Connect to Vantage</li>\n",
    "    <li>Data Exploration Getting Data for This Demo</li>\n",
    "    <li>Read source data</li>\n",
    "    <li>Generate embeddings from the chunks</li>\n",
    "    <li>Insert Prompts into a Table</li>\n",
    "    <li>Generate Embeddings from the Prompts</li>\n",
    "    <li>Find top 10 matching chunks</li>\n",
    "    <li>Configuring AWS CLI and Initialize Bedrock Model</li>\n",
    "    <li>Test and Compare Results</li>\n",
    "    <li>Cleanup</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833b5bf-74af-42be-8543-3782e1da95dc",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>1. Configuring the environment</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94f1e8-489b-4084-80c6-c9cff1ef6ee8",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.1 Install the required libraries</b></p>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i><b>Note:</b>The installation of the required libraries will take approximately <b>4 to 5 minutes</b> for the first-time installation. However, if the libraries are already installed, the execution will complete within 5 seconds.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8bf123-c1c7-4b3a-8c42-404be0936ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install --upgrade -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bd85d-a80f-4903-bc2d-71f542590450",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install torchaudio transformers langchain-aws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b160ce-5ace-4116-86b6-394d6502553b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><b>Note: </b><i>The above statements will install the required libraries to run this demo. Be sure to restart the kernel after executing the above lines to bring the installed libraries into memory. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61067c88-2e9a-4c92-985b-34dc4ab74a13",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.2 Import the required libraries</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Here, we import the required libraries, set environment variables and environment paths (if required).</p>\n",
    "\n",
    "<ul style=\"font-size: 16px; font-family: Arial; list-style-type: disc; padding-left: 20px;\">\n",
    "    <li>\n",
    "        <b>teradataml</b>: Enables enables us to establish a connection to our database using the <code>create_context()</code> function and allows us to create virtual DataFrames, which serve as references to database objects, allowing exploration of object storage data and enabling operations directly on Vantage without transferring entire datasets to the client, except when needed. For this demo, we will be exploring a dataset in S3 via a foreign table on Vantage.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>LangChain’s SQLDatabase class </b>: A wrapper around the SQLAlchemy engine to facilitate interactions with databases using SQLAlchemy’s Python SQL toolkit and ORM capabilities.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b> LangChain’s create_sql_agent function</b>: A LangChain function to build a SQL agent by providing a language model and a database connection.\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>LangChain’s ChatBedrockConverse class</b>: A common interface for working with Amazons Bedrock's FM's that support chat functionalities.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea50a4aa-1211-44fc-8166-317c35253207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "import timeit\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, Dropdown\n",
    "\n",
    "# Data manipluation and Visualization libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Teradata libraries\n",
    "from teradataml import (\n",
    "    create_context,\n",
    "    delete_byom,\n",
    "    execute_sql,\n",
    "    save_byom,\n",
    "    remove_context,\n",
    "    in_schema,\n",
    "    display,\n",
    "    DataFrame,\n",
    "    db_drop_table,\n",
    "    db_drop_view,\n",
    "    VectorDistance,\n",
    "    configure,\n",
    "    ONNXEmbeddings\n",
    ")\n",
    "\n",
    "# helper functions\n",
    "from utils.sql_helper_func import *\n",
    "from utils.transcripts_helper_func import *\n",
    "\n",
    "# LLM - Updated for langchain 1.0.5\n",
    "from langchain_text_splitters  import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "import bs4\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "display.max_rows = 5\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30714e67-7ba2-449f-8970-3aac125e48d8",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>1.3 Load Audio model and test</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Let's load the <b>Small Language Model (SLM)</b> from <code>huggingface</code> and verify it output.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d264ff-8919-4799-a2d7-1e06ed527674",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56af632-837c-4d94-a950-ca93106d4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[2][\"audio\"]\n",
    "input_features = processor(\n",
    "    sample[\"array\"],\n",
    "    sampling_rate=sample[\"sampling_rate\"],\n",
    "    return_tensors=\"pt\"\n",
    ").input_features\n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features)\n",
    "\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(\"--\"*25)\n",
    "print(\"transcription: \\n\",transcription[0])\n",
    "print(\"--\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11368d-2efb-4906-8e28-d50f0bca6429",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i>The code above will download the necessary models to generate the embeddings required to run this demo. The initial download may take approximately 50-60 seconds if you are running this demo for the first time in this environment. However, subsequent runs will be much faster since the models will already be available locally.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59718f8-7af4-4d1a-abc7-a860eb7cbae3",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>2. Connect to Vantage</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83770df6-b923-4cc4-a839-de55c62b32ae",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>2.1 Connect to Vantage</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164cfc91-93ed-45b9-98ba-73b91a50c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)\n",
    "execute_sql('''SET query_band='DEMO=PP_Teradata_Enterprise_VectorStore_VectorizingPDFs_GenAI_Python.ipynb;' UPDATE FOR SESSION;''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f3fe2-ed63-4838-bb19-4c0d0157453d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>3. Data Exploration Getting Data for This Demo</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The Chat with documentation demo aims to demonstrate how users can interact with documents such as insurance policy wordings, invoices, and other similar documents through a conversational interface. Additionally in this demo, we have added Audio and text files as well to extract transcripts from audio and make it conversational.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b94e0-bb25-46a2-a6a9-5eedaa1d39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call get_data('DEMO_ComplaintAnalysis_local');\"     # Takes about 2 minutes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a762f2e5-fd9a-4306-8dd9-f635996a7ddb",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Optional step – We should execute the below step only if we want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ba64d-11f6-4fb5-a52f-51cb6f379955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20a791-cc79-45b6-bce7-56cb26c37ac7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We have a Customer 360 details table containing all the customers' personal and banking-related information. We will use this table to ask questions in natural language and retrieve answers from the Vantage Database.<p/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89760921-033b-4c67-bee4-68b85da3b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_data = DataFrame(in_schema(\"DEMO_ComplaintAnalysis\", \"Customer_360_Details\"))\n",
    "complaints_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1626b5-1693-4683-a275-5aa80c862f8d",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<a id='section4'></a>\n",
    "<b style = 'font-size:20px;font-family:Arial'>4. Read source data. </b>\n",
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>4.1 Run the data loader </b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfd968-aaac-45cd-8f3c-35fac1218981",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>The Traveller Easy Single Trip - International insurance policy is a comprehensive travel insurance plan that provides cover for a wide range of risks, including medical expenses, trip cancellation, loss of luggage, and personal accident. The policy is designed to be affordable and flexible, and it can be purchased online or over the phone.<p/>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>The source data from <a href=\"https://axa-com-my.cdn.axa-contento-118412.eu/axa-com-my/3d2f84a5-42b9-459b-911a-710546df0633_Policy+wording+-+SmartTraveller+Easy+Single+Trip+-+International+%280820%29.pdf\">AXA</a> is loaded in Teradata Vantage as Vector Database.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>Now, let's use <code>PyMuPDFLoader</code> library to read the pdf document and split it into pages.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>For Audio files <code>openai/whisper-small</code> open source audio model we have used to extract the transcripts and split it into pages.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2386198-83cb-4c17-9eb2-597807b58720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_id(df, id_column=\"id\", start_id=1000):\n",
    "    if df.empty or df[id_column].max() < start_id:\n",
    "        return start_id\n",
    "    else:\n",
    "        return df[id_column].max() + 1\n",
    "\n",
    "\n",
    "def get_splitter():\n",
    "    # split the page content\n",
    "    return RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=30,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def read_document_content(raw_data_df, pages, file_name):\n",
    "    docs = [p.page_content for p in pages]\n",
    "    docs = get_splitter().create_documents(docs)\n",
    "\n",
    "    texts_data = []\n",
    "    for t in docs:\n",
    "        texts_data.append(t.page_content)\n",
    "\n",
    "    # generate the dataframe\n",
    "    temp_df = pd.DataFrame(data=texts_data, columns=[\"txt\"])\n",
    "    next_id = get_next_id(raw_data_df)\n",
    "    temp_df[\"id\"] = range(next_id, len(temp_df.index) + next_id)\n",
    "    temp_df[\"txt\"] = texts_data\n",
    "    temp_df[\"file_name\"] = file_name\n",
    "\n",
    "    # Concatenate the new DataFrame with the existing one\n",
    "    return pd.concat([raw_data_df, temp_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658e404-6618-4379-b915-3f8e1b2b04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_files(directory_path):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", \"Due to a bug fix in https://github.com/huggingface/transformers/pull\")\n",
    "    # main raw df\n",
    "    columns = {\n",
    "        \"id\": \"int64\",\n",
    "        \"txt\": \"object\",\n",
    "        \"file_name\": \"object\",\n",
    "    }\n",
    "\n",
    "    # Create a loading spinner\n",
    "    loading_spinner = widgets.HTML(\n",
    "        value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Reading the raw data...\",\n",
    "    )\n",
    "    # for temp df\n",
    "    raw_data_df = pd.DataFrame(\n",
    "        {col: pd.Series(dtype=dt) for col, dt in columns.items()}\n",
    "    )\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        display(loading_spinner)\n",
    "        if \".ipynb_checkpoints\" in root:\n",
    "            continue\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            if file_name.lower().endswith(\".mp3\"):\n",
    "                print(f\"MP3 File: {file_name}\")\n",
    "                transcripts = process_audio(file_path)\n",
    "                texts = get_splitter().create_documents([transcripts])\n",
    "                raw_data_df = read_document_content(raw_data_df, texts, file_name)\n",
    "            elif file_name.lower().endswith(\".pdf\"):\n",
    "                print(f\"PDF File: {file_name}\")\n",
    "                pages = PyMuPDFLoader(file_path).load_and_split()\n",
    "                print(\"total pages: \", len(pages))\n",
    "                raw_data_df = read_document_content(raw_data_df, pages, file_name)\n",
    "            elif file_name.lower().endswith(\".txt\"):\n",
    "                print(f\"TXT File: {file_name}\")\n",
    "                with open(file_path, \"r\") as file:\n",
    "                    text_content = file.read()\n",
    "                    texts = get_splitter().create_documents([text_content])\n",
    "                    raw_data_df = read_document_content(raw_data_df, texts, file_name)\n",
    "            else:\n",
    "                print(f\"Skipping: {file_name}\")\n",
    "    print(\"*\" * 70)\n",
    "    print(\"All the source files have been read, and chunking has been completed.\")\n",
    "    print(\"*\" * 70)\n",
    "    # Hide the loading spinner after the process is done\n",
    "    loading_spinner.value = \"\"\n",
    "    return raw_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0aa71d-457d-448a-ad11-da18ed77dd94",
   "metadata": {},
   "source": [
    "<p style='font-size:16px;font-family:Arial'>In the above cell, we will read all the pages of the PDF file and split them into pages. To process further, we will save documents to Vantage.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cac2b5-9e86-427a-9579-cbccebb1c2bd",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;'><b>5. Load HuggingFace Model</b>\n",
    "<p style = 'font-size:16px;font-family:Arial;'>To generate embeddings, we need an ONNX model capable of transforming text into vector representations. We use a pretrained model from [Teradata's Hugging Face repository](https://huggingface.co/Teradata/gte-base-en-v1.5), such as gte-base-en-v1.5. The model and its tokenizer are downloaded and stored in Vantage tables as BLOBs using the save_byom function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1460110-5323-406b-b468-749449243fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# set env\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "\n",
    "model_name = \"bge-base-en-v1.5\"\n",
    "number_dimensions_output = 768\n",
    "model_file_name = \"model.onnx\"\n",
    "\n",
    "hf_hub_download(repo_id=f\"Teradata/{model_name}\", filename=f\"onnx/{model_file_name}\", local_dir=\"./\")\n",
    "hf_hub_download(repo_id=f\"Teradata/{model_name}\", filename=\"tokenizer.json\", local_dir=\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823b1e2a-1c34-4e45-bd19-6e7d4057c6ab",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none\">\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.1 Save the Model</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>In above steps, we have checked that the model is working fine in ONNX format. Now we will save the model file.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde3d2ff-3497-4a2c-80f6-35ba3d9a6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db_drop_table(\"embeddings_models\")\n",
    "except Exception as e:\n",
    "    pass\n",
    "try:\n",
    "    db_drop_table(\"embeddings_tokenizers\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af356f-3e78-4af5-9b59-c5e66833e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Models into Vantage\n",
    "# a) Embedding model\n",
    "save_byom(model_id = model_name, # must be unique in the models table\n",
    "               model_file = f\"onnx/{model_file_name}\",\n",
    "               table_name = 'embeddings_models' )\n",
    "# b) Tokenizer\n",
    "save_byom(model_id = model_name, # must be unique in the models table\n",
    "              model_file = 'tokenizer.json',\n",
    "              table_name = 'embeddings_tokenizers') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f5eb7-e703-473b-bc39-58c1b355f8ca",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;'>Recheck the installed model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dde44f-1aca-4f1e-be90-4c7a8703b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = DataFrame('embeddings_models')\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827989c-736f-462a-864d-763679415240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token = DataFrame('embeddings_tokenizers')\n",
    "df_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2501ce-3fe6-40d5-9f86-f696d51d033c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Load the mode that we have save to DB in previous notebook by passing Model ID.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc277932-60cf-4f14-92db-546f33c2dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = DataFrame.from_query(f\"select * from embeddings_models where model_id = '{model_name}'\")\n",
    "my_tokenizer = DataFrame.from_query(f\"select model as tokenizer from embeddings_tokenizers where model_id = '{model_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a95d65-8866-4423-9a52-2dec04c63ab4",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>5. Generate embeddings from the chunks.</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will create prompts for different questions that can be answered from the document. Below are some sample questions that can be asked.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46431bb8-5524-42cf-bc36-62863277b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure.val_install_location = \"val\"\n",
    "configure.byom_install_location = \"mldb\"\n",
    "\n",
    "def generate_embeddings_data(input_tdf, cols_to_preserve):\n",
    "    return ONNXEmbeddings(\n",
    "    newdata = input_tdf,\n",
    "    modeldata = my_model, \n",
    "    tokenizerdata = my_tokenizer, \n",
    "    accumulate = cols_to_preserve,\n",
    "    model_output_tensor = \"sentence_embedding\",\n",
    "    output_format = f'FLOAT32({number_dimensions_output})',\n",
    "    enable_memory_check = False\n",
    ").result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d565b-5898-4859-b28e-0a2779c2cee9",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;\">\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>5.1 Do you want to generate the embeddings?</b></p>    \n",
    "<p style = 'font-size:16px;font-family:Arial'>Generating embeddings will take around <b>35-40 minutes.</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial'>We have already generated embeddings for the pdf and stored them in <b>Vantage</b> table.</p>\n",
    " \n",
    "<center><img src=\"images/decision_emb_gen_2.svg\" alt=\"embeddings_decision\"  width=300 height=400/></center>\n",
    " \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial'><i><b>Note: If you would like to skip the embedding generation step to save the time and move quickly to next step, please enter \"No\" in the next prompt.</b></i></p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i><b>Note:</b>If you choose <b>\"yes\"</b> to run the embeddings generation step, you must first execute the <a href=\"./Initialization_and_Model_Load.ipynb\">Initialization_and_Model_Load.ipynb</a> file to install the ONNX model on the ClearScape machine.</i></p>\n",
    "</div>\n",
    "\n",
    " \n",
    "<p style = 'font-size:16px;font-family:Arial'>To save time, you can move to the already generated embeddings section. However, if you would like to see how we generate the embeddings, or if you need to generate the embeddings for a different dataset, then continue to the following section.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a6b6a-24a2-45d2-b0e1-269bc6d29b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emb():\n",
    "    display(loading_spinner)\n",
    "    print(\"*\" * 50)\n",
    "    print(\"Step1: Reading the row data from pdf, txt and audio files...\")\n",
    "    print(\"*\" * 50)\n",
    "    directory_path = \"./data\"\n",
    "    final_raw_data_df = read_data_files(directory_path)\n",
    "\n",
    "    print(\"*\" * 50)\n",
    "    print(\"Step2: Saving raw data to SQL ...\")\n",
    "    print(\"*\" * 50)\n",
    "    # copy docs to vantage\n",
    "    copy_to_sql(\n",
    "        final_raw_data_df,\n",
    "        table_name=\"docs_data\",\n",
    "        primary_index=\"id\",\n",
    "        if_exists=\"replace\",\n",
    "    )\n",
    "\n",
    "    tdf_docs = DataFrame(\"docs_data\")\n",
    "    print(\"Data information: \\n\", tdf_docs.shape)\n",
    "    tdf_docs.sort(\"id\")\n",
    "\n",
    "    print(\"*\" * 50)\n",
    "    print(\"Step3: Now, we will start generating embeddings.\")\n",
    "    print(\"*\" * 50)\n",
    "    display(loading_spinner)\n",
    "\n",
    "    display(Markdown(get_section5_desc_start(tdf_docs)))\n",
    "    start = time.time()\n",
    "\n",
    "    # create views\n",
    "    cols_to_preserve = [\"id\", \"txt\", \"file_name\"]\n",
    "    docs_data = DataFrame(\"docs_data\")\n",
    "    df_embeddings = generate_embeddings_data(docs_data, cols_to_preserve)\n",
    "    copy_to_sql(df_embeddings,table_name='pdf_embeddings_store', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb90189-e906-4e40-b3fd-79fd45faccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_emb():\n",
    "    print(\"*\" * 60)\n",
    "    print(\"Step1: Loading raw data from the parquet file stored locally.\")\n",
    "    print(\"*\" * 60)\n",
    "    # load raw data to sql\n",
    "    raw_data_prq = pd.read_parquet(\"./embeddings/all_source_data_v1.parquet.gzip\")\n",
    "\n",
    "    # save to DB\n",
    "    delete_and_copy_embeddings(\n",
    "        table_name=\"docs_data\",\n",
    "        tdf=raw_data_prq,\n",
    "        eng=eng,\n",
    "    )\n",
    "\n",
    "    print(\"*\" * 60)\n",
    "    print(\"Step2: Loading embeddings from the parquet file stored locally.\")\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "    # load embeddings to sql\n",
    "    embeddings_prq = pd.read_parquet(\"./embeddings/all_embeddings_v3.parquet.gzip\")\n",
    "\n",
    "    # save to DB\n",
    "    delete_and_copy_embeddings(\n",
    "        table_name=\"pdf_embeddings_store\",\n",
    "        tdf=embeddings_prq,\n",
    "        eng=eng,\n",
    "    )\n",
    "\n",
    "    print(\"*\" * 50)\n",
    "    print(\"Embeddings loaded and saved successfully!\")\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb446a-daac-4bc7-81f5-7a8559da2f95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loading_spinner = widgets.HTML(\n",
    "    value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Generating the embeddings for documents...\",\n",
    ")\n",
    "\n",
    "\n",
    "def get_section5_desc_start(tdf):\n",
    "    return f\"\"\"<div class=\"alert alert-info\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i><b>Please be patient:</b> Generating embeddings for {tdf.shape[0]} document contents may take up to 35 to 40 minutes. It is depends on number of APMS in the database. Since the volume of data is large and the machine is small, going through the below code could take up to 40 minutes. </i></p>\n",
    "</div>\"\"\"\n",
    "\n",
    "\n",
    "# Request user's input\n",
    "generate = input(\"Do you want to generate embeddings? ('yes'/'no'): \")\n",
    "\n",
    "try:\n",
    "    # Check the user's input\n",
    "    if generate.lower() == \"yes\":\n",
    "        generate_emb()\n",
    "    elif generate.lower() == \"no\":\n",
    "        load_data_emb()\n",
    "    else:\n",
    "        print(\"\\nInvalid input. Please enter 'yes' or 'no' to proceed.\")\n",
    "except Exception as e:\n",
    "    print(\"ERROR: \", e)\n",
    "finally:\n",
    "    loading_spinner.value = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae7c12f-d304-4b4d-b497-1c3319f09bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_embeddings_store = DataFrame(\"pdf_embeddings_store\")\n",
    "tdf_embeddings_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508be45c-ebdf-4292-badf-72507128c3d1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>Let's view the shape of embeddings table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf215b71-2758-406a-b962-b1bf5651ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_embeddings_store.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb61754-527e-467b-86f1-4ec6f98e0d45",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>6. Insert Prompts into a Table</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0664918-7abc-4c30-8fc3-7af415475475",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will create the required table and than we will insert different values for the prompts.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d9aac-60e9-4422-aafb-205e3d00bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_to_ask_table():\n",
    "    qry = \"\"\"CREATE MULTISET TABLE question_to_ask(\n",
    "      txt VARCHAR(1024) CHARACTER SET UNICODE NOT CASESPECIFIC,\n",
    "      query_id INT) NO PRIMARY INDEX\"\"\"\n",
    "    try:\n",
    "        execute_sql(qry)\n",
    "    except:\n",
    "        db_drop_table(\"question_to_ask\")\n",
    "        execute_sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b19d1-80cc-4397-aee5-d43fb1fa5656",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>We will create prompts for different questions that can be answered from the document. Below are some sample questions that can be asked.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761d88d-813f-40ae-b8b2-a9500bbb04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_questions():\n",
    "    prompts = [\n",
    "    \"Does this policy cover  Loss of or Damage to the Insured’s Articles?\",\n",
    "    \"What is the reimbursement limit per Baggage?\",\n",
    "    \"What is the sum insured amount in the case Accidental Death in domestic and international for adult as well as child?\",\n",
    "    \"What documents are required for Rental Car Excess?\",\n",
    "    \"Where can I submit my complaints or feedback?\",\n",
    "    \"What is the bank tenure of customer 789456123?\",\n",
    "    \"What is the Total credit balance of customer 456789123?\",\n",
    "    \"How many customers have only Credit Card as product holdings?\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    for idx, prompt in enumerate(prompts, start=100):\n",
    "        execute_sql(f\"\"\"INSERT into question_to_ask values ('{prompt}', {idx});\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244cf33-0a9c-44a2-87cb-3599897fa484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_que_emb(table_name):\n",
    "    print(\"*\" * 50)\n",
    "    print(\"Loading question embeddings from the parquet file stored locally.\")\n",
    "    print(\"*\" * 50)\n",
    "\n",
    "    # load embeddings to sql\n",
    "    embeddings_prq = pd.read_parquet(\"./embeddings/questions_embeddings.parquet.gzip\")\n",
    "\n",
    "    # save to DB\n",
    "    copy_to_sql(embeddings_prq, table_name=table_name,primary_index='query_id', if_exists='replace')\n",
    "    \n",
    "    print(\"*\" * 50)\n",
    "    print(\"Question embeddings loaded and saved successfully!\")\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd2227-0019-4ff4-b9f0-d62e7e1e1086",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>7. Generate Embeddings from the Prompts</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will create embeddings for the prompts which we have inserted into the table above.</p>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p style = 'font-size:16px;font-family:Arial'><i><b>Note:</b>If you choose <b>\"yes\"</b> to run the embeddings generation step, you must first execute the <a href=\"./Initialization_and_Model_Load.ipynb\">Initialization_and_Model_Load.ipynb</a> file to install the ONNX model on the ClearScape machine.</i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e0de0-62f0-41b5-90ca-36d60d880f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "loading_spinner = widgets.HTML(\n",
    "    value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Generating the embeddings for documents...\",\n",
    ")\n",
    "\n",
    "# create que table\n",
    "create_question_to_ask_table()\n",
    "add_questions()\n",
    "\n",
    "# Request user's input\n",
    "generate = input(\"Do you want to generate embeddings? ('yes'/'no'): \")\n",
    "\n",
    "# Check the user's input\n",
    "if generate.lower() == \"yes\":\n",
    "    # create views\n",
    "    loading_spinner = widgets.HTML(\n",
    "        value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Generating the embeddings for questions...\",\n",
    "    )\n",
    "\n",
    "    display(loading_spinner)\n",
    "    cols_to_preserv = [\"query_id\", \"txt\"]\n",
    "    question_to_ask = DataFrame(\"question_to_ask\")\n",
    "    df_embeddings_que = generate_embeddings_data(question_to_ask, cols_to_preserv)\n",
    "    copy_to_sql(df_embeddings_que,table_name='question_to_ask_embeddings', if_exists='replace', index=False)\n",
    "    loading_spinner.value = \"\"\n",
    "    \n",
    "elif generate.lower() == \"no\":\n",
    "    load_que_emb(table_name = \"question_to_ask_embeddings\")\n",
    "else:\n",
    "    print(\"\\nInvalid input. Please enter 'yes' or 'no' to proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026aa09-8275-44c7-ad55-8a9b4040631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_question_embeddings_store = DataFrame(\"question_to_ask_embeddings\")\n",
    "tdf_question_embeddings_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd839251-4c35-48e7-9427-712680348ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_question_embeddings_store.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098357a-9a91-4df6-adef-b8fc79003a64",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>8. Find top 10 matching chunks</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will find the top 10 chunks that match the queries using the <b>TD_VectorDistance</b>. The TD_VectorDistance function accepts a table of target vectors and a table of reference vectors and returns a table that contains the distance between target-reference pairs. The function computes the distance between the target pair and the reference pair from the same table. We must have the same column order in the TargetFeatureColumns argument and the RefFeatureColumns argument. The function ignores the feature values during distance computation if the value is either NULL, NAN, or INF.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe2b4f-2fce-4a28-9352-00ee495f6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vector_distance(target_table, reference_table, emb_column_names, topk):\n",
    "    start = timeit.default_timer()\n",
    "    VectorDistance_out = VectorDistance(\n",
    "        target_id_column=\"query_id\",\n",
    "        target_feature_columns=emb_column_names,\n",
    "        ref_id_column=\"id\",\n",
    "        ref_feature_columns=emb_column_names,\n",
    "        distance_measure=[\"Cosine\"],\n",
    "        topk=topk,\n",
    "        target_data=target_table,\n",
    "        reference_data=reference_table,\n",
    "    )\n",
    "\n",
    "    print(f\"vector-distance calculation time:\\t\", timeit.default_timer() - start)\n",
    "    return VectorDistance_out.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb1eeb-262b-48d4-9bb9-88a81549da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_column_names = DataFrame(\"question_to_ask_embeddings\").columns[2:]\n",
    "\n",
    "# select top matching\n",
    "number_of_recommendations = 10\n",
    "\n",
    "\n",
    "vector_distance_df = calculate_vector_distance(\n",
    "    target_table=tdf_question_embeddings_store,\n",
    "    reference_table=tdf_embeddings_store,\n",
    "    emb_column_names=emb_column_names,\n",
    "    topk=number_of_recommendations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7757e751-fba5-45c0-8b41-845bd0662fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_matching_chunks(\n",
    "    vector_distance_df, tdf_embeddings_store, tdf_question_embeddings_store\n",
    "):\n",
    "    embeddings_df_selected_columns = tdf_embeddings_store.select(\n",
    "        [\"id\", \"txt\", \"file_name\"]\n",
    "    )\n",
    "\n",
    "    # join vector-distance results and txt\n",
    "    vec_prod_join_result = vector_distance_df.merge(\n",
    "        right=embeddings_df_selected_columns,\n",
    "        left_on=\"reference_id\",\n",
    "        right_on=\"id\",\n",
    "        lsuffix=\"t1\",\n",
    "        rsuffix=\"t2\",\n",
    "    )\n",
    "\n",
    "    # join the above joined table with search txt\n",
    "    vec_prod_join_result_selected = vec_prod_join_result[\n",
    "        [\"id\", \"txt\", \"file_name\", \"target_id\", \"distancetype\", \"distance\"]\n",
    "    ]\n",
    "\n",
    "    # join_result_sorted_selected\n",
    "    df_que_selected = tdf_question_embeddings_store.select([\"query_id\", \"txt\"])\n",
    "\n",
    "    # recommendation results\n",
    "    df_matched_chunks = df_que_selected.merge(\n",
    "        right=vec_prod_join_result_selected,\n",
    "        left_on=\"query_id\",\n",
    "        right_on=\"target_id\",\n",
    "        how=\"inner\",\n",
    "        lsuffix=\"que\",\n",
    "        rsuffix=\"matched\",\n",
    "    )\n",
    "\n",
    "    # filter with exact match\n",
    "    df_matched_chunks = df_matched_chunks[df_matched_chunks.distance > 0.001]\n",
    "\n",
    "    # sort by distance\n",
    "    df_matched_chunks = df_matched_chunks.sort([\"query_id\", \"distance\"], ascending=True)\n",
    "\n",
    "    return df_matched_chunks[\n",
    "        [\"query_id\", \"txt_que\", \"txt_matched\", \"id\", \"file_name\", \"distance\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f697808-ba33-4d79-90f5-6e3acccb2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top-k final chunks for each query\n",
    "tdf_matching_chunks = get_final_matching_chunks(\n",
    "    vector_distance_df, tdf_embeddings_store, tdf_question_embeddings_store\n",
    ")\n",
    "\n",
    "# copy results to sql for improve the performance\n",
    "copy_to_sql(tdf_matching_chunks, table_name=\"df_matching_chunks\", if_exists=\"replace\")\n",
    "\n",
    "tdf_matching_chunks = DataFrame(\"df_matching_chunks\")\n",
    "tdf_matching_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b04db-fe87-4ed2-8ca4-21c8311ce990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching row docs\n",
    "tdf_docs = DataFrame(\"docs_data\")\n",
    "\n",
    "\n",
    "def get_similarity_search_context(target_id):\n",
    "    # return context, file_name\n",
    "    tmp1 = tdf_matching_chunks.loc[tdf_matching_chunks[\"query_id\"] == target_id][\n",
    "        [\"txt_matched\", \"query_id\", \"file_name\", \"id\"]\n",
    "    ]\n",
    "    file_names = list(\n",
    "        set(\n",
    "            [\n",
    "                f\n",
    "                for f in set(\n",
    "                    tdf_docs[\n",
    "                        tdf_docs.id.isin(list(tmp1[[\"id\"]].get_values().flatten()))\n",
    "                    ][[\"file_name\"]]\n",
    "                    .get_values()\n",
    "                    .flatten()\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    context = \"\\n\".join(tmp1[[\"txt_matched\"]].get_values().flatten())\n",
    "    ref_ids = tmp1[[\"id\"]].get_values().flatten()\n",
    "    return context, file_names, ref_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853abd90-5f61-4f02-be41-bf1fef65b325",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>9. Configuring AWS CLI and Initialize Bedrock Model</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following cell will prompt us for the following information:</p>\n",
    "<ol style = 'font-size:16px;font-family:Arial'>\n",
    "<li><b>aws_access_key_id</b>: Enter your AWS access key ID</li>\n",
    "<li><b>aws_secret_access_key</b>: Enter your AWS secret access key</li>\n",
    "<li><b>region name</b>: Enter the AWS region you want to configure (e.g., us-east-1)</li>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd085be-9012-43e8-9a02-5975d32d5c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = getpass.getpass(\"aws_access_key_id \")\n",
    "secret_key = getpass.getpass(\"aws_secret_access_key \")\n",
    "session_token = getpass.getpass(\"aws_session_token\")\n",
    "region_name = getpass.getpass(\"region name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f6369-811f-45a3-8bbb-28e853695e7c",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.1 Connect to databases using SQL Alchemy</b></p>    \n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Under the hood, we use SQLAlchemy to connect to SQL databases. This means that the SQLDatabaseChain can be used with any SQL dialect supported by SQLAlchemy, such as Teradata Vantage, MS SQL, MySQL, MariaDB, PostgreSQL, Oracle SQL, and SQLite. For more information about the requirements for connecting to our database, we recommend referring to the <a href=\"https://docs.sqlalchemy.org/en/20/\">SQLAlchemy documentation</a>.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>Important: The code below establishes a database connection for our data sources and Large Language Models. Please note that the solution will only work if we define the database connection for our sources in the cell below.</p>\n",
    "\n",
    "<p style='font-size:16px;font-family:Arial'>We build a consolidated view of the Table Data Catalog by combining metadata stored for the database and table.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5842c-e7f1-4333-820a-2c7430110276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create the vantage SQLAlchemy engine\n",
    "database = \"DEMO_ComplaintAnalysis_db\"\n",
    "db = SQLDatabase(\n",
    "    eng,\n",
    "    schema=database,\n",
    "    include_tables=[\"Customer_360_Details\"],\n",
    ")\n",
    "\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b7e93-a952-4794-9e78-e292c54eafd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_d = {\n",
    "    \"Customer_360_Details\": complaints_data.columns,\n",
    "}\n",
    "\n",
    "\n",
    "def get_db_schema():\n",
    "    table_dicts = []\n",
    "    for k in main_d:\n",
    "        table_dicts.append(\n",
    "            {\n",
    "                # \"database_name\": database,\n",
    "                \"table_name\": k,\n",
    "                \"column_names\": main_d[k],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    database_schema_string = \"\\n\".join(\n",
    "        [\n",
    "            f\"Table: {table['table_name']}\\nColumns: {', '.join(table['column_names'])}\"\n",
    "            for table in table_dicts\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return database_schema_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae81a2fc-990e-4f44-be64-f6a198f27329",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_schema = get_db_schema()\n",
    "print(database_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae78e9-76ce-42ce-9c89-722a372ef3e0",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.2 Define LLM model</b></p>  \n",
    "\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "    Define the LLM using the <code>ChatBedrockConverse</code> interface. When defining <code>ChatBedrockConverse</code>, set the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns\">Amazon Bedrock base model ID</a>, the client as <code>boto3_bedrock</code>, and the common inference parameters.\n",
    "</p> \n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "    We use the optional parameter <b>temperature</b> to make our Teradata SQL outputs more predictable.\n",
    "</p>\n",
    "\n",
    "<div style=\"margin-left: 16px; font-size: 16px; font-family: Arial;\">\n",
    "    <b>- Temperature:</b> which can range from 0.0 to 2 and controls how creative our results will be, Setting it to 0.1 ensures the model favors higher-probability (more predictable) words, resulting in more consistent and less varied outputs.<br>\n",
    "</div>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "    For a complete list of optional parameters for base models provided by Amazon Bedrock, visit the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\"> AWS docs</a>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4129e1-f1dd-4546-a985-108ec7a656fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c81df1-0946-44ed-a5ba-80c440129ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(model_provider=\"bedrock\", model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "                        region_name=region_name,\n",
    "                        aws_access_key_id=access_key,\n",
    "                        aws_secret_access_key=secret_key,\n",
    "                        aws_session_token=session_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb82f7-ea5c-48c5-a91d-c4e090c3ce7e",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.3 Define SQL Agent</b></p>  \n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "    With the connection to Teradata Vantage established and our database (<code>db</code>) and Large Language Model (<code>LLM</code>) defined, we are ready to create and invoke our SQL Agent using the <code>create_sql_agent()</code> function. \n",
    "    </p>\n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "    We pass in our <code>llm</code> and <code>db</code> as required parameters and set <code>agent_type</code> to \"zero-shot-react-description\" to instruct the agent to perform a reasoning step before acting.  \n",
    "    </p>\n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "    We set <code>verbose</code> to true so that the agent can output detailed information of intermediate steps. Additionally, we set <code>handle_parsing_errors</code> to <code>True</code>, ensuring that errors are sent back to the LLM as observations, for the LLM to attempt handling the errors.\n",
    "    </p>\n",
    "    \n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    " We can optimize the agents performance with additional prompt engineering. \n",
    "</p>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial;\">\n",
    "We import a <code>ChatPromptTemplate</code> class to build flexible reusable prompts in our agent. Here we define a prefix, format instructions, and a suffix and join them to create a custom prompt. The prefix has unique rules that apply to Teradata. The format guides it's Question, thought, observation behavior and the suffix cues it to begin. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f06e9-9fb2-492d-a679-c3081062d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = (\n",
    "    \"\"\"You are an helpful and expert TeradataSQL database admin. TeradataSQL shares many similarities to SQL, with a few key differences.\n",
    "Given an input question, first create a syntactically correct TeradataSQL query to run, then look at the results of the query and return the answer.\n",
    "Given an input question, create a syntactically correct {dialect} query to run,\n",
    "then look at the results of the query and return the answer. Unless the user\n",
    "specifies a specific number of examples they wish to obtain, always limit your\n",
    "query to at most {top_k} results.\n",
    "\n",
    "IMPORTANT: Unless the user specifies an exact number of rows they wish to obtain, you must always limit your query to at most {top_k} results by using \"SELECT TOP {top_k}\".\n",
    "\n",
    "The following keywords do not exist in TeradataSQL: \n",
    "1. LIMIT \n",
    "2. FETCH\n",
    "3. FIRST\n",
    "Instead of LIMIT or FETCH, use the TOP keyword. The TOP keyword should immediately follow a \"SELECT\" statement.\n",
    "For example, to select the top 3 results, use \"SELECT TOP 3 FROM <table_name>\"\n",
    "Enclose all value identifiers in quotes to prevent errors from restricted keywords. Append an underscore to all alias keywords (e.g., AS count_).\n",
    "Always use double quotation marks (\" \") for column names in SQL queries to avoid syntax errors.\n",
    "NOT make any DML statements (INSERT, UPDATE, DELETE, DROP, etc.) to the database. \n",
    "If the question does not seem related to the database, just return \"I don't know\" as the answer\n",
    "\n",
    "IMPORTANT: Use default database as 'DEMO_ComplaintAnalysis_db'\n",
    "\n",
    "IMPORTANT:Use the following Tables: \\n\n",
    "\"\"\"\n",
    "    + database_schema\n",
    "    + \"\"\"\n",
    "\n",
    "Few examples of Question-SQL Pairs:\n",
    "Question: What is the Total credit balance of customer 456789123?\n",
    "SQL: SELECT TOP 1 \"Total Credit Balance\" FROM DEMO_ComplaintAnalysis_db.Customer_360_Details WHERE \"Customer Identifier\" = '456789123'\n",
    "\n",
    "IMPORTANT: Here are some tips for writing Teradata style queries:\n",
    "\n",
    "Always use table aliases when your SQL statement involves more than one source\n",
    "Aggregated fields like COUNT(*) must be appropriately named \n",
    "Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 3 results by using SELECT TOP 3, note that LIMIT function does not works in Teradata DB.\n",
    "[Best] If the question can be answered with the available tables: {{'sql': <sql here>}} \n",
    "If the question cannot be answered with the available tables: {{'error': <explanation here>}} \n",
    "Remove unnecessary ORDER BY clauses unless required. \n",
    "Remember: Do not use 'LIMIT' or 'FETCH' keyword in the SQLQuery, instead of TOP keyword, For Example: To select top 3 results, use TOP keyword instead of LIMIT or FETCH.  \n",
    "\\nResponse Guidelines: \n",
    "If the provided context is insufficient, please explain why it can't be generated. \n",
    "Most important: Always give property options with details like PropertyID, Property Type, Building Size, Price, Address, Bedroom Count. PropertyID is mandatory in the response.\n",
    "Critical Instruction: Ensure responses are exclusively derived from query results. Refrain from generating or adding synthetic data in any form.\n",
    "Most important: The function should return the relevant answer for the question asked only based on Query results.\n",
    "Given a user's question about this data, write a valid Teradata SQL query that accurately extracts or calculates the requested information from these tables \n",
    "and adheres to SQL best practices for Teradata database, optimizing for readability and performance where applicable. Do not try to make any answer\n",
    "\n",
    "You have access to the following tools:\"\"\"\n",
    ")\n",
    "\n",
    "format_instructions = \"\"\"You must always the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Don't forget to prefix your final answer with the string, \"Final Answer:\"!\"\"\"\n",
    "\n",
    "suffix = \"\"\"Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "custom_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\\n\\n\".join(\n",
    "        [\n",
    "            prefix,\n",
    "            \"{tools}\",\n",
    "            format_instructions,\n",
    "            suffix,\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# print(custom_prompt[0].prompt.template)\n",
    "agent = create_sql_agent(\n",
    "    llm=llm,\n",
    "    db=db,\n",
    "    agent_type=\"zero-shot-react-description\",\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    prompt=custom_prompt,\n",
    "    max_iterations=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5546204-b5f3-430b-805d-c58b28962af7",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>9.4 Setup Hybrid RAG</b></p>  \n",
    "<p style=\"font-size: 16px; font-family: Arial;\">We have source data stored in both VectorDB and Vantage Database. Our hybrid RAG system is designed to automatically identify the appropriate source and query it accordingly. In some cases, responses to certain questions may be derived from both sources.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13949f98-c2c8-4a98-acb2-f51e7a8e6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def _get_classifier(query):\n",
    "    \"\"\"Classify the query type\"\"\"\n",
    "    query_classifier_prompt = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=\"\"\"Classify if the following query requires:\n",
    "            1. SQL database (if it's asking about structured data like Customer_360_Details, Customer Identifier, Name, City, State, Customer Type, Product Holdings, Total Deposit Balance, Total Credit Balance, Total Investments AUM, Customer Profitability,\n",
    "             Customer Lifetime Value, Bank Tenure, Affluence Segment, Digital Banking Segment, Branch Banking Segment.)\n",
    "            2. Vector database (if it's asking about document content, general knowledge, customer complaints)\n",
    "            3. Both (if it needs to combine information from structured data and documents)\n",
    "            \n",
    "            Query: {query}\n",
    "            \n",
    "            Return only one word: SQL, VECTOR, or BOTH\n",
    "            \"\"\",\n",
    "    )\n",
    "    chain = query_classifier_prompt | llm | StrOutputParser()\n",
    "    return chain\n",
    "\n",
    "\n",
    "def _query_vector_store(query_id, query):\n",
    "    \"\"\"Query the vector store and return relevant content\"\"\"\n",
    "    context, file_names, ref_ids = get_similarity_search_context(query_id)\n",
    "\n",
    "    response_prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"query\"],\n",
    "        template=\"\"\"Using the following context, answer the question:\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\",\n",
    "    )\n",
    "\n",
    "    response_chain = response_prompt | llm | StrOutputParser()\n",
    "    response = response_chain.invoke({\"context\": context, \"query\": query})\n",
    "    return {\"response\": response, \"reference\": file_names, \"ref_ids\": ref_ids}\n",
    "\n",
    "\n",
    "def _combine_responses(sql_response, vector_response, query):\n",
    "    \"\"\"Combine responses from SQL and vector stores\"\"\"\n",
    "    combination_prompt = PromptTemplate(\n",
    "        input_variables=[\"sql_response\", \"vector_response\", \"query\"],\n",
    "        template=\"\"\"Combine the following information to provide a complete answer:\n",
    "\n",
    "        SQL Database Info: {sql_response}\n",
    "        Document Info: {vector_response}\n",
    "        Original Question: {query}\n",
    "\n",
    "        Combined Answer:\"\"\",\n",
    "    )\n",
    "\n",
    "    combination_chain = combination_prompt | llm | StrOutputParser()\n",
    "    combined_response = combination_chain.invoke(\n",
    "        {\n",
    "            \"sql_response\": sql_response,\n",
    "            \"vector_response\": vector_response,\n",
    "            \"query\": query,\n",
    "        }\n",
    "    )\n",
    "    return combined_response\n",
    "\n",
    "\n",
    "def process_query(query_id):\n",
    "    \"\"\"\n",
    "    Process user query and return appropriate response\n",
    "    \"\"\"\n",
    "    if type(query_id) != str:\n",
    "        # get query from id\n",
    "        query = tdf_question_embeddings_store[\n",
    "            tdf_question_embeddings_store[\"query_id\"] == query_id\n",
    "        ][[\"txt\"]].get_values()[0][0]\n",
    "\n",
    "        # Classify query type\n",
    "        query_type = _get_classifier(query).invoke(query).strip().upper()\n",
    "        if query_type == \"SQL\":\n",
    "            return {\n",
    "                \"response\": agent.invoke(query)[\"output\"],\n",
    "                \"reference\": [\"Customer_360_Details\"],\n",
    "                \"ref_ids\": [],\n",
    "            }\n",
    "\n",
    "        elif query_type == \"VECTOR\":\n",
    "            return _query_vector_store(query_id, query)\n",
    "\n",
    "        elif query_type == \"BOTH\":\n",
    "            sql_response = agent.invoke(query)\n",
    "            vector_response = _query_vector_store(query_id, query)\n",
    "            return {\n",
    "                \"response\": _combine_responses(\n",
    "                    sql_response, vector_response[\"response\"], query\n",
    "                ),\n",
    "                \"reference\": vector_response[\"reference\"],\n",
    "                \"ref_ids\": vector_response[\"ref_ids\"],\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            return \"Unable to classify query type. Please rephrase your question.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8df1cb-0cf6-48ba-8a3e-94e5987b1fb3",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<a id=\"rule\"></a>\n",
    "<p style = 'font-size:20px;font-family:Arial'><b>10. Test and Compare Results</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>To test and compare our results let's invoke the agent by selecting question from dropdown.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0138427-167c-42e4-8599-406833564f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_template(response):\n",
    "    view = \"\"\"<p style = 'font-size:18px;font-family:Arial;'><b>Here is your response:</b></p>\"\"\"\n",
    "    view = (\n",
    "        view\n",
    "        + f\"\"\"<ul style = 'font-size:16px;font-family:Arial;'>\n",
    "    <li><strong>{response['response']}</strong><ul>\n",
    "    <li>References: \"\"\"\n",
    "    )\n",
    "    for i in response[\"reference\"]:\n",
    "\n",
    "        view = (\n",
    "            view\n",
    "            + f\"\"\"<ul style = 'font-size:16px;font-family:Arial;'><li>{i}</li></ul>\"\"\"\n",
    "        )\n",
    "\n",
    "    view = view + \"\"\"</ul></ul>\"\"\"\n",
    "    return view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41024a54-5c9f-4c26-b147-03e9ab7aff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vectors_2d(question_vector, matching_chunks, doc_chunks, loading_spinner2):\n",
    "    # Combine all vectors for PCA\n",
    "    all_vectors = np.vstack(\n",
    "        [question_vector.reshape(1, -1), matching_chunks, doc_chunks]\n",
    "    )\n",
    "\n",
    "    # Reduce to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    vectors_2d = pca.fit_transform(all_vectors)\n",
    "\n",
    "    # Split back into separate arrays\n",
    "    question_2d = vectors_2d[0]\n",
    "    matching_2d = vectors_2d[1 : len(matching_chunks) + 1]\n",
    "    docs_2d = vectors_2d[len(matching_chunks) + 1 :]\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Plot document chunks\n",
    "    plt.scatter(\n",
    "        docs_2d[:, 0],\n",
    "        docs_2d[:, 1],\n",
    "        c=\"gray\",\n",
    "        alpha=0.5,\n",
    "        label=\"All document embeddings\",\n",
    "    )\n",
    "\n",
    "    # Plot matching chunks\n",
    "    plt.scatter(\n",
    "        matching_2d[:, 0],\n",
    "        matching_2d[:, 1],\n",
    "        c=\"green\",\n",
    "        marker=\"s\",\n",
    "        s=100,\n",
    "        label=\"Matching embeddings\",\n",
    "    )\n",
    "\n",
    "    # Plot question vector\n",
    "    plt.scatter(\n",
    "        question_2d[0],\n",
    "        question_2d[1],\n",
    "        c=\"red\",\n",
    "        marker=\"*\",\n",
    "        s=200,\n",
    "        label=\"Question embeddings\",\n",
    "    )\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.title(\"2D Visualization of Question with matched documents\")\n",
    "    plt.xlabel(\"X-axis\")\n",
    "    plt.ylabel(\"Y-axis\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add arrows from question to matching chunks\n",
    "    for match in matching_2d:\n",
    "        plt.arrow(\n",
    "            question_2d[0],\n",
    "            question_2d[1],\n",
    "            match[0] - question_2d[0],\n",
    "            match[1] - question_2d[1],\n",
    "            color=\"blue\",\n",
    "            alpha=0.3,\n",
    "            head_width=0.001,\n",
    "            head_length=0.001,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    loading_spinner2.value = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f509b29-6753-4323-a934-0f11734f9acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# for setup dropdown\n",
    "loading_spinner3 = widgets.HTML(\n",
    "    value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Please wait while we prepare the question-answer interface and get the answer for first question...\",\n",
    ")\n",
    "\n",
    "pdf_question_embeddings_store = tdf_question_embeddings_store.to_pandas().reset_index()\n",
    "\n",
    "display(loading_spinner3)\n",
    "\n",
    "# set options\n",
    "op = list(\n",
    "    zip(\n",
    "        pdf_question_embeddings_store[['txt']].values.flatten(),\n",
    "        pdf_question_embeddings_store[['query_id']].values.flatten(),\n",
    "    )\n",
    ")\n",
    "\n",
    "prod_dw = Dropdown(\n",
    "    options=op,\n",
    "    description=\"Please select the query:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    display=\"flex\",\n",
    "    flex_flow=\"column\",\n",
    "    align_items=\"stretch\",\n",
    "    layout=widgets.Layout(width='50%', height='50px'),\n",
    "    value=101,\n",
    ")\n",
    "\n",
    "# for 2D plot\n",
    "doc_chunks = tdf_embeddings_store.loc[:, \"emb_0\":\"emb_767\"].get_values()\n",
    "\n",
    "@interact(query_id=prod_dw)\n",
    "def print_product(query_id):\n",
    "    \n",
    "    # for querying\n",
    "    loading_spinner = widgets.HTML(\n",
    "        value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Thinking...\",\n",
    "    )\n",
    "    \n",
    "    # for drawing a plot     \n",
    "    loading_spinner2 = widgets.HTML(\n",
    "        value=\"<i class='fas fa-cog fa-spin' style='font-size:24px'></i> Drawing...\",\n",
    "    )\n",
    "    \n",
    "    if query_id != \"\":\n",
    "        type(query_id)\n",
    "        display(loading_spinner)\n",
    "\n",
    "    response = process_query(query_id)\n",
    "    if response is not None:\n",
    "        loading_spinner.value = \"\"\n",
    "        display(Markdown(response_template(response)))\n",
    "\n",
    "        if len(response[\"ref_ids\"]) > 0:\n",
    "            print(\"Drawing a 2D plot, please wait.\")\n",
    "            display(loading_spinner2)\n",
    "\n",
    "            # filter matching chunks to the query\n",
    "            matching_chunks = (\n",
    "                tdf_embeddings_store.loc[\n",
    "                    tdf_embeddings_store[\"id\"].isin(list(response[\"ref_ids\"]))\n",
    "                ]\n",
    "                .iloc[:, 3:]\n",
    "                .get_values()\n",
    "            )\n",
    "\n",
    "            # get question emb\n",
    "            question_vector = (\n",
    "                tdf_question_embeddings_store.loc[\n",
    "                    tdf_question_embeddings_store[\"query_id\"] == query_id\n",
    "                ]\n",
    "                .iloc[:, 2:]\n",
    "                .get_values()\n",
    "            )\n",
    "\n",
    "            plot_vectors_2d(question_vector, matching_chunks, doc_chunks, loading_spinner2)\n",
    "        loading_spinner3.value = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b9d245-50a1-4ae4-bdb2-d8212950fe39",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;\">\n",
    "<b style = 'font-size:20px;font-family:Arial'>11. Integrated data with customer 360</b>\n",
    "<p style = 'font-size:16px;font-family:Arial'>The following is an example of the output from LLM integrated with existing customer360 data. Please scroll to the right to see all the columns.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ad647-0d71-4317-8904-a30ae9d552b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cusomter360_augmented = complaints_data.to_pandas().reset_index()\n",
    "cusomter360_augmented[\"num_of_question_asked\"] = np.random.randint(\n",
    "    8, 40, size=len(cusomter360_augmented)\n",
    ")\n",
    "cusomter360_augmented[\"types of question\"] = np.random.choice(\n",
    "    [\"insurance\", \"personal banking\"], size=len(cusomter360_augmented)\n",
    ")\n",
    "cusomter360_augmented[\"bank strategy\"] = [\n",
    "    \"Insurance Manager to contact customer immediately\",\n",
    "    \"Send Policy Letter from Insurance Servicing\",\n",
    "    \"Insurance Manager to follow-up with Title Company for documentation and contact customer\",\n",
    "    \"Insurance Manager to contact customer immediately\",\n",
    "    \"All answered by chatbot, no action required\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78573e7-5f4d-46bd-98cd-dda87e1f4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "cusomter360_augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8b817-c0ff-42b3-a651-c8268ac40942",
   "metadata": {},
   "source": [
    "<hr style='height:2px;border:none;'>\n",
    "<b style = 'font-size:20px;font-family:Arial'>11. Cleanup</b>\n",
    "<p style = 'font-size:18px;font-family:Arial'><b>11.1 Work Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>Cleanup work tables to prevent errors next time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed8c3d7-3772-419f-b462-3e8ab8cc5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the list of tables and execute the drop table command for each table\n",
    "for table in [\n",
    "    \"question_to_ask\",\n",
    "    \"question_to_ask_embeddings\",\n",
    "    \"df_matching_chunks\",\n",
    "    \"docs_data\",\n",
    "    \"pdf_embeddings_store\",\n",
    "]:\n",
    "    try:\n",
    "        db_drop_table(table_name=table, schema_name=\"demo_user\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a412d-382e-4215-8ed2-d45367fd0f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "views = [\"v_tokenizer_encode\", \"v_ivsm_score\"]\n",
    "for view in views:\n",
    "    try:\n",
    "        db_drop_view(view_name=view)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d26a8a-23a9-4324-8379-1e6b426ce6e5",
   "metadata": {},
   "source": [
    "<hr style='height:1px;border:none;'>\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial'> <b>11.2 Databases and Tables </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial'>We will use the following code to clean up tables and databases created for this demonstration.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e4803-5b5f-464b-8004-15df96c99d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_ComplaintAnalysis');\"   # Takes 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30521be8-1d18-48b2-a857-d5377ac0e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5463848-592f-4321-b852-287e133872dd",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; border-bottom:3px solid #91A0Ab\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analytics™</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright © Teradata Corporation - 2023. All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
