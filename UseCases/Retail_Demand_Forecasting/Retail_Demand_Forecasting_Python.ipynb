{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c7b2ca-a3a2-4525-aeb3-2276de42e3c2",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Demand Forecasting with In-Database Time Series\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"https://storage.googleapis.com/clearscape_analytics_demo_data/DEMO_Logo/teradata.svg\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>Introduction</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Retail stores rely on sales and an accurate amount of inventory to support these sales. However, demand can be everchanging leading to stores being overstocked or out of stock. To drive sales and control costs, retailers need to be able to adapt to changes fast. But that can be challenging without the right data. Stay ahead of supply and demand fluctuations with VantageCloud and ClearScape Analytics. Our complete cloud analytics and data platform for AI enables you to harmonize and analyze sales and inventory data across all your stores while considering factors like holidays or sudden spikes in demand.</p> \n",
    "\n",
    "\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>Business Value </b></p>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Accurately predict sales over specific time periods. </li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Identify seasonal trends in sales and demand to improve inventory management. </li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Better plan for non-seasonal sales spikes and dips. </li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Drive customer satisfaction and strengthen customer loyalty.</li></p>  \n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>Why Vantage? </b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Unbounded Array Framework (UAF) is the Teradata framework for building end-to-end time series forecasting pipelines. It also provides functions for digital signal processing and 4D spatial analytics. The series can reside in any Teradata supported or Teradata accessible table or in an analytic result table (ART). </p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>UAF provides data scientists with the tools for all phases of forecasting: </p>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Data preparation functions </li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Data exploration functions </li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Model coefficient estimation functions </li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Model validation functions </li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Model scoring functions </li>\n",
    "    </p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Plus, with Teradata Vantage, users can perform these functions at scale and analyze and forecast hundreds/thousands at once. Time Series analysis requires significant effort in analyzing, preparing, and testing forecast models. Traditional approaches require users to perform these laborious tasks multiple times for each prediction, so scaling forecasting efforts beyond a small number of different forecasts becomes prohibitive. The UAF architecture provides a range of unique benefits including: </p>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Rapid data exploration, preparation, and testing functions that can analyze massive amounts of data across an unlimited number of forecasts in parallel; drastically reducing the development and testing times. </li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>The creation of a nearly unlimited number of forecasts in parallel, unlocking value in hyper-segmented (per-store-per-SKU inventory demand, per-household energy consumption) predictions, based on individualized models.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>The ability to deploy the preparation and forecasting functions into automated pipelines that can run in near-real-time, eliminating the gaps between preparation, development, and deployment. \n",
    "</li>\n",
    "\n",
    "\n",
    "<p></p>    \n",
    " \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Hence as a data science consultant, we are showcasing the complete approach about how we can make prediction for the demand for each store. We are demonstrating how we can train our models and use them for scoring using the ClearScape Analytics platform. The data we are using is a sample dataset and the results and predictions may not be entirely accurate.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d3da0-34dd-4d23-8401-c2d817268273",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>1. Connect to Vantage.</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527135cc-6731-4019-8011-62bd08b18ba1",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We start by importing the required libraries and set environment variables and environment paths (if required).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e070e01-e395-4f3f-a661-d63b70d897e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# # '%%capture' suppresses the display of installation steps of the following packages\n",
    "# !pip install tdsense==0.1.3.11\n",
    "# !pip install  tdnpathviz==0.1.2.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a4f2b-da84-4013-9481-f7ef4ef38665",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Note: </b><i>The above statements may need to be uncommented if you run the notebooks on a platform other than ClearScape Analytics Experience that does not have the libraries installed.Â If you uncomment those installs, be sure to restart the kernel after executing those lines to bring the installed libraries into memory. The simplest way to restart the Kernel is by typing zero zero: <b> 0 0</b></i></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f6655-e7d8-45ee-8b73-32ea33151f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import teradataml as tdml\n",
    "from teradataml import * \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import getpass\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from tdsense.plot import plotcurves\n",
    "from tdsense.clustering import resample\n",
    "\n",
    "display.max_rows=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9c2a75-1ce4-4481-8c62-239cb4154244",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will be prompted to provide the password. We will enter the password, press the Enter key, and then use the down arrow to go to the next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c3c50-7ca9-4d24-a29d-8797a490372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa17e5c-756f-4d9d-9985-c3e3d2cebf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_sql('''SET query_band='DEMO=Retail_Demand_Forecasting_Python.ipynb;' UPDATE FOR SESSION; ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18fd14a-95f7-4a3f-a963-3830cf78f1e7",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>2. Getting Data for This Demo </b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We have provided data for this demo on cloud storage. We have the option of either running the demo using foreign tables to access the data without using any storage on our environment or downloading the data to local storage, which may yield somewhat faster execution. However, we need to consider available storage. There are two statements in the following cell, and one is commented out. We may switch which mode we choose by changing the comment string.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389d258-6ef6-4b26-8afb-7ee5e7ebecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ../run_procedure.py \"call get_data('DEMO_DemandForecast_cloud');\"\n",
    " # Takes about 45 seconds\n",
    "%run -i ../run_procedure.py \"call get_data('DEMO_DemandForecast_local');\"\n",
    " # Takes about 70 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c754c-2e1c-40da-8298-b4c89bcebb8f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Optional step â We should execute the below step only if we want to see the status of databases/tables created and space used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94903d-62d7-4d32-8d20-52b25c6ac94f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call space_report();\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223fe73-10a0-4d0d-9fdb-cfcc08653c45",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>3. Analyze Raw Data.</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Let us start by creating a \"Virtual DataFrame\" that points directly to the dataset in Vantage.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d415b0-4606-4d70-888c-d7e2173cb31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(in_schema('DEMO_DemandForecast','Demand_Data'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714207f-d261-4489-b44e-beb618a3959b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The dataset is a retail dataset where we have the timekey(is the lowest granularity column used for our analysis), the Product(MODELID), the Store(MARKET) and the column DEMAND which will be used for analysis. The timekey is the column generated for creating a series for analyzing our data over time period.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae2205-8131-40b3-866b-935f85bdf882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df.select(['timeKey', 'MARKET', 'MODELID'])\n",
    "df_count.count(distinct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77898705-7de6-4519-b6c6-78ca867f5b8f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The dataset contains 6 different Stores and 4106 different Products which we are analyzing over the timeKey generated series having 166 series IDs</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b92a3f0-2002-45aa-bbb6-0cb9b77acb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df_count.groupby('MARKET')\n",
    "df_plot=df2.count(distinct=True).to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04166473-cff2-43c0-8ab2-0e03bc3ff4b7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We can see that the aggregated data is available to us in teradataml dataframe. Let's visualize this data to better understand the count of products by each market. Vantage's Clearscape Analytics can easily integrate with 3rd party visualization tools like Tableau, PowerBI or many python modules available like plotly, seaborn etc. We can do all the calculations and pre-processing on Vantge and pass only the necessary information to visualization tools, this will not only make the calculation faster but also reduce the time due to less data movement between tools. We do the data transfer for this and the subsequent visualizations wherever necessary.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5757c6-63b6-4ea7-a448-3b131d7bac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = 'MARKET',y = 'count_MODELID',data = df_plot)\n",
    "plt.xlabel('MARKET')\n",
    "plt.ylabel('Count of Products')\n",
    "plt.title('Count of Products Sold by each Market')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b271bd4-a38a-4667-86c5-7213d578505c",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>As seen in the above chart , as MARKET01 and MARKET06 do not have much data we will not consider these in further analysis.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will check if the Demand is Zero, and also calculate the duration of the Demand based on the timekey for these Products.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde850a-f49c-49f4-80ad-b594eee10470",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metrics = df[['MODELID','timeKey','DEMAND']]. \\\n",
    "                    assign(demand_is_zero=tdml.sqlalchemy.literal_column('CASE WHEN DEMAND=0 THEN 1 ELSE 0 END')). \\\n",
    "                    groupby('MODELID'). \\\n",
    "                    agg({'timeKey' : ['min','max'], 'demand_is_zero':['sum']}). \\\n",
    "                    assign(duration=tdml.sqlalchemy.literal_column('max_timeKey - min_timeKey')). \\\n",
    "                    select(['MODELID','sum_demand_is_zero','duration']). \\\n",
    "                    assign(ratio = tdml.sqlalchemy.literal_column('CAST(sum_demand_is_zero AS FLOAT) / NULLIFZERO(duration)'))\n",
    "\n",
    "dataset_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1fb7a-353e-4c62-9bf4-1287da02a2d7",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will check only those Series where the duration(difference between the max timekey and min timekey for each Product) is greater than 30 and MATERIAL less than 300. To fit data in a seasonal model, we would like to consider only series with at least 2 years  (24 months). So, considering duration greater than 30.</p>\n",
    "\n",
    "<p style = 'font-size:14px;font-family:Arial'><i>Material less than 300 is considered only to show lesser data in plots for better understanding </i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a932f90-c8fe-4efb-bb22-af2c2dc2d9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = df.join(other=dataset_metrics, on='MODELID', how='inner', lsuffix='l', rsuffix='r').assign(MODELID=tdml.sqlalchemy.literal_column('MODELID_l')).drop(columns=['MODELID_l','MODELID_r'])\n",
    "dataset = dataset[dataset.duration > 30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10edb3e5-7963-491b-ab73-25473d01bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnpathviz.visualizations import plotcurves\n",
    "plotcurves(dataset[dataset.MATERIAL < 300],field='DEMAND',row_axis='timeKey', series_id='MODELID',row_axis_type='sequence',plot_type='line',\n",
    "           legend='best',width=1800,height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827f7d6-8fd0-4c44-bf22-dece382fea0e",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The above graph shows the Demand for each Product(MODELID) along the timekey axis.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>One of our strengths is that we can run thousands of models concurrently, however for the purposes of this demo, we will arbitrarily choose 3 products so you can follow the process. The Product selection is random. </p>\n",
    "<p style = 'font-size:12px;font-family:Arial'><i>** You can change the products(MODELIDs) in the below cell in case you want to see the output for other Products. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc84c85-f56f-4b14-b759-8adb61332b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELID_1 = 'MARKET0412164'\n",
    "MODELID_2 = 'MARKET0412341'\n",
    "MODELID_3 = 'MARKET0205595'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa4bb62-f7ab-4476-863f-d8ffa0118db5",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial'>We will check the Demand for these Products</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8889d-fb90-4025-a17e-b0bac04a73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts_1 = df.loc[df.MODELID == MODELID_1,['timeKey','DEMAND']].sort('timeKey')#.to_pandas().set_index('timeKey')\n",
    "df_ts_2 = df.loc[df.MODELID == MODELID_2,['timeKey','DEMAND']].sort('timeKey')#.to_pandas().set_index('timeKey')\n",
    "df_ts_3 = df.loc[df.MODELID == MODELID_3,['timeKey','DEMAND']].sort('timeKey')#.to_pandas().set_index('timeKey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb79679-688f-41ec-80ff-8535d0a1e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = subplots(nrows=1, ncols=3)\n",
    "fig.height,fig.width = 400,1200 \n",
    "plot = df_ts_1.plot(x=df_ts_1.timeKey, y=df_ts_1.DEMAND,\n",
    "                          ax=axes[0],\n",
    "                          figure=fig, kind=\"line\",\n",
    "                          title=\"MARKET0412164\", style=\"blue\")\n",
    " \n",
    "plot = df_ts_2.plot(x=df_ts_2.timeKey, y=df_ts_2.DEMAND,\n",
    "                          ax=axes[1],\n",
    "                          figure=fig, kind=\"line\",\n",
    "                          title=\"MARKET0412341\", style=\"blue\")\n",
    " \n",
    "plot = df_ts_3.plot(x=df_ts_3.timeKey, y=df_ts_3.DEMAND,\n",
    "                          ax=axes[2], \n",
    "                          figure=fig, kind=\"line\",\n",
    "                          title=\"MARKET0205595\", style=\"blue\")\n",
    " \n",
    "# Display the plot.\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179702f-a003-47d1-b65c-fb3e2cbf85f4",
   "metadata": {},
   "source": [
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We use the window function on the timekey column to build a series for the Demands for different ModelIDs. We use this function to count the series length and filter out timeseries that are too short for ARIMA.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7ccaf-672e-4e4f-9b62-5deeffce5642",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_for_counting = dataset.timeKey.window(\n",
    "                            partition_columns   = \"MODELID\",\n",
    "                            order_columns       = 'timeKey'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d4f4f1-c010-4b74-b874-70d00ff62300",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = dataset.assign(series_length = window_for_counting.count(),\n",
    "                             nb_zeros = tdml.sqlalchemy.literal_column('SUM(CASE WHEN DEMAND = 1 THEN 1 ELSE 0 END) OVER (PARTITION BY MODELID)'),\n",
    "                             frac_zeros = tdml.sqlalchemy.literal_column('CAST((SUM(CASE WHEN DEMAND = 0 THEN 1 ELSE 0 END) OVER (PARTITION BY MODELID)) AS FLOAT)/series_length'),\n",
    "                             fold = tdml.sqlalchemy.literal_column(\"CASE WHEN timeKey < 0.67*series_length + (min(timeKey) OVER (PARTITION BY MODELID)) THEN 'train' ELSE 'test' END\"),\n",
    "                             time_no_unit = tdml.sqlalchemy.literal_column(\"timeKey-(min(timeKey) OVER (PARTITION BY MODELID))\")\n",
    "                            )\n",
    "dataset_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee3c58-50a8-4af2-ac21-be995e028ffc",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We use the subset of data where the series length is greater than 90 and the ratio of zero demand and series length is less than 0.1, which will filter out the Markets which show almost zero Demand (Market01 and Market06).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d2ba0-a5bb-4818-8bc5-8d3ec8f4814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset_new[(dataset_new.series_length > 90)&(dataset_new.frac_zeros < 0.1)]\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff06e4-85ae-4193-9879-ac54dcd670ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918b4fb-d432-4732-ac89-55525815d322",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>So, the dataset we are using for our analysis has around 46k rows and 21 columns.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed8e73-e9f2-454d-88b8-5c67ee87aef9",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>4. Checking for Stationarity of Time Series using the Dickey Fuller Test</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf337f98-70d1-4b0e-8ea2-dd3ba147757a",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To be able to model a time series, it needs to be stationary(Stationarity is a property of a time series where the statistical properties of the series do not change over time. In other words, a stationary time series exhibits constant mean, constant variance, and constant covariance (or autocovariance) over different time periods.). ARIMA models deal with non-stationary time series by differencing (The \"d' parameter in ARIMA determines the number of differences needed to make a series stationary)</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here we will check for stationarity of the time series using the Dickey-Fuller Test. For more info on the test,  see <a href=\"https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/Teradata-VantageTM-Unbounded-Array-Framework-Time-Series-Reference-17.20/Diagnostic-Statistical-Test-Functions/TD_DICKEY_FULLER/TD_DICKEY_FULLER-Example\">here.</a> \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The null hypothesis for the test is that the data is non-stationary. We want to REJECT the null hypothesis for this test. So, we want a p-value of less than 0.05 (or smaller) and a negative coefficient value for the lag term in our regression model.</p> \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The Dickey fuller function needs series data so we use the TDSeries function to create a series and apply DickeyFuller to check the stationarity of the data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f74d5-a88f-4b9c-81dc-29e6d47bc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create teradataml TDSeries object.\n",
    "data_series_df = tdml.TDSeries(data=subset,\n",
    "                          id=\"MODELID\",\n",
    "                          row_index=\"time_no_unit\",\n",
    "                          row_index_style=\"SEQUENCE\",\n",
    "                          payload_field=\"DEMAND\",\n",
    "                          payload_content=\"REAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b1994d-006e-4ba6-8a07-6a2cc62b7159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from teradataml import DickeyFuller\n",
    "df_out = DickeyFuller(   data=data_series_df,\n",
    "                           algorithm='NONE')\n",
    "\n",
    "# Print the result DataFrame.\n",
    "print(df_out.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b2f94-5022-41a5-b950-2646431cfb52",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In the above output the p-value corresponding to the calculated test statistic is less than 0.05. It means that the series is stationary. The output column NULL_HYP which means NULL HYPOTHESIS can have 2 values \n",
    "    <li style = 'font-size:16px;font-family:Arial'>ACCEPT means the null hypothesis is accepted. No Unit roots are present, and therefore the process is stationary.</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>REJECT means the null hypothesis is rejected. Unit roots are present, and the process may or may not be stationary, depending on other factors.</li>\n",
    "</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Since the P_VALUE is less than 0.05 we consider the series and stationary.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80890992-f8a7-47a3-87ec-3b3f399372b8",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>5. Autocorrelation and Partial Autocorrelation of the time series</b></p>\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>5.1 Check for Autocorrelation of the time series</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>ACF calculates the autocorrelation or autocovariance of a time series. The autocorrelation and autocovariance show how the time series correlates or covaries with itself when delayed by a lag in time or space. Here we check autocorrelation with a maximum lag of 10 time steps.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce9923-8586-4f3b-9f97-d7585b47b2af",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>First we use the Series created above to get the ACF and PACF.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8cb5c-bf6b-4a0d-9cbe-c7edbf7440cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import ACF, PACF\n",
    "uaf_out = ACF(data=data_series_df,\n",
    "                  max_lags=12,\n",
    "              demean=True,\n",
    "              qstat = False,\n",
    "             alpha=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81780e3d-b55d-4667-bbf7-487b76920e43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uaf_out.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb685a4-7d12-4e4b-b7d6-a60a493d417b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The ACF() function calculates the autocorrelation or autocovariance of a time series. The autocorrelation and autocovariance show how the time series correlates or covaries with itself when delayed by a lag in time or space.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In the above output: </p> \n",
    "    <li style = 'font-size:16px;font-family:Arial'>ROW_I :- index of the series.</li>\n",
    "    <li style = 'font-size:16px;font-family:Arial'>CONF_OFF :- Confidence bands in accordance with Bartlettâs formula.</li>\n",
    "    <li style = 'font-size:16px;font-family:Arial'>CONF_LOW :- Confidence bands in accordance with Bartlettâs formula (Lower limit).</li>\n",
    "    <li style = 'font-size:16px;font-family:Arial'>CONF_HI :- Confidence bands in accordance with Bartlettâs formula (Higher limit).</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a678260-f071-417d-a1e4-ee73328adf38",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>5.2. Check for partial autocorrelation of the time series</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The PACF function provides insight as to whether the modelled function is stationary. The partial autocorrelations measure the degree of correlation between time series sample points. Here we check partial autocorrelation with a maximum lag of 10 time steps.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e4538-ea17-49fd-81b1-f465ba59417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PACF_out = PACF(data=data_series_df,\n",
    "                    algorithm='LEVINSON_DURBIN',\n",
    "                    max_lags=12,\n",
    "             alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86660cda-fdc2-4514-adf4-70a4f950badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PACF_out.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22454b44-d86c-4150-abe3-a09c663ec6dc",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The PACF() function provides insight as to whether the function being modeled is stationary or not. The partial auto correlations are used to measure the degree of correlation between series sample points. The algorithm removes the effects of the previous lag.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In the above output: </p> \n",
    "    <li style = 'font-size:16px;font-family:Arial'>ROW_I :- index of the series.</li>\n",
    "    <li style = 'font-size:16px;font-family:Arial'>CONF_OFF :- Confidence bands in accordance with Bartlettâs formula.</li>\n",
    "    <li style = 'font-size:16px;font-family:Arial'>CONF_LOW :- Confidence bands in accordance with Bartlettâs formula (Lower limit).</li>\n",
    "    <li style = 'font-size:16px;font-family:Arial'>CONF_HI :- Confidence bands in accordance with Bartlettâs formula (Higher limit).</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67515454-0011-4ca2-98c9-f7e56d22a8d4",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>5.3. Plot graphs for ACF and PACF of the time series</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We plot the ACF and PACF graphs for all the 3 series we are considering in our analysis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7a53f-4108-49ee-966b-3e078110b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acf=uaf_out.result\n",
    "df_pacf=PACF_out.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7caf37-c3b5-4c8f-93b6-e742eb20410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acf_plot = df_acf.loc[df_acf.MODELID == MODELID_3]\n",
    "df_pacf_plot = df_pacf.loc[df_pacf.MODELID == MODELID_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954ce84-6ac1-4127-8465-08bfd88a8baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = subplots(nrows=2, ncols=1)\n",
    " \n",
    "plot = df_acf_plot.plot(x=df_acf_plot.ROW_I, \n",
    "        y=(df_acf_plot.OUT_DEMAND, df_acf_plot.CONF_OFF_DEMAND),\n",
    "        kind='corr', figsize=(600,400),ylabel = \" \",\n",
    "        color=\"blue\",title=\"Auto Correlation\")\n",
    "plot.show()\n",
    "plot = df_pacf_plot.plot(x=df_pacf_plot.ROW_I, \n",
    "        y=(df_pacf_plot.OUT_DEMAND, df_pacf_plot.CONF_OFF_DEMAND),\n",
    "        kind='corr',figsize=(600,400),ylabel = \" \",\n",
    "        color=\"blue\",title=\"Partial Auto Correlation\")\n",
    "\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0798849-1614-4cf5-ac49-be4c35e6eeec",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To get the value of the Moving Average or Q, we need the lag(here, ROW_I is the X axis) where the value from the ACF plot is outside the significant limit above the zero line. Looking at the graph, the Auto-Correlation value at ROW_I = 3 is outside the confidence band and much closer to it. Hence it is acceptable to say that the value of the Moving Average or <b>Q = 2</b>.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>To get the value of Auto-Regressive lags or P, we need the lag(here, Row_I) where the value from the PACF plot falls just outside the significant limit. Looking at the graph, the Partial Auto-Correlation value at ROW_I = 1 falls way outside the significant limit of the confidence band so here we will consider the value as zero. Hence we can say that the value of Auto-Regressive lags or <b>P = 0</b>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e1931-ebbd-4be7-be38-8bacd3064b76",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>6. Using ARIMA (AutoRegressive Integrated Moving Average) model to forecast Demand</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>ARIMA functions on VANTAGE run in the following order:</p>\n",
    "\n",
    "<li style = 'font-size:16px;font-family:Arial'>Run <b>ARIMAESTIMATE</b> function to get the coefficients for the ARIMA model.\n",
    "</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'><i>[Optional]</i> Run <b>ARIMAVALIDATE</b> function to validate the \"goodness of fit\" of the ARIMA model, when FIT_PERCENTAGE is not 100 in ARIMAESTIMATE.\n",
    "</li>\n",
    "<li style = 'font-size:16px;font-family:Arial'>Run the <b>ARIMAFORECAST</b> function with input from step 1 or step 2 to forecast the future periods beyond the last observed period.</li>\n",
    "</p>\n",
    "\n",
    "\n",
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>6.1 Estimation step using ARIMAESTIMATE</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The ARIMAESTIMATE function estimates the coefficients corresponding to an ARIMA model and fits a series with an existing ARIMA model. The function can also provide the \"goodness of fit\" and the residuals of the fitting operation. The function generates a model layer used as input for the ARIMAVALIDATE and ARIMAFORECAST functions. This function is for univariate series.</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Here, the previously estimated parameters P, d and Q need to be passed in the MODEL_ORDER(P, d, Q), i.e. <b>MODEL_ORDER(0, 1, 2)</b>. The output is stored in a dataframe. The fit percentage is 80, meaning the ARIMA model is being trained on 80% of the data. The remaining 20% of the data will be used to validate the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8feba-644e-4db4-9c15-a18da1887658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import ArimaEstimate,ArimaValidate, ArimaForecast, TDAnalyticResult\n",
    "arima_estimate_op = ArimaEstimate(data1=data_series_df,\n",
    "                                       nonseasonal_model_order=[0,1,2],\n",
    "                                       seasonal_period=12,\n",
    "                                       seasonal_model_order=[1,1,1], \n",
    "                                       constant=False,\n",
    "                                       algorithm=\"MLE\",\n",
    "                                       coeff_stats=True,\n",
    "                                       fit_metrics=True,\n",
    "                                       residuals=True,\n",
    "                                       fit_percentage=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc98d38-81f0-4f26-bb3b-e4be7c9af38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_estimate = arima_estimate_op.fitresiduals\n",
    "results_estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f59402-94b0-4530-8fe7-d30a7fcf8ccc",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The ArimaEstimate() function estimates the coefficients corresponding to an ARIMA (AutoRegressive Integrated Moving Average) model, and to fit a series with an existing ARIMA model.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In the above output: </p> \n",
    "    <li style = 'font-size:16px;font-family:Arial'>ROW_I :- Indexing column for the one dimensional multivariate output array containing the residuals. It is incremented by 1 for each row, starting from 1.</li>\n",
    "    <li style = 'font-size:16px;font-family:Arial'>ACTUAL_VALUE :- The actual value of the response variable.</li>\n",
    "    <li style = 'font-size:16px;font-family:Arial'>CALC_VALUE :- The calculated value of the response variable using the model.</li>\n",
    "    <li style = 'font-size:16px;font-family:Arial'>RESIDUAL :- The difference between the calculated response value and the actual response value.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c92f98-ed41-4be6-8f47-f5c2305e9201",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>6.2 Validate using ArimaValidate</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The ArimaValidate() function performs an in-sample     forecast for both seasonal and non-seasonal auto-regressive (AR), moving-average (MA), ARIMA models and Box-Jenkins seasonal ARIMA model formula followed by an analysis of the produced residuals. The aim is to provide a collection of metrics useful to select the model and expose the produced residuals such that multiple model validation and statistical tests can be conducted.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The TDAnalyticResult function retrieves auxiliary result sets stored in the output dataframe of the ArimaEstimate. Here we extract the residuals from the previous estimation step. Analytical Result Tables have multiple layers that store different data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f7ca54-d2bf-4cda-b54d-a2aa3e24536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_art_df = tdml.TDAnalyticResult(data=arima_estimate_op.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8409e0e4-7911-4a88-920c-cd4ff98d1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_validate_op = ArimaValidate(data=data_art_df, fit_metrics=True, residuals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a65cc2-2b50-41c1-8cef-3dfb59b0658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_validate_op.result.sort('AIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54257d0f-d6a0-48ea-b5ce-58ab9ce2c0f0",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The ArimaValidate function produces a multilayer output and returns up to four result\n",
    "sets (layers).</p>\n",
    "<li style = 'font-size:14px;font-family:Arial;color:#00233C'>Primary layer contains the model selection metrics.</li>\n",
    "<li style = 'font-size:14px;font-family:Arial;color:#00233C'>Secondary layer contains the goodness-of-fit metrics.</li>\n",
    "<li style = 'font-size:14px;font-family:Arial;color:#00233C'>Tertiary layer contains the residuals from the validation procedure.</li>\n",
    "<li style = 'font-size:14px;font-family:Arial;color:#00233C'>Quaternary layer contains the model context, which can be used for forecasting with the model.</li>\n",
    "<br>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In the above output:    <li style = 'font-size:14px;font-family:Arial'>ROW_I :- index of the series.</li>\n",
    "    <li style = 'font-size:14px;font-family:Arial'>NUM_SAMPLES :- Total number of sample points found in each of the original, calculated,\n",
    "and residual series.</li>\n",
    "    <li style = 'font-size:14px;font-family:Arial'>VAR_COUNT :- Integer Total number of parameters involved in the model. For an ARMA(p,q) model, the calculation of VAR_COUNT is p + q + 1.</li>\n",
    "    <li style = 'font-size:14px;font-family:Arial;color:#00233C'>AIC :- The calculated Akaike Information Criteria value.</li>\n",
    "    <li style = 'font-size:14px;font-family:Arial;color:#00233C'>SBIC :- The calculated Schwarz Bayesian Information Criteria value.</li>\n",
    "    <li style = 'font-size:14px;font-family:Arial;color:#00233C'>HQIC :- The calculated Hannon Quinn Information Criteria value.</li>\n",
    "    <li style = 'font-size:14px;font-family:Arial;color:#00233C'>MLR :- The calculated Maximum Likelihood Rule value.</li>\n",
    "    <li style = 'font-size:14px;font-family:Arial;color:#00233C'>MSE :- The calculated Mean Square Error value.</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0391cf92-d8b0-4c5f-862c-1f7964c079e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_validate = arima_validate_op.fitresiduals\n",
    "results_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d1cc5-7fd9-4cbc-b828-15bf8a6ebc0f",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We plot the actual vs calculated values for the 3 different Products(MODELIDs) we are analyzing.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa0b85-c1e3-4770-b435-d3fb848e1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = results_validate[results_validate.MODELID == MODELID_1].sort('ROW_I')#.to_pandas()\n",
    "res2 = results_estimate[results_estimate.MODELID == MODELID_1].sort('ROW_I')#.to_pandas()\n",
    "res3 = subset[subset.MODELID == MODELID_1][['MODELID','time_no_unit','DEMAND']].sort('time_no_unit')\n",
    "res3 = res3.assign(drop_columns=True, MODELID = res3.MODELID, ROW_I = res3.time_no_unit, DEMAND = res3.DEMAND)\n",
    "val1=res1.get(\"ROW_I\").iloc[0].get_values()\n",
    "val2 = res2.get(\"ROW_I\").iloc[res2.shape[0]-1].get_values()\n",
    "res1 = res1.assign(drop_columns=True, ROW_I = res1.ROW_I-val1[0][0]+val2[0][0]+1 ,CALC_VALUE = res1.CALC_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8378b9-8ecb-444c-94b4-805714d5cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "res4 = results_validate[results_validate.MODELID == MODELID_2].sort('ROW_I')#.to_pandas()\n",
    "res5 = results_estimate[results_estimate.MODELID == MODELID_2].sort('ROW_I')#.to_pandas()\n",
    "res6 = subset[subset.MODELID == MODELID_2][['MODELID','time_no_unit','DEMAND']].sort('time_no_unit')\n",
    "res6 = res6.assign(drop_columns=True, MODELID = res6.MODELID, ROW_I = res6.time_no_unit, DEMAND = res6.DEMAND)\n",
    "val3=res4.get(\"ROW_I\").iloc[0].get_values()\n",
    "val4 = res5.get(\"ROW_I\").iloc[res5.shape[0]-1].get_values()\n",
    "res4 = res4.assign(drop_columns=True, ROW_I = res4.ROW_I-val3[0][0]+val4[0][0]+1 ,CALC_VALUE = res4.CALC_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691a2a8-bf0f-4eea-a7f6-59750e55d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "res7 = results_validate[results_validate.MODELID == MODELID_3].sort('ROW_I')#.to_pandas()\n",
    "res8 = results_estimate[results_estimate.MODELID == MODELID_3].sort('ROW_I')#.to_pandas()\n",
    "res9 = subset[subset.MODELID == MODELID_3][['MODELID','time_no_unit','DEMAND']].sort('time_no_unit')\n",
    "res9 = res9.assign(drop_columns=True, MODELID = res9.MODELID, ROW_I = res9.time_no_unit, DEMAND = res9.DEMAND)\n",
    "val5=res7.get(\"ROW_I\").iloc[0].get_values()\n",
    "val6 = res8.get(\"ROW_I\").iloc[res8.shape[0]-1].get_values()\n",
    "res7 = res7.assign(drop_columns=True, ROW_I = res7.ROW_I-val5[0][0]+val6[0][0]+1 ,CALC_VALUE = res7.CALC_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37848c8-3e27-4693-9cd4-6c5d3e88414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure = Figure(width=800, height=400,  heading=\"Actual vs Predicted Demand\")\n",
    "fig, axes = subplots(nrows=3, ncols=1)\n",
    "fig.height,fig.width = 800,1000\n",
    "plot = res3.plot(\n",
    "                x=res3.ROW_I,\n",
    "                y=[res1.CALC_VALUE, res2.CALC_VALUE, res3.DEMAND],\n",
    "                ax=axes[0],\n",
    "                figure=fig,\n",
    "                \n",
    "                xlabel='Time',\n",
    "                ylabel='Demand',\n",
    "                grid_linestyle='--',\n",
    "                grid_linewidth=0.5,\n",
    "                marker=[\"o\",\"s\"]\n",
    ")\n",
    "\n",
    "plot = res6.plot(\n",
    "                x=res6.ROW_I,\n",
    "                y=[res4.CALC_VALUE, res5.CALC_VALUE, res6.DEMAND],\n",
    "                ax=axes[1],\n",
    "                figure=fig,\n",
    "                xlabel='Time',\n",
    "                ylabel='Demand',\n",
    "                grid_linestyle='--',\n",
    "                grid_linewidth=0.5,\n",
    "                marker=[\"o\",\"s\"]\n",
    ")\n",
    "\n",
    "plot = res9.plot(\n",
    "                x=res9.ROW_I,\n",
    "                y=[res7.CALC_VALUE, res8.CALC_VALUE, res9.DEMAND],\n",
    "                ax=axes[2],\n",
    "                figure=fig,\n",
    "                xlabel='Time',\n",
    "                ylabel='Demand',\n",
    "                grid_linestyle='--',\n",
    "                grid_linewidth=0.5,\n",
    "                marker=[\"o\",\"s\"]\n",
    ")\n",
    "\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65166abb-9e31-48cc-ab19-751ba47195b5",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The above graphs show the <b>Actual Demand Values(Green)</b> and the Calculated Values for the Demand using the <b>ArimaEstimate(Orange)</b>, which is the train dataset(80%) as specified in the ArimaEstimate and <b>ArimaValidate(Blue)</b>, which is the test dataset(remaining 20%)</b>. The 3 graphs are for the 3 Products(MODELIDs).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29847e03-39f8-4792-b34a-269197e76e2f",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>6.3 Forecast Demand using ArimaForecast</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The ArimaForecast() function is used to forecast a user-defined number of periods based on\n",
    "    models fitted from the ArimaEstimate() function.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The ArimaForecast() function with input from step 1 or step 2 to forecast the future periods beyond the last observed period. Here we are forecasting for 20 periods.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804bf238-c0d2-45eb-92fe-45f8674a6c19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_art_df = TDAnalyticResult(data=arima_validate_op.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931335b-23e8-4c9e-bd3e-52086e9f5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_forecast_op = ArimaForecast(data=data_art_df, forecast_periods=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d84f9-d29f-4c96-aa6d-ad7e0790eecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_forecast = arima_forecast_op.result\n",
    "results_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd06092-7de0-4a80-a010-da0549717734",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>This function outputs a result set that contains the forecasted values.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In the above output:    \n",
    "    <li style = 'font-size:14px;font-family:Arial;color:#00233C'>ROW_I :- index of the series.</li>\n",
    "    <li style = 'font-size:14px;font-family:Arial;color:#00233C'>FORECAST_VALUE :- Forecasted values for the model.</li>\n",
    "    <li style = 'font-size:14px;font-family:Arial;color:#00233C'>LO_80 :- Low end of the 80% prediction interval.</li>\n",
    "    <li style = 'font-size:14px;font-family:Arial;color:#00233C'>HI_80 :- High end of the 80% prediction interval.</li>\n",
    "    <li style = 'font-size:14px;font-family:Arial;color:#00233C'>LO_95 :- Low end of the 95% prediction interval.</li>\n",
    "    <li style = 'font-size:14px;font-family:Arial;color:#00233C'>HI_95 :- High end of the 95% prediction interval.</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3bbff-aad3-41ac-9a95-8d8ac29517fe",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We can see that the ARIMA estimation, validation and forecasting is done using Teradata functions in a teradataml dataframe. Let's visualize this data to better understand the relation between the actual demand and forecasted values for the demand. Vantage's Clearscape Analytics can easily integrate with 3rd party visualization tools like Tableau, PowerBI or many python modules available like plotly, seaborn etc. We can do all the calculations and pre-processing on Vantge and pass only the necessary information to visualization tools, this will not only make the calculation faster but also reduce the time due to less data movement between tools. We do the data transfer for this and the subsequent visualizations wherever necessary.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We plot the actual vs forecast values for the 3 different Products(MODELIDs) we are analyzing.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9fe5e-2c8e-41a6-bcaa-f30919ed31e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(MODELID):\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    # Plot prediction\n",
    "    mean_forecast = results_forecast[results_forecast.MODELID==MODELID].sort('ROW_I').to_pandas()\n",
    "    res3 = subset[subset.MODELID == MODELID][['MODELID','time_no_unit','DEMAND']].sort('time_no_unit').to_pandas()\n",
    "    res3['time_no_unit'] = res3['time_no_unit'] - res3.time_no_unit.values[-1]\n",
    "    res3.plot(x='time_no_unit',y='DEMAND',label='actual',ax=ax)\n",
    "    mean_forecast.plot(x='ROW_I',y='FORECAST_VALUE',label='forecast',color='red',ax=ax)\n",
    "    # Shade uncertainty area\n",
    "    plt.fill_between(mean_forecast.ROW_I, mean_forecast.LO_80, mean_forecast.HI_80, color='pink', alpha=0.5)\n",
    "    plt.fill_between(mean_forecast.ROW_I, mean_forecast.LO_95, mean_forecast.HI_95, color='pink', alpha=0.2)\n",
    "    plt.title(MODELID)\n",
    "    plt.show()\n",
    "    return\n",
    "plot_forecast(MODELID_1)\n",
    "plot_forecast(MODELID_2)\n",
    "plot_forecast(MODELID_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba1d9de-f015-47ab-9298-04b756016db9",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>The plot in pink color shows the forecasted values for each Product(MODELID) for the 20 periods we have specified in the ArimaForecast.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4342cc-9661-48bf-ad96-b2057f8b359d",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>Conclusion:</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We have trained and validated the ARIMA model on the Weekly Sales dataset, and the results closely match the actual data. The goodness of fit metrics calculated in the estimate and validate phase also resonate with our understanding that the model is well-trained to forecast. This can be observed in the Estimate and the Validate function graphs. So, we can say that the model is well trained to forecast the Weekly Sales.</p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Thus, with the Teradata VantageCloud, we are able to build a powerful end-to-end forecasting pipelines. Tools for each forecasting phase, from data preparation and exploration to model validation and scoring, empower you to forecast more efficiently and at scale with lesser development and testing times and later deploy forecasting functions into automated pipelines to run in near real-time.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b3987-5668-4706-a7a3-5af33cfa1896",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>7. Cleanup</b></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf292234-0cdb-43a4-a370-f8039cf5f39b",
   "metadata": {},
   "source": [
    "<p style = 'font-size:18px;font-family:Arial;color:#00233C'><b>Databases and Tables</b></p>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>We will use the following code to clean up tables and databases created for this demonstration.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333849a1-b510-404b-ad01-8a2e41defeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../run_procedure.py \"call remove_data('DEMO_DemandForecast');\" \n",
    "#Takes 45 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418001cb-0540-4836-9a34-35cc910aa2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b8161-a419-4e7a-9591-8c8c260a6084",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;border:none;background-color:#00233C;\">\n",
    "<b style = 'font-size:18px;font-family:Arial;color:#00233C'>Required Materials</b>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Letâs look at the elements we have available for reference for this notebook:</p>\n",
    "<b style = 'font-size:18px;font-family:Arial;color:#00233C'>Filters: </b>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Industry:</b> Retail </li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Functionality:</b> ARIMA Estimation and Forecasting </li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Use Case:</b> Retail Demand Forecast </li> </p>\n",
    "<b style = 'font-size:18px;font-family:Arial;color:#00233C'>Related Resources: </b>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'><a href = 'https://www.teradata.com/Blogs/NPS-is-a-metric-not-the-goal'>In the fight to improve customer experience, NPS is a metric, not the goal</a></li> \n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'><a href = 'https://www.teradata.com/Blogs/Hyper-scale-time-series-forecasting-done-right'>Hyper-scale time series forecasting done right </a></li>\n",
    "<li style = 'font-size:16px;font-family:Arial;color:#00233C'><a href = 'https://www.teradata.com/Blogs/Crystal-Ball-or-Black-Box-in-Retail-and-CPG'>Crystal Ball, Black Box or Advanced Forecasting and Demand Planning in Retail and CPG</a></li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f134d-77d6-4a62-be43-51bb64920882",
   "metadata": {},
   "source": [
    "<footer style=\"padding-bottom:35px; background:#f9f9f9; border-bottom:3px solid #00233C\">\n",
    "    <div style=\"float:left;margin-top:14px\">ClearScape Analyticsâ¢</div>\n",
    "    <div style=\"float:right;\">\n",
    "        <div style=\"float:left; margin-top:14px\">\n",
    "            Copyright Â© Teradata Corporation - 2023,2024 All Rights Reserved\n",
    "        </div>\n",
    "    </div>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
