{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262f8d25",
   "metadata": {},
   "source": [
    "<header style=\"padding:1px;background:#f9f9f9;border-top:3px solid #00b2b1\"><img id=\"Teradata-logo\" src=\"https://www.teradata.com/Teradata/Images/Rebrand/Teradata_logo-two_color.png\" alt=\"Teradata\" width=\"220\" align=\"right\" />\n",
    "\n",
    "<b style = 'font-size:28px;font-family:Arial;color:#E37C4D'>ModelOps demo - Python H2O AutoML using Git</b>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8b9a2-99c7-4013-9a8e-9e4e47b52e0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "![image](images/git_meth.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea796a98",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook will cover the Operationalization of the PIMA diabetes use case with H2O model format. **H2O** is an open source, distributed in-memory machine learning library with linear scalability. H2O supports the most widely used statistical & machine learning algorithms including gradient boosted machines, generalized linear models, deep learning and more. The most used capabilities in H2O are the **AutoML** capabilities that provides. \n",
    "\n",
    "In this example, we will use the AutoML capabilities to generate a H2O xgboost and operationalize it through ModelOps in the same Model Catalog than other trained models based on other libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e603c",
   "metadata": {},
   "source": [
    "## Steps in this Notebook\n",
    "\n",
    "<li>1. Configure the Environment </li>\n",
    "    <li>2. Connect to Vantage</li>\n",
    "    <li>3. Define Training function </li>\n",
    "    <li>4. Define Evaluate function </li>\n",
    "    <li>5. Define Scoring function</li>\n",
    "    <li>6. Define Model Metadata</li>\n",
    "    <li>7. Commit and Push to Git to let ModelOps manage</li>\n",
    "    <li>8. ModelOps full lifecycle till deployment</li>\n",
    "    <li>9. ModelOps Monitoring</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aec107",
   "metadata": {},
   "source": [
    "## Step 1. Configure the Environment\n",
    "\n",
    "Here, we import the required libraries, set environment variables and environment paths (if required)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012151e5",
   "metadata": {},
   "source": [
    "### 1.1 Libraries installation\n",
    "\n",
    "A restart of the Kernel is needed to confirm changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q teradataml==17.20.0.3 aoa==7.0.1 pandas==1.1.5 scikit-learn==0.24.2 h2o==3.40.0.4 matplotlib==3.5.2 install-jdk==1.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2614ad3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ab039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teradataml import (\n",
    "    create_context, \n",
    "    remove_context,\n",
    "    get_context,\n",
    "    get_connection,\n",
    "    DataFrame,\n",
    "    configure\n",
    ")\n",
    "import os\n",
    "import getpass\n",
    "import logging\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100eb04",
   "metadata": {},
   "source": [
    "## Step 2. Connect to Vantage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a837e73d-d31b-4924-ab4e-4b3ae2c88395",
   "metadata": {},
   "source": [
    "<p style = 'font-size:16px;font-family:Arial'>You will be prompted to provide the password. Enter your password, press the Enter key, then use down arrow to go to next cell. Begin running steps with Shift + Enter keys.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33de00b-c220-4a41-bfa8-4ec21811368b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run -i ../UseCases/startup.ipynb\n",
    "eng = create_context(host = 'host.docker.internal', username='demo_user', password = password)\n",
    "print(eng)\n",
    "eng.execute('''SET query_band='DEMO=08_ModelOps_GIT_PIMA_Python_H2OAutoML.ipynb;' UPDATE FOR SESSION; ''')\n",
    "\n",
    "# configure byom/val installation\n",
    "configure.val_install_location = \"VAL\"\n",
    "configure.byom_install_location = \"MLDB\"\n",
    "\n",
    "# set the path to the local project repository for this model demo\n",
    "model_local_path = '~/modelops-demo-models/model_definitions/pima_h2o_xgb'\n",
    "res = os.system(f'mkdir -p {model_local_path}/model_modules')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c03dc",
   "metadata": {},
   "source": [
    "## Step 3. Define Training Function\n",
    "\n",
    "The training function takes the following shape\n",
    "\n",
    "```python\n",
    "def train(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "    \n",
    "    # your training code\n",
    "    \n",
    "    # export model artefacts\n",
    "    mojo = model.download_mojo(path=context.artifact_output_path, get_genmodel_jar=True)\n",
    "    new_mojo = os.path.join(os.path.abspath(os.getcwd()), context.artifact_output_path, \"model.h2o\")\n",
    "    if os.path.isfile(new_mojo):\n",
    "        print(\"The file already exists\")\n",
    "    else:\n",
    "        # Rename the file\n",
    "        os.rename(mojo, new_mojo)\n",
    "    \n",
    "    record_training_stats(...)\n",
    "```\n",
    "\n",
    "You can execute this from the CLI or directly within the notebook as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b64643",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $model_local_path/model_modules/training.py\n",
    "from teradataml import DataFrame\n",
    "from aoa import (\n",
    "    record_training_stats,\n",
    "    save_plot,\n",
    "    aoa_create_context,\n",
    "    ModelContext\n",
    ")\n",
    "import os\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "\n",
    "def check_java():\n",
    "    try:\n",
    "        print(os.environ['JAVA_HOME'])\n",
    "    except:\n",
    "        print ('Installing Java...')\n",
    "        import jdk\n",
    "        jdk.install('17', path='/usr/local/jdk')\n",
    "        os.environ['JAVA_HOME'] = '/usr/local/jdk/jdk-17.0.7+7'\n",
    "\n",
    "\n",
    "def train(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "\n",
    "    feature_names = context.dataset_info.feature_names\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "    \n",
    "    # read training dataset from Teradata and convert to pandas\n",
    "    check_java()\n",
    "    h2o.init()\n",
    "    train_df = DataFrame.from_query(context.dataset_info.sql)\n",
    "    train_hdf = h2o.H2OFrame(train_df.to_pandas())\n",
    "\n",
    "    # convert target column to categorical\n",
    "    train_hdf[target_name] = train_hdf[target_name].asfactor()\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "  \n",
    "    # Execute AutoML on training data\n",
    "    aml = H2OAutoML(max_models=context.hyperparams['max_models'], seed=context.hyperparams['seed'])\n",
    "    aml.train(x=feature_names, y=target_name, training_frame=train_hdf)\n",
    "\n",
    "    # Here we are getting the best GBM algorithm for demo purposes\n",
    "    model = aml.get_best_model(algorithm=\"gbm\")\n",
    "    if not model:\n",
    "        model = aml.leader\n",
    "\n",
    "    print(\"Finished training\")\n",
    "\n",
    "    # export model artefacts\n",
    "    mojo = model.download_mojo(path=context.artifact_output_path, get_genmodel_jar=True)\n",
    "    new_mojo = os.path.join(os.path.abspath(os.getcwd()), context.artifact_output_path, \"model.h2o\")\n",
    "    if os.path.isfile(new_mojo):\n",
    "        print(\"The file already exists\")\n",
    "    else:\n",
    "        # Rename the file\n",
    "        os.rename(mojo, new_mojo)\n",
    "\n",
    "    print(\"Saved trained model\")\n",
    "\n",
    "    try:\n",
    "        model.varimp_plot(server=True, save_plot_path=os.path.join(os.path.abspath(os.getcwd()), context.artifact_output_path, \"feature_importance.png\"))\n",
    "        fi = model.varimp(True)\n",
    "        fix = fi[['variable','scaled_importance']]\n",
    "        fis = fix.to_dict('records')\n",
    "        feature_importance = {v['variable']:float(v['scaled_importance']) for (k,v) in enumerate(fis)}\n",
    "    except:\n",
    "        print(\"Warning: This model doesn't support feature importance (Stacked Ensemble)\")\n",
    "        aml.varimp_heatmap()\n",
    "        save_plot('Feature Heatmap', context=context)\n",
    "        feature_importance = {}\n",
    "\n",
    "    record_training_stats(train_df,\n",
    "                          features=feature_names,\n",
    "                          targets=[target_name],\n",
    "                          categorical=[target_name],\n",
    "                          feature_importance=feature_importance,\n",
    "                          context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# Check Java installation, installs if required\n",
    "try:\n",
    "    print(f\"Java is installed at {os.environ['JAVA_HOME']}\")\n",
    "except:\n",
    "    os.environ['JAVA_HOME'] = '/home/jovyan/.jdk/jdk-17.0.7+7'\n",
    "    if os.path.isdir(os.environ['JAVA_HOME']) is False:\n",
    "        print ('Installing Java...')\n",
    "        import jdk\n",
    "        jdk.install('17', path='/home/jovyan/.jdk')\n",
    "\n",
    "# define the training dataset \n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "    F.*, D.hasdiabetes\n",
    "FROM PIMA_PATIENT_FEATURES F \n",
    "JOIN PIMA_PATIENT_DIAGNOSES D\n",
    "ON F.patientid = D.patientid\n",
    "    WHERE D.patientid MOD 5 <> 0\n",
    "\"\"\"\n",
    "\n",
    "feature_metadata =  {\n",
    "    \"database\": \"demo_user\",\n",
    "    \"table\": \"aoa_statistics_metadata\"\n",
    "}\n",
    "hyperparams = {\"max_models\": 3, \"seed\": 1}\n",
    "\n",
    "entity_key = \"PatientId\"\n",
    "target_names = [\"HasDiabetes\"]\n",
    "feature_names = [\"NumTimesPrg\", \"PlGlcConc\", \"BloodP\", \"SkinThick\", \"TwoHourSerIns\", \"BMI\", \"DiPedFunc\", \"Age\"]\n",
    "\n",
    "from aoa import ModelContext, DatasetInfo\n",
    "\n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=feature_names,\n",
    "                           target_names=target_names,\n",
    "                           feature_metadata=feature_metadata)\n",
    "\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=\"artifacts/\",\n",
    "                   model_version=\"v1\",\n",
    "                   model_table=\"aoa_model_h2oaml_v1\")\n",
    "\n",
    "sys.path.append(os.path.expanduser(f\"{model_local_path}/model_modules\"))\n",
    "import training\n",
    "training.train(context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the generated files\n",
    "!ls -lh artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94661e",
   "metadata": {},
   "source": [
    "### Define Evaluation Function\n",
    "\n",
    "The evaluation function takes the following shape\n",
    "\n",
    "```python\n",
    "def evaluate(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "\n",
    "    # read your model\n",
    "    with open(f\"{context.artifact_input_path}/model.h2o\", \"rb\") as f:\n",
    "        model_bytes = f.read()\n",
    "    \n",
    "    model = store_byom_tmp(get_context(), \"byom_models_tmp\", context.model_version, model_bytes)\n",
    "    \n",
    "    # your evaluation logic\n",
    "    \n",
    "    record_evaluation_stats(...)\n",
    "```\n",
    "\n",
    "You can execute this from the CLI or directly within the notebook as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $model_local_path/model_modules/evaluation.py\n",
    "from sklearn import metrics\n",
    "from teradataml import DataFrame, copy_to_sql, get_context, H2OPredict\n",
    "from aoa import (\n",
    "    record_evaluation_stats,\n",
    "    save_plot,\n",
    "    aoa_create_context,\n",
    "    store_byom_tmp,\n",
    "    ModelContext\n",
    ")\n",
    "import json\n",
    "import os\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "def evaluate(context: ModelContext, **kwargs):\n",
    "\n",
    "    aoa_create_context()\n",
    "\n",
    "    with open(f\"{context.artifact_input_path}/model.h2o\", \"rb\") as f:\n",
    "        model_bytes = f.read()\n",
    "\n",
    "    model = store_byom_tmp(get_context(), \"byom_models_tmp\", context.model_version, model_bytes)\n",
    "\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "\n",
    "    byom_target_sql = \"CAST(prediction AS INT)\"\n",
    "\n",
    "    h2o = H2OPredict(\n",
    "        modeldata=model,\n",
    "        newdata=DataFrame.from_query(context.dataset_info.sql),\n",
    "        accumulate=[context.dataset_info.entity_key, target_name]\n",
    "    )\n",
    "\n",
    "    predictions_df = h2o.result\n",
    "\n",
    "    predictions_df.to_sql(table_name=\"predictions_tmp\", if_exists=\"replace\", temporary=True)\n",
    "\n",
    "    metrics_df = DataFrame.from_query(f\"\"\"\n",
    "    SELECT \n",
    "        {target_name} as y_test, \n",
    "        {byom_target_sql} as y_pred\n",
    "        FROM predictions_tmp\n",
    "    \"\"\")\n",
    "    metrics_df = metrics_df.to_pandas()\n",
    "\n",
    "    y_pred = metrics_df[[\"y_pred\"]]\n",
    "    y_test = metrics_df[[\"y_test\"]]\n",
    "\n",
    "    evaluation = {\n",
    "        'Accuracy': '{:.2f}'.format(metrics.accuracy_score(y_test, y_pred)),\n",
    "        'Recall': '{:.2f}'.format(metrics.recall_score(y_test, y_pred)),\n",
    "        'Precision': '{:.2f}'.format(metrics.precision_score(y_test, y_pred)),\n",
    "        'f1-score': '{:.2f}'.format(metrics.f1_score(y_test, y_pred))\n",
    "    }\n",
    "\n",
    "    with open(f\"{context.artifact_output_path}/metrics.json\", \"w+\") as f:\n",
    "        json.dump(evaluation, f)\n",
    "\n",
    "    cf = metrics.confusion_matrix(y_test, y_pred)\n",
    "    display = metrics.ConfusionMatrixDisplay(confusion_matrix=cf)\n",
    "    display.plot()\n",
    "    save_plot('Confusion Matrix', context=context)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=context.model_version)\n",
    "    display.plot()\n",
    "    save_plot('ROC Curve', context=context)\n",
    "\n",
    "    # calculate stats if training stats exist\n",
    "    if os.path.exists(f\"{context.artifact_input_path}/data_stats.json\"):\n",
    "        record_evaluation_stats(features_df=DataFrame.from_query(context.dataset_info.sql),\n",
    "                                predicted_df=DataFrame(\"predictions_tmp\"),\n",
    "                                context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c3fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the evaluation dataset \n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "    F.*, D.hasdiabetes \n",
    "FROM PIMA_PATIENT_FEATURES F \n",
    "JOIN PIMA_PATIENT_DIAGNOSES D\n",
    "ON F.patientid = D.patientid\n",
    "    WHERE D.patientid MOD 5 = 0\n",
    "\"\"\"\n",
    "\n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=feature_names,\n",
    "                           target_names=target_names,\n",
    "                           feature_metadata=feature_metadata)\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=\"artifacts/\",\n",
    "                   artifact_input_path=\"artifacts/\",\n",
    "                   model_version=\"h2oaml_v1\",\n",
    "                   model_table=\"aoa_model_h2oaml_v1\")\n",
    "\n",
    "# drop volatile table from session if executing multiple times\n",
    "try:\n",
    "    get_context().execute(f\"DROP TABLE byom_models_tmp\")\n",
    "except: \n",
    "    pass\n",
    "\n",
    "import evaluation\n",
    "evaluation.evaluate(context=ctx)\n",
    "\n",
    "# view evaluation results\n",
    "import json\n",
    "with open(f\"{ctx.artifact_output_path}/metrics.json\") as f:\n",
    "    print(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fd4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the generated files\n",
    "!ls -lh artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c08a5",
   "metadata": {},
   "source": [
    "### Define Scoring Function\n",
    "\n",
    "The scoring function takes the following shape\n",
    "\n",
    "```python\n",
    "def score(context: ModelContext, **kwargs):\n",
    "    aoa_create_context()\n",
    "\n",
    "    # read your model\n",
    "    with open(f\"{context.artifact_input_path}/model.h2o\", \"rb\") as f:\n",
    "        model_bytes = f.read()\n",
    "\n",
    "    model = store_byom_tmp(get_context(), \"byom_models_tmp\", context.model_version, model_bytes)\n",
    "    \n",
    "    # your evaluation logic\n",
    "    \n",
    "    record_scoring_stats(...)\n",
    "```\n",
    "\n",
    "You can execute this from the CLI or directly within the notebook as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $model_local_path/model_modules/scoring.py\n",
    "from teradataml import copy_to_sql, get_context, DataFrame, H2OPredict\n",
    "from aoa import (\n",
    "    record_scoring_stats,\n",
    "    aoa_create_context,\n",
    "    store_byom_tmp,\n",
    "    ModelContext\n",
    ")\n",
    "\n",
    "\n",
    "def score(context: ModelContext, **kwargs):\n",
    "\n",
    "    aoa_create_context()\n",
    "\n",
    "    with open(f\"{context.artifact_input_path}/model.h2o\", \"rb\") as f:\n",
    "        model_bytes = f.read()\n",
    "\n",
    "    model = store_byom_tmp(get_context(), \"byom_models_tmp\", context.model_version, model_bytes)\n",
    "\n",
    "    target_name = context.dataset_info.target_names[0]\n",
    "    entity_key = context.dataset_info.entity_key\n",
    "\n",
    "    byom_target_sql = \"CAST(prediction AS INT)\"\n",
    "\n",
    "    print(\"Scoring\")\n",
    "    h2o = H2OPredict(\n",
    "        modeldata=model,\n",
    "        newdata=DataFrame.from_query(context.dataset_info.sql),\n",
    "        accumulate=context.dataset_info.entity_key)\n",
    "\n",
    "    print(\"Finished Scoring\")\n",
    "\n",
    "\n",
    "    # store the predictions\n",
    "    predictions_df = h2o.result\n",
    "    \n",
    "    # add job_id column so we know which execution this is from if appended to predictions table\n",
    "    predictions_df = predictions_df.assign(job_id=context.job_id)\n",
    "    cols = {}\n",
    "    cols[target_name] = predictions_df['prediction']\n",
    "    predictions_df = predictions_df.assign(**cols)\n",
    "    predictions_df = predictions_df[[\"job_id\", entity_key, target_name, \"json_report\"]]\n",
    "\n",
    "    copy_to_sql(df=predictions_df,\n",
    "                schema_name=context.dataset_info.predictions_database,\n",
    "                table_name=context.dataset_info.predictions_table,\n",
    "                index=False,\n",
    "                if_exists=\"append\")\n",
    "\n",
    "    print(\"Saved predictions in Teradata\")\n",
    "    \n",
    "    # calculate stats\n",
    "    predictions_df = DataFrame.from_query(f\"\"\"\n",
    "        SELECT \n",
    "            * \n",
    "        FROM {context.dataset_info.get_predictions_metadata_fqtn()} \n",
    "            WHERE job_id = '{context.job_id}'\n",
    "    \"\"\")\n",
    "\n",
    "    record_scoring_stats(features_df=DataFrame.from_query(context.dataset_info.sql),\n",
    "                         predicted_df=predictions_df,\n",
    "                         context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5364936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ModelContext to test with. The ModelContext is created and managed automatically by ModelOps \n",
    "# when it executes your code via CLI / UI. However, for testing in the notebook, you can define as follows\n",
    "\n",
    "# define the scoring dataset \n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "    F.*\n",
    "FROM PIMA_PATIENT_FEATURES F \n",
    "    WHERE F.patientid MOD 5 = 0\n",
    "\"\"\"\n",
    "\n",
    "# where to store predictions\n",
    "predictions = {\n",
    "    \"database\": \"demo_user\",\n",
    "    \"table\": \"pima_patient_predictions_tmp\"\n",
    "}\n",
    "\n",
    "import uuid\n",
    "job_id=str(uuid.uuid4())\n",
    "\n",
    "dataset_info = DatasetInfo(sql=sql,\n",
    "                           entity_key=entity_key,\n",
    "                           feature_names=feature_names,\n",
    "                           target_names=target_names,\n",
    "                           feature_metadata=feature_metadata,\n",
    "                           predictions=predictions)\n",
    "\n",
    "ctx = ModelContext(hyperparams=hyperparams,\n",
    "                   dataset_info=dataset_info,\n",
    "                   artifact_output_path=\"artifacts/\",\n",
    "                   artifact_input_path=\"artifacts/\",\n",
    "                   model_version=\"h2oaml_v1\",\n",
    "                   model_table=\"aoa_model_h2oaml_v1\",\n",
    "                   job_id=job_id)\n",
    "\n",
    "# drop volatile table from session if executing multiple times\n",
    "try:\n",
    "    get_context().execute(f\"DROP TABLE byom_models_tmp\")\n",
    "except: \n",
    "    pass\n",
    "\n",
    "import scoring\n",
    "scoring.score(context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f24b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame.from_query(f\"SELECT * FROM pima_patient_predictions_tmp WHERE job_id='{job_id}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "os.system('rm -f artifacts/*')\n",
    "\n",
    "try:\n",
    "    get_context().execute(f\"DROP TABLE aoa_model_indb_v1\")\n",
    "except: \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    get_context().execute(f\"DROP TABLE pima_patient_predictions_tmp\")\n",
    "except: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c9e4a8",
   "metadata": {},
   "source": [
    "### Define Model Metadata\n",
    "\n",
    "Now let's create the configuration files.\n",
    "\n",
    "Requirements file with the dependencies and versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $model_local_path/model_modules/requirements.txt\n",
    "teradataml==17.20.0.3\n",
    "aoa==7.0.1\n",
    "pandas==1.1.5\n",
    "matplotlib==3.5.2\n",
    "scikit-learn==0.24.2\n",
    "h2o==3.40.0.4\n",
    "install-jdk==1.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cea7f3",
   "metadata": {},
   "source": [
    "The hyper parameter configuration (default values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa9e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $model_local_path/config.json\n",
    "{\n",
    "   \"hyperParameters\": {\n",
    "      \"max_models\": 10,\n",
    "      \"seed\": 1\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98bb820",
   "metadata": {},
   "source": [
    "The model configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $model_local_path/model.json\n",
    "{\n",
    "    \"id\": \"cdce3e32-3f8c-417f-b10e-493f21740a92\",\n",
    "    \"name\": \"Python PIMA H2O AutoML\",\n",
    "    \"description\": \"Python PIMA H2O AutoML for Diabetes Prediction\",\n",
    "    \"language\": \"python\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd08392",
   "metadata": {},
   "source": [
    "### Commit and push changes\n",
    "\n",
    "Run the command below to commit and push changes to our forked repository, so ModelOps can fetch the changes to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c64e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $model_local_path/../.. && git add . && git commit -m \"Added Python PIMA H2O AutoML demo model ðŸ’¦\" && git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252cc8d5",
   "metadata": {},
   "source": [
    "Now that changes are pushed, you can make the lifecycle inside **ModelOps User Interface**, plan for new trainings, evaluations, scorings. Compare models and operationalize into Production with automated Monitoring and alerting capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e1ede4",
   "metadata": {},
   "source": [
    "## Step 8. ModelOps full lifecycle till deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab9295e",
   "metadata": {},
   "source": [
    "Use or Create a Project with the git code repository with the model code, then you should see the model in the catalog already created\n",
    "\n",
    "<img src=\"images/08_01.png\" alt=\"Model Catalog with inDB\"/>\n",
    "\n",
    "Select the Model and then click Train a new Model. Use default hyper-parameters. This will launch the training job with the training script we generated and pushed to Git.\n",
    "\n",
    "<img src=\"images/08_02.png\" alt=\"Train\"/>\n",
    "\n",
    "<img src=\"images/08_03.png\" alt=\"Train job\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"images/08_04.png\" alt=\"Train finished\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "When Model is trained a new Model Id is created and you can get inside the Model Lifecycle screen to review artifacts and other details\n",
    "\n",
    "<img src=\"images/08_06.png\" alt=\"Model lifecycle\"/>\n",
    "\n",
    "Now, let's evaluate the Model, click the button and select the evaluation dataset. This will launch the evaluation job with the training script we generated and pushed to Git.\n",
    "\n",
    "<img src=\"images/08_07.png\" alt=\"Evaluation\" width=\"500\" height=\"500\"/> <img src=\"images/08_08.png\" alt=\"Evaluation job\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "When evaluation job is finished a Model evaluation Report is generated with the metrics and charts that evaluation script generates\n",
    "\n",
    "<img src=\"images/08_26.png\" alt=\"Model Report\" />\n",
    "\n",
    "Now, let's approve the model and provide an approval description\n",
    "\n",
    "<img src=\"images/08_09.png\" alt=\"Approval\" />\n",
    "\n",
    "<img src=\"images/08_10.png\" alt=\"Approval description\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "The model is ready to be deployed. Let's deploy using a Batch scheduling option - Run it manual\n",
    "\n",
    "<img src=\"images/08_11.png\" alt=\"Deployment Engine\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"images/08_12.png\" alt=\"Deployment Publish\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"images/08_13.png\" alt=\"Deployment Schedule\" width=\"500\" height=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d131f-1a18-4517-aa96-16ae2a4a12b0",
   "metadata": {},
   "source": [
    "Go and try this Step by yourself. Launch ModelOps from this button below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b0115-03cd-42f1-8287-4a78c093e333",
   "metadata": {},
   "source": [
    "[![image](images/launchModelOps.png)](/modelops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975717c",
   "metadata": {},
   "source": [
    "## Step 9. ModelOps Monitoring\n",
    "\n",
    "Now the model is deployed and a new Deployment appears in the deployment screen\n",
    "\n",
    "\n",
    "<img src=\"images/08_15.png\" alt=\"Deploymet\" />\n",
    "\n",
    "\n",
    "You can run jobs manually from here, review history of executions and view the predictions for a specific job\n",
    "\n",
    "<img src=\"images/08_16.png\" alt=\"Deployment Run\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"images/08_17.png\" alt=\"Deployment Jobs\" />\n",
    "\n",
    "<img src=\"images/08_18.png\" alt=\"Deployment view\" width=\"500\" height=\"500\" />\n",
    "\n",
    "<img src=\"images/08_19.png\" alt=\"Deployment predictions\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "<img src=\"images/08_20.png\" alt=\"Deployment\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "\n",
    "From the Feature Drift and Prediction Drift tabs you can check on the monitoring of the data drift\n",
    "\n",
    "<img src=\"images/08_22.png\" alt=\"Feature Drift\" />\n",
    "\n",
    "<img src=\"images/08_21.png\" alt=\"Prediction Drift\" />\n",
    "\n",
    "<img src=\"images/08_23.png\" alt=\"Performance Monitoring\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "From the Performance Drift, you can review multiple evaluations, let's evaluate the model with a new dataset. We create a new evaluation dataset with this query:\n",
    "    \n",
    "    SELECT * FROM pima_patient_diagnoses F WHERE F.patientid MOD 8 <> 0\n",
    "    \n",
    "<img src=\"images/08_24.png\" alt=\"Evaluate\" width=\"500\" height=\"500\" />\n",
    "\n",
    "and now see the evolution of the metrics\n",
    "\n",
    "<img src=\"images/08_25.png\" alt=\"Metrics monitoring\" />\n",
    "\n",
    "\n",
    "With ModelOps you can close the cycle and review make decisions when you need to replace yor model in production, For example, You could get alerting from Data Drift of Performance Drift and you can create multiple versions and compare them, select a champion and deploy new versions that replace existing in Production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a1c68-423c-4fdf-8c9e-2c5c20ebdc07",
   "metadata": {},
   "source": [
    "Go and try this Step by yourself. Launch ModelOps from this button below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298a689-a1a6-47a0-8483-da3992cdb00a",
   "metadata": {},
   "source": [
    "[![image](images/launchModelOps.png)](/modelops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784043ac",
   "metadata": {},
   "source": [
    "<footer style=\"padding:10px;background:#f9f9f9;border-bottom:3px solid #394851\">Â©2023 Teradata. All Rights Reserved</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
